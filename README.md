# Agentic Alzheimer's Analyzer

**Autonomous AI agents for accelerating Alzheimer's disease and related dementias (ADRD) research**

## üéØ Overview

The Agentic Alzheimer's Analyzer is a revolutionary open-source framework that democratizes advanced Alzheimer's research through autonomous AI agents. This system transforms cognitive assessment data analysis from a months-long expert process into hours of autonomous computation, making sophisticated research capabilities accessible to any researcher worldwide.

### Core Value Proposition
- **Research Acceleration**: Reduces analysis time from months to hours
- **Cost Democratization**: $50-100 per analysis vs. $50,000+ traditional consulting
- **Global Accessibility**: No specialized personnel or infrastructure required
- **Standardized Science**: Eliminates researcher bias, ensures reproducibility
- **Open Source Impact**: Accelerates Alzheimer's research globally through shared frameworks

## üåç Who Benefits From This System?

### **Academic Researchers**
- **Small Labs**: Enterprise-level analysis without dedicated data science teams
- **Graduate Students**: Focus on discovery rather than months of data processing
- **International Collaborations**: Standardized analysis enables direct cross-study comparisons
- **Resource-Limited Institutions**: Advanced capabilities without expensive infrastructure

### **Clinical Research Organizations** 
- **Pharmaceutical Companies**: Rapid biomarker discovery for drug trials
- **Medical Device Companies**: Digital cognitive assessment validation
- **Healthcare Systems**: Population-level cognitive screening programs
- **Clinical Research Organizations**: Accelerated patient stratification and outcome analysis

### **Global Health Impact**
- **Developing Countries**: Participation in global research without extensive resources
- **Underserved Populations**: Scalable screening for at-risk communities
- **Public Health Agencies**: Population-wide cognitive health monitoring
- **Early Detection Programs**: Automated screening at massive scale

## üöÄ Transformative Capabilities

### **Expert-Level AI Analysis**
- **Domain Intelligence**: Built-in knowledge of Alzheimer's research methodology, cognitive assessments, and clinical best practices
- **Statistical Expertise**: Understands effect sizes, power analysis, and clinical significance thresholds
- **Research Context**: AI agents trained on Alzheimer's research standards and biomarker discovery approaches
- **Clinical Translation**: Interprets findings through the lens of patient care and therapeutic implications

### **Sophisticated Yet Generalizable**
- **Configuration-Driven**: Domain knowledge stored in configurable ontologies, not hardcoded rules
- **Assessment Agnostic**: Automatically discovers and analyzes any cognitive assessment combination
- **Extensible Framework**: New assessment types and research domains easily added
- **Pure AI Discovery**: No fallback rules - all insights generated through agentic AI analysis

### **Research Revolution**
- **Multi-modal Integration**: Seamlessly combines cognitive assessments, demographics, and literature
- **Scale Processing**: Handles hundreds of thousands of subjects automatically
- **Real-world Evidence**: Processes actual clinical datasets, not just research samples
- **Reproducible Science**: Every analysis step documented and repeatable

## üß† Framework Capabilities: Multi-Modal Cognitive Assessment

This framework automatically discovers and analyzes relationships across any combination of:
- **Self-Report Measures**: ECOG, CDR, MoCA questionnaires  
- **Digital Assessments**: MemTrax, NIH Toolbox, custom cognitive tests
- **Clinical Data**: Demographics, medical history, medications
- **Biomarker Data**: CSF, plasma, imaging results (extensible)

## üèóÔ∏è Architecture & Generalization Principles

### **Intelligent Multi-Agent System**

1. **Discovery Agent** (`agents/discovery_agent.py`)
   - **Autonomous Dataset Characterization**: Automatically discovers and maps any cognitive assessment data
   - **Intelligent Variable Mapping**: Uses domain ontologies to identify assessment types (ECOG, MemTrax, MoCA, etc.)
   - **Quality Assessment**: Applies clinical research standards for data completeness and validity

2. **Cognitive Analysis Agent** (`agents/cognitive_analysis_agent.py`)  
   - **Pure AI Analysis**: No hardcoded rules - all insights generated through advanced AI reasoning
   - **Multi-modal Integration**: Discovers relationships across any combination of cognitive measures
   - **Clinical Significance**: Interprets statistical findings through Alzheimer's research expertise
   - **Biomarker Discovery**: Identifies novel patterns using domain-informed AI analysis
   - **Methodological Learning**: Incorporates data processing principles from domain expertise while remaining dataset-agnostic

3. **Literature Research Agent** (`agents/literature_agent.py`)
   - **Expert Literature Search**: Uses domain-specific search strategies across PubMed and Semantic Scholar
   - **Intelligent Synthesis**: AI-powered extraction of relevant findings and effect sizes
   - **Novelty Detection**: Compares current findings against existing research to identify contributions

4. **AI Orchestrator** (`core/orchestrator.py`)
   - **Agentic Coordination**: Manages multi-agent workflow with AI-powered decision making
   - **Expert Synthesis**: Generates research-quality insights by combining all agent findings
   - **Human-Readable Summaries**: AI translates complex findings into clear clinical implications

### **Dataset Adapters (Pluggable Loaders)**
- Adapters live in `core/datasets/` and encapsulate dataset-specific loading/cleaning.
- Current adapters:
  - `OasisAdapter` for OASIS cross-sectional/longitudinal (`training_data/oasis/`)
  - `BrfssAdapter` for BRFSS surveillance CSVs (`dataset.file_patterns`)
- The agent automatically selects an adapter based on `config.dataset.name` and on-disk availability, with a safe fallback to legacy OASIS loading.

### **Generalization Principles: Learning Methods, Not Data**

The framework incorporates domain expertise through **methodological principles** rather than dataset-specific hardcoding:

#### **Universal Data Processing Patterns**
```python
# Generalized filtering (not dataset-specific)
if 'Status' in df.columns:
    df = df[df['Status'] == 'Collected']  # Universal quality filter

# Pattern-based variable discovery (configurable)
memtrax_vars = [col for col in df.columns 
                if any(pattern in col.lower() 
                      for pattern in ['reaction_time', 'correctpct', 'accuracy'])]
```

#### **Standard Medical Coding Integration**
- **QID Medical Codes**: Uses standard medical classification (QID1-5: Dementia, QID1-12: Alzheimer's, etc.)
- **Assessment Patterns**: Recognizes common cognitive assessment structures across datasets
- **Clinical Variables**: Identifies demographics, medical history, and outcomes using healthcare standards

#### **Configuration-Driven Intelligence**
```yaml
# Dataset-agnostic variable patterns
target_variables:
  cognitive_performance: ["accuracy", "reaction_time", "correct_pct"]
  clinical_status: ["diagnosis", "cognitive_status", "mci", "dementia"]
  quality_indicators: ["status", "validity_flag", "completed"]
```

#### **Adaptive Processing Logic**
- **Smart Data Type Detection**: Automatically identifies and converts numeric, categorical, and date variables
- **Quality Validation**: Applies clinical research standards (valid ranges, completeness thresholds)
- **Scale Management**: Handles datasets from hundreds to millions of records through intelligent sampling
- **Assessment Discovery**: Recognizes cognitive test patterns regardless of specific implementation
- **Cartesian Join Protection**: Advanced merge safety system prevents data explosion from duplicate records
- **Intelligent Deduplication**: Automatically removes duplicate subjects using timestamp-based or completeness-based strategies

### **Expert Knowledge Systems**

- **Domain Ontologies** (`config/data_dictionary.json`)
  - **Cognitive Assessment Library**: ECOG, MemTrax, MoCA, CDR, and custom assessments
  - **Statistical Standards**: Effect size thresholds, power analysis parameters, clinical significance criteria
  - **Research Methodology**: Multi-modal assessment best practices, biomarker discovery protocols
  - **Extensible Framework**: Easily add new cognitive domains and assessment types

- **AI-Powered Infrastructure**
  - **Token Manager** (`core/token_manager.py`): Multi-provider AI usage optimization and cost control
  - **Configuration System** (`config/`): Expert-curated research parameters and domain knowledge
  - **Pure AI Analysis**: No fallback rules - all insights generated through domain-informed AI reasoning

### **How We Ensure Generalizability**

#### **Learning from Domain Expertise Without Overfitting**
The system incorporates insights from experienced researchers' analysis scripts, but abstracts them into **universal principles**:

1. **Data Quality Patterns**: 
   - "Filter to collected/valid assessments" becomes a configurable quality check
   - "Convert key performance variables to numeric" becomes automatic type detection
   - "Remove invalid dates/outliers" becomes configurable validation rules

2. **Variable Recognition Logic**:
   - Instead of hardcoding "CorrectPCT", we search for patterns: `["accuracy", "correct_pct", "percent_correct"]`
   - Instead of specific column names, we use semantic categories: `cognitive_performance`, `clinical_outcomes`
   - Medical codes (QID1-x) are recognized as standard healthcare classifications, not dataset-specific

3. **Assessment-Agnostic Processing**:
   ```python
   # Generalizable: Works with any cognitive assessment
   for assessment_type in discovered_assessments:
       variables = find_variables_by_pattern(assessment_type.patterns)
       quality_filter = apply_quality_standards(variables)
       analysis_results = ai_analyze_relationships(quality_filter)
   
   # NOT: Hardcoded for specific tests
   # ecog_scores = df['ECOG_Total']  # Too specific!
   # memtrax_rt = df['CorrectResponsesRT']  # Won't work with other datasets!
   ```

#### **Configuration-Based Adaptation**
New datasets require only **configuration updates**, not code changes:
- Update `file_patterns` to match your file naming
- Add your variable names to `target_variables` patterns  
- Specify any custom quality criteria
- The AI agents handle the rest automatically

This approach ensures the framework benefits from domain expertise while remaining **truly generalizable** to any Alzheimer's research dataset worldwide.

### **Production-Grade Data Safety**

#### **Cartesian Join Protection System**
Real-world datasets often contain duplicate records, multiple timepoints, and inconsistent subject IDs. Our advanced merge safety system prevents catastrophic data explosions:

```python
# Early Warning System
‚ö†Ô∏è CARTESIAN JOIN RISK: Potential 7,397,690,978,650,471,200 record combinations
üí° Using inner joins to reduce risk, but verify subject ID consistency

# Automatic Risk Detection
üö® HIGH DUPLICATION RISK: 72.5% duplication could cause Cartesian joins
üìä Deduplicating cognitive_data: 181,855 ‚Üí 50,101 records
‚úÖ Deduplication by most recent timestamp

# Safe Results
‚úÖ Final dataset: 38,948 subjects (prevented 32M+ record explosion)
```

#### **Intelligent Data Processing**
- **Duplicate Detection**: Identifies and warns about high duplication rates (>50%)
- **Smart Deduplication**: Uses timestamp-based or data completeness strategies
- **Growth Monitoring**: Detects suspicious merge growth patterns (>10x = error, >2x = warning)
- **Graceful Failures**: Clear error messages with diagnostic information for troubleshooting
- **Memory Protection**: Prevents system crashes from runaway memory usage

#### **Real-World Dataset Handling**
The system safely processes complex datasets with:
- **Multiple timepoints per subject** (longitudinal studies)
- **Duplicate records** (data collection artifacts)  
- **Inconsistent subject IDs** (data integration challenges)
- **Large-scale data** (200K+ records, millions of data points)
- **Missing data patterns** (real-world clinical data)

#### **Validation Before Analysis**
```
üìä Pre-merge validation:
   cognitive_data: 50,101 unique subjects, 181,855 total records
   ‚ö†Ô∏è 131,754 duplicate subjects (72.5% duplication rate)
   ecog_data: 52,900 unique subjects, 227,737 total records  
   ‚ö†Ô∏è 174,837 duplicate subjects (76.8% duplication rate)
```

This **production-grade robustness** ensures the framework works reliably with messy real-world data that would crash traditional analysis pipelines.

## üöÄ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/lawrencechernin/agentic_alzheimer_analyzer.git
cd agentic_alzheimer_analyzer

# Install dependencies
pip install -r requirements.txt
```

### 2. Download OASIS Dataset

**‚ö†Ô∏è Important**: This repository does not include the OASIS dataset. You must download it yourself:

1. **Visit Kaggle**: [https://www.kaggle.com/datasets/jboysen/mri-and-alzheimers](https://www.kaggle.com/datasets/jboysen/mri-and-alzheimers)
2. **Download the dataset** (requires free Kaggle account)
3. **Extract the CSV files** to `training_data/oasis/`

**üìã Detailed Instructions**: See [training_data/oasis/README.md](training_data/oasis/README.md) for complete setup instructions, alternative download methods, and verification steps.

### 3. Configuration

**Set up API keys** (choose at least one AI provider):
```bash
# For Claude/Anthropic (recommended)
export ANTHROPIC_API_KEY="your_anthropic_key"

# For OpenAI GPT (alternative)
export OPENAI_API_KEY="your_openai_key" 

# For Google Gemini (alternative)
export GEMINI_API_KEY="your_gemini_key"
```

**Configure your dataset** in `config/config.yaml`:
```yaml
dataset:
  name: "BHR_2022_ECOG_MemTrax_Validation"  # Descriptive study name
  description: "Your study description"
  
  # Data sources - can be local paths, URLs, or cloud storage
  data_sources:
    - path: "../bhr.0/BHR-ALL-EXT_Mem_2022/"  # Your data path
      type: "local_directory"
      description: "Primary study data"
    # Add more sources if needed:
    # - url: "https://api.example.com/data"
    #   type: "api_endpoint"
    # - path: "s3://bucket/data/"
    #   type: "cloud_storage"

experiment:
  name: "BHR_2022_ECOG_MemTrax_Validation_Study"  # Specific experiment name
  dataset: "BHR_2022_ECOG_MemTrax_Validation"     # Links to dataset name
  
  primary_objectives:
    - "Compare ECOG self-report vs informant ratings"
    - "Correlate ECOG scores with MemTrax performance"
    - "Your additional research questions here"
```

### 3. Run Analysis

```bash
python run_analysis.py
```

The system will:
1. üîç Discover your dataset automatically (full dataset analysis by default)
2. üß† Execute comprehensive analysis with Cartesian join protection
3. üìö Research relevant literature (158+ papers typically)
4. üí° Generate insights and recommendations using domain-expert AI
5. üìÑ Create research-quality reports with human-readable summaries

## üìÅ Output Structure

After analysis, check the `outputs/` directory:

```
outputs/
‚îú‚îÄ‚îÄ complete_analysis_results.json      # Complete results
‚îú‚îÄ‚îÄ dataset_discovery_results.json      # Dataset characterization
‚îú‚îÄ‚îÄ ecog_memtrax_analysis_results.json  # Core analysis results
‚îú‚îÄ‚îÄ literature_research_results.json    # Literature findings
‚îú‚îÄ‚îÄ grant_application_section.md        # Grant preliminary data
‚îú‚îÄ‚îÄ executive_summary.md                # Executive summary
‚îî‚îÄ‚îÄ visualizations/                     # Generated plots
    ‚îú‚îÄ‚îÄ ecog_memtrax_correlations.png
    ‚îú‚îÄ‚îÄ self_informant_comparison.png
    ‚îî‚îÄ‚îÄ cognitive_performance_distributions.png
```

## ‚öôÔ∏è Configuration Guide

### Dataset Configuration

**Step 1: Set your data path**
Edit the `data_sources` section in `config/config.yaml`:
```yaml
data_sources:
  - path: "../your-data-directory/"     # Update this path
    type: "local_directory"
    description: "Your study description"
```

**Step 2: File discovery**
The system automatically discovers data files based on patterns:
```yaml
file_patterns:
  cognitive_data:
    - "*MemTrax*.csv"
    - "*cognitive*.csv"
  ecog_data:
    - "*ECOG*.csv" 
    - "*EverydayCognition*.csv"
  demographic_data:
    - "*Demographics*.csv"
```

**Step 3: Customize experiment**
Update the experiment section:
```yaml
experiment:
  name: "Your_Experiment_Name"
  primary_objectives:
    - "Your research question 1"
    - "Your research question 2"
```

### Variable Mapping

Variables are mapped using the data dictionary (`config/data_dictionary.json`):

```json
{
  "variable_mappings": {
    "cognitive_assessments": {
      "memtrax_reaction_time": {
        "possible_names": [
          "CorrectResponsesRT", "ResponseTime", "reaction_time"
        ],
        "description": "Reaction time for correct responses",
        "valid_range": [0.3, 3.0]
      }
    }
  }
}
```

### Token Management

Control AI API usage in `config/usage_limits.json`:

```json
{
  "claude": {
    "daily_token_limit": 1000000,
    "monthly_cost_limit": 1000.0,
    "enabled": true
  }
}
```

## üî¨ Research Applications

### Primary Use Cases

1. **ECOG-Digital Assessment Validation**
   - Correlate self-report questionnaires with objective measures
   - Identify discrepancies between self and informant reports
   - Validate digital cognitive assessments

2. **Biomarker Discovery**
   - Identify cognitive patterns predictive of decline
   - Stratify populations for biomarker testing
   - Generate hypotheses for validation studies

3. **Literature Synthesis**
   - Automatically contextualize findings within existing research
   - Identify research gaps and novel discoveries
   - Generate citation networks and meta-analyses

### Extensibility

The framework is designed to be easily extended:

- **New Datasets**: Simply update configuration files
- **New Analyses**: Add custom analysis modules
- **New AI Providers**: Extend the token manager
- **New Domains**: Modify data dictionary and ontologies

## ü§ñ Expert-Level AI Integration

### **Domain-Informed AI Analysis**
- **Research Expertise**: AI agents understand Alzheimer's research methodology, statistical best practices, and clinical significance
- **Pure Agentic Discovery**: No hardcoded rules - all insights generated through advanced AI reasoning with domain context
- **Multi-Provider Intelligence**: Automatic selection of optimal AI provider based on task complexity and availability

### **Supported AI Providers**
- **Claude (Anthropic)**: Primary choice for complex research analysis and clinical interpretation
- **GPT (OpenAI)**: Alternative for statistical analysis and literature synthesis  
- **Gemini (Google)**: Backup option for large-scale data processing (optional)

### **Intelligent Resource Management**
- **Adaptive Provider Selection**: Automatically chooses best AI model for each analysis phase
- **Real-time Cost Monitoring**: Track token usage across all providers with detailed analytics
- **Graceful Error Handling**: Robust fallbacks ensure analysis completion even with API issues
- **Expert Prompt Engineering**: Domain-specific prompts maximize AI reasoning quality

## üìä Expert-Level Analysis Capabilities

### **AI-Powered Statistical Analysis**
- **Adaptive Statistical Methods**: AI selects appropriate tests (Pearson, Spearman, non-parametric) based on data characteristics
- **Clinical Significance Assessment**: Goes beyond p-values to assess practical significance for patient care
- **Effect Size Intelligence**: Interprets Cohen's d, correlation coefficients through clinical research expertise
- **Multi-Modal Integration**: Discovers complex relationships across cognitive, demographic, and clinical variables

### **Domain-Informed Insights**
- **Biomarker Discovery**: AI identifies novel cognitive patterns with potential clinical utility
- **Risk Stratification**: Develops data-driven models for identifying high-risk individuals
- **Anosognosia Detection**: Specialized analysis of self-awareness deficits in cognitive decline
- **Assessment Validation**: Evaluates psychometric properties of digital cognitive tools

### **Research-Quality Outputs**
- **Publication-Ready Visualizations**: Professional figures with clinical research standards
- **Human-Readable Summaries**: AI translates complex findings into clear clinical implications
- **Novel Hypothesis Generation**: Creates testable research hypotheses based on discovered patterns
- **Literature-Contextualized Results**: Positions findings within existing Alzheimer's research landscape

## üåê Global Research Impact

### **Democratizing Alzheimer's Research Worldwide**

**For Academic Institutions:**
- **Small Universities**: Access to enterprise-level AI analysis without million-dollar infrastructure
- **International Collaborations**: Standardized protocols enable direct cross-study comparisons
- **Graduate Education**: Students can focus on scientific discovery rather than technical implementation
- **Reproducible Science**: Eliminates "black box" analysis - every step is auditable and repeatable

**For Clinical Research:**
- **Pharmaceutical Industry**: Accelerated biomarker discovery reduces drug development timelines
- **Medical Device Validation**: Rapid assessment of digital cognitive tools across diverse populations  
- **Healthcare Systems**: Population-wide screening programs for early detection
- **Regulatory Support**: Standardized analysis frameworks for FDA/EMA submissions

**For Global Health Equity:**
- **Developing Countries**: First-time access to advanced Alzheimer's research capabilities
- **Resource-Limited Settings**: No specialized personnel or expensive infrastructure required
- **Underserved Populations**: Scalable cognitive screening for at-risk communities worldwide
- **Open Science**: Breaking down barriers between well-funded and resource-limited research

### **Community-Driven Innovation**
- **Crowdsourced Validation**: Global research community validates and improves analysis methods
- **Shared Data Dictionaries**: Collaborative development of standardized cognitive assessment ontologies
- **Plugin Ecosystem**: Custom analysis modules for specific research domains
- **Transparent Development**: Open-source transparency builds trust in AI-generated insights

## üîí Privacy and Ethics

### Data Protection

- Offline mode available: set `ai_settings.offline_mode: true` to disable any external AI calls and use deterministic summaries
- When online AI is enabled, the framework avoids sending raw subject-level data in prompts; only aggregated, anonymized summaries are used
- Automatic anonymization of outputs
- Configurable privacy levels
- HIPAA-compatible deployment options

### Reproducible Research

- Complete audit trail of analysis steps
- Version control for all components
- Deterministic results with fixed random seeds
- Open-source transparency

## üÜò Troubleshooting

### Common Issues

**"No data files found"**
- Check `data_sources` path in `config/config.yaml`
- Verify file naming patterns match `file_patterns`
- Ensure files are in CSV format
- Confirm the path exists and contains data files

**"Token limit exceeded"**
- Check usage with token manager
- Adjust limits in `config/usage_limits.json`
- Consider using different AI provider

**"Variable mapping failed"**
- Review variable names in your dataset
- Update `config/data_dictionary.json`
- Add custom variable mappings

**"Cartesian join detected" / Data explosion**
```
üö® CRITICAL ERROR: Cartesian join detected!
üìä Data explosion: 181,855 ‚Üí 32,404,889 records (178.1x growth)
```
- **Cause**: Duplicate subject IDs or multiple timepoints per subject
- **Solution**: System automatically deduplicates, but verify your subject ID column
- **Prevention**: Check data structure before analysis - look for multiple records per subject

**"HIGH DUPLICATION RISK" warnings**
```
‚ö†Ô∏è 131,754 duplicate subjects (72.5% duplication rate)
üö® HIGH DUPLICATION RISK: 72.5% duplication could cause Cartesian joins
```
- **Normal for longitudinal studies**: Multiple timepoints create "duplicates"
- **System response**: Automatic deduplication by most recent timestamp
- **Action needed**: Usually none - system handles this automatically
- **Manual override**: Set `use_sampling: true` if you need smaller datasets

**"Very few matches found" warnings**
```
‚ö†Ô∏è Very few matches found: 1,234 records from 50,000
üí° This may indicate mismatched subject IDs between datasets
```
- **Cause**: Subject ID inconsistencies between datasets (e.g., "SUBJ001" vs "1")
- **Solution**: Check subject ID column names and formats across your datasets
- **Common fix**: Standardize subject ID formats before analysis

**"Misleading subject counts" / Discovery vs Analysis mismatch**
```
üìä DATASET OVERVIEW: Files analyzed: 6, Total subjects: 1,000
üß† ANALYSIS PHASE: Subjects analyzed: 38,948
```
- **Cause**: Discovery was using sampling (1,000 rows) while analysis used full datasets
- **Fix**: Framework now uses `full_analysis: true` by default for accurate discovery
- **Override**: Set `discovery.full_analysis: false` if you need faster discovery on very large datasets

**Out of memory / System killed**
- **Modern protection**: Updated system prevents memory explosions with Cartesian join detection
- **Legacy issues**: Update to latest version with automatic deduplication
- **Very large datasets**: Disable full discovery: `discovery.full_analysis: false`

### Support

For issues and contributions:
1. Check existing documentation
2. Search issue tracker (if available)
3. Create detailed issue report
4. Consider contributing improvements

## üéØ Future Roadmap: Transforming Alzheimer's Research

### **Near-term Enhancements (6-12 months)**
- [ ] **Multimodal Integration**: Imaging data (MRI, PET), genetic variants, blood biomarkers
- [ ] **Real-time Collaboration**: Multi-site federated analysis while preserving privacy
- [ ] **Clinical Decision Support**: Integration with electronic health records
- [ ] **Enhanced AI Models**: Domain-specific foundation models for cognitive assessment

### **Performance Metrics**
**Latest OASIS CDR Prediction Results (608 subjects):**
- **XGBoost Model**: 80.7% test accuracy, 79.4% CV accuracy
- **Weighted F1-Score**: 0.804 (exceeding clinical benchmark)  
- **Feature Importance**: MMSE (17.5%), Age, Gender, Brain volumes (nWBV, eTIV)
- **Clinical Significance**: Approaches current diagnostic accuracy standards

### **Long-term Vision (1-3 years)**  
- [ ] **Global Research Network**: Federated learning across international cohorts
- [ ] **Regulatory Integration**: FDA/EMA-qualified analysis frameworks for drug approval
- [ ] **Real-world Evidence**: Continuous learning from clinical practice data
- [ ] **Precision Medicine**: AI-powered personalized treatment recommendations

### **Transformative Impact Goals**
- **10x Research Acceleration**: Reduce time from data to publication from years to months
- **1000x Cost Reduction**: Make advanced analysis accessible to every researcher globally  
- **Global Equity**: Enable developing countries to participate equally in Alzheimer's research
- **Clinical Translation**: Bridge the gap from research findings to patient care

## üèÜ Citation

If you use this framework in your research, please cite:

```
Agentic Alzheimer's Analyzer: An autonomous AI framework for ADRD research acceleration.
[Year]. Available at: [URL]
```

## üìÑ License

MIT License - See LICENSE file for details.

## ü§ù Contributing

We welcome contributions from researchers, developers, and domain experts worldwide! This project aims to democratize Alzheimer's research through collaborative open-source development.

### üöÄ How to Contribute

1. **Fork the repository** on GitHub
2. **Create a feature branch**: `git checkout -b feature/your-feature-name`
3. **Make your changes** following our guidelines
4. **Add tests and documentation** as needed
5. **Submit a pull request** with a clear description
6. **Engage in code review** with maintainers and community

### üéØ Contribution Areas

**High-Priority Needs:**
- üî• **Dataset Adapters**: Add support for ADNI, NACC, UK Biobank, and international cohorts
- üß† **Analysis Enhancements**: Longitudinal modeling, biomarker integration, advanced visualizations  
- üìä **Research Validation**: Cross-dataset validation, benchmark comparisons, clinical significance testing
- üìö **Documentation**: Tutorials, methodology guides, configuration examples

**üìã Detailed Guidelines**: See [CONTRIBUTING.md](CONTRIBUTING.md) for:
- Development workflow and branch naming conventions
- Code style guidelines and testing requirements
- Pull request process and review timeline
- Research contribution standards and clinical accuracy requirements
- Community guidelines and code of conduct

### üåç Community Impact

Every contribution helps:
- **Democratize Research**: Make advanced analysis accessible globally
- **Accelerate Discovery**: Reduce research timelines from years to months  
- **Improve Patient Outcomes**: Enable earlier detection and intervention
- **Foster Open Science**: Promote transparency and reproducibility

Together, we can build the world's largest open-source framework for Alzheimer's research acceleration.

---

## üí° The Vision: A World Without Alzheimer's

This framework represents more than just a technical tool - it's a catalyst for global research acceleration. By democratizing access to sophisticated AI-powered analysis, we can:

- **Accelerate Discovery**: Reduce research timelines from decades to years
- **Amplify Impact**: Enable every researcher worldwide to contribute meaningfully  
- **Ensure Equity**: Break down barriers between well-funded and resource-limited institutions
- **Save Lives**: Faster research means earlier interventions and better outcomes for millions

**Together, we can build the largest, most inclusive Alzheimer's research network in history.**

---

**üß† Empowering every researcher worldwide to accelerate the fight against Alzheimer's disease through autonomous AI.**

## üîê Data Policy
- This repository does not include any raw subject-level data.
- Local datasets should be placed under `training_data/` (which is git-ignored) following adapter expectations:
  - BRFSS CSVs under `training_data/brfss/`
  - OASIS CSVs under `training_data/oasis/`
- See adapters and docs for acquisition steps; do not commit raw data to the repository.

## üìÅ Repository Layout

- `agents/` ‚Äî discovery, analysis, literature agents
- `core/` ‚Äî orchestrator, token manager, datasets adapters
- `improvements/` ‚Äî feature engineering, clinical metrics, modeling utilities
- `tests/` ‚Äî unit/integration tests (non-interactive)
- `scripts/` ‚Äî utility scripts (e.g., ADNI debugging, downloader setup)
- `examples/` ‚Äî example analysis scripts and debugging helpers
- `docs/` ‚Äî `ARCHITECTURE.md`, `CONTRIBUTING.md`, improvements summaries
- `training_data/` ‚Äî placeholders; user-provided local data only (git-ignored)
- `outputs/` ‚Äî analysis outputs (git-ignored)

See `docs/ARCHITECTURE.md` and `docs/CONTRIBUTING.md` for architecture and contribution details.