#!/usr/bin/env python3
"""
Cognitive Analysis Agent
========================

Generalizable autonomous agent for analyzing relationships between cognitive assessments
in Alzheimer's research. Works with any cognitive assessment combination through 
configuration-driven analysis.

Key Features:
- Dataset agnostic: Works with any Alzheimer's dataset structure
- Assessment flexible: Supports any cognitive assessment combination
- Configuration-driven: All specifics defined in config files
- Extensible: Easy plugin architecture for custom analyses
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr, spearmanr, mannwhitneyu, ttest_ind
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
import json
import yaml
import os
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import logging
import warnings
warnings.filterwarnings('ignore')

# Import enhancement module if available
try:
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from cognitive_agent_enhancements import EnhancedCDRPredictor, integrate_enhancements
    ENHANCEMENTS_AVAILABLE = True
except ImportError:
    ENHANCEMENTS_AVAILABLE = False

# Import F1-focused clinical evaluation system
try:
    sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'improvements'))
    from clinical_evaluation_metrics import ClinicalEvaluator
    F1_EVALUATION_AVAILABLE = True
except ImportError:
    F1_EVALUATION_AVAILABLE = False

# Multiple-testing correction
try:
    from statsmodels.stats.multitest import multipletests
    STATSMODELS_AVAILABLE = True
except Exception:
    STATSMODELS_AVAILABLE = False

# Dataset adapters
try:
    sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'core'))
    from datasets import get_adapter
    ADAPTERS_AVAILABLE = True
except Exception:
    ADAPTERS_AVAILABLE = False

class CognitiveAnalysisAgent:
    """
    Generalizable analysis agent for cognitive assessment correlation studies
    
    This agent is completely dataset and assessment agnostic. It discovers
    cognitive assessments from your data based on configuration patterns
    and performs comprehensive analysis on any combination of:
    - Self-report questionnaires
    - Digital cognitive assessments  
    - Informant reports
    - Performance measures
    - Biomarkers
    
    All analysis types and variable mappings are defined in configuration
    files, making it easy to adapt to any Alzheimer's research dataset.
    """
    
    def __init__(self, config_path: str = "config/config.yaml",
                 data_dict_path: str = "config/data_dictionary.json",
                 discovery_results_path: str = "outputs/dataset_discovery_results.json"):
        
        self.config_path = config_path
        self.data_dict_path = data_dict_path
        self.discovery_results_path = discovery_results_path
        
        # Load configurations
        self.config = self._load_config()
        self.data_dict = self._load_data_dictionary()
        self.discovery_results = self._load_discovery_results()
        
        # Initialize data containers
        self.assessment_data = {}  # Dictionary of loaded assessment datasets
        self.combined_data = None
        
        # Setup logging
        self._setup_logging()
        
        # Get experiment configuration
        self.experiment_config = self.config.get('experiment', {})
        self.analysis_config = self.config.get('analysis', {})
        
        self.logger.info(f"üß† Cognitive Analysis Agent initialized for experiment: {self.experiment_config.get('name', 'Unknown')}")
    
    def _load_config(self) -> Dict[str, Any]:
        """Load main configuration file"""
        try:
            with open(self.config_path, 'r') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"Warning: Could not load config {self.config_path}: {e}")
            return {}
    
    def _load_data_dictionary(self) -> Dict[str, Any]:
        """Load data dictionary for variable mapping"""
        try:
            with open(self.data_dict_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Warning: Could not load data dictionary {self.data_dict_path}: {e}")
            return {}
    
    def _load_discovery_results(self) -> Dict[str, Any]:
        """Load dataset discovery results"""
        try:
            with open(self.discovery_results_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Warning: Could not load discovery results {self.discovery_results_path}: {e}")
            return {}
    
    def _setup_logging(self):
        """Setup logging system"""
        log_config = self.config.get('logging', {})
        log_level = getattr(logging, log_config.get('level', 'INFO').upper())
        
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Initialize F1-focused clinical evaluator
        self.clinical_evaluator = ClinicalEvaluator() if F1_EVALUATION_AVAILABLE else None
        if F1_EVALUATION_AVAILABLE:
            self.logger.info("‚úÖ F1-focused clinical evaluation system loaded")
    
    def run_complete_analysis(self) -> Dict[str, Any]:
        """Execute complete cognitive assessment analysis pipeline - adaptive for different data types"""
        experiment_name = self.experiment_config.get('name', 'Cognitive_Assessment_Analysis')
        self.logger.info(f"üöÄ Starting Cognitive Analysis Pipeline: {experiment_name}")
        
        analysis_results = {
            'analysis_info': {
                'agent_type': 'CognitiveAnalysisAgent',
                'experiment_name': experiment_name,
                'start_time': datetime.now().isoformat(),
                'config': self.config
            },
            'data_summary': {},
            'assessment_analysis': {},
            'correlation_analysis': {},
            'self_informant_comparison': {},
            'cognitive_performance_analysis': {},
            'clinical_insights': {},
            'statistical_summary': {}
        }
        
        try:
            # Step 1: Load and preprocess data
            self.logger.info("Step 1: Data loading and preprocessing")
            data_summary = self._load_and_preprocess_data()
            analysis_results['data_summary'] = data_summary
            
            if self.combined_data is None or len(self.combined_data) == 0:
                raise ValueError("No data available for analysis")
            
            # NEW: Detect data type and adapt analysis strategy
            data_type = self._detect_data_type()
            self.logger.info(f"   üîç Detected data type: {data_type}")
            
            if data_type == 'surveillance':
                # Run surveillance-specific analyses for population health data
                self.logger.info("   üìä Running surveillance data analysis pipeline")
                analysis_results = self._run_surveillance_analysis(analysis_results)
            else:
                # Original individual-level cognitive assessment analysis
                # Step 2: Assessment-specific analysis
                assessment_types = self._identify_assessment_types()
                for assessment_type in assessment_types:
                    self.logger.info(f"Step 2.{assessment_type}: {assessment_type} analysis")
                    analysis_results['assessment_analysis'][assessment_type] = self._analyze_assessment_type(assessment_type)
            
            if data_type != 'surveillance':
                # Step 3: Cross-assessment correlation analysis (for individual data only)
                self.logger.info("Step 3: Cross-assessment correlation analysis")
                analysis_results['correlation_analysis'] = self._analyze_cross_assessment_correlations()
            
            if data_type != 'surveillance':
                # Step 4: Self vs informant comparison (if available)
                if self._has_self_informant_data():
                    self.logger.info("Step 4: Self vs informant comparison")
                    analysis_results['self_informant_comparison'] = self._analyze_self_informant_differences()
                
                # Step 5: Cognitive performance analysis
                self.logger.info("Step 5: Cognitive performance analysis")
                analysis_results['cognitive_performance_analysis'] = self._analyze_cognitive_performance()
                
                # Step 5b: MemTrax predictive modeling for cognitive impairment
                if self._has_memtrax_and_outcomes_data():
                    self.logger.info("Step 5b: MemTrax cognitive impairment prediction analysis")
                    analysis_results['memtrax_prediction'] = self._analyze_memtrax_predictive_power()
                
                # Step 5c: Advanced CDR prediction (if CDR column exists)
                if 'CDR' in self.combined_data.columns:
                    self.logger.info("Step 5c: Advanced CDR prediction with state-of-the-art ML")
                    analysis_results['advanced_cdr_prediction'] = self._advanced_cdr_prediction()
            
            # Step 6: Generate clinical insights
            self.logger.info("Step 6: Clinical insights generation")
            analysis_results['clinical_insights'] = self._generate_clinical_insights(analysis_results)
            
            # Step 7: Statistical summary
            analysis_results['statistical_summary'] = self._generate_statistical_summary(analysis_results)
            
            # Step 8: Create visualizations
            self.logger.info("Step 8: Creating visualizations")
            self._create_analysis_visualizations(analysis_results)
            
            # Step 9: Save results
            analysis_results['analysis_info']['end_time'] = datetime.now().isoformat()
            self._save_analysis_results(analysis_results)
            
            self.logger.info("‚úÖ Cognitive analysis complete!")
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"Analysis failed: {e}")
            analysis_results['analysis_info']['error'] = str(e)
            analysis_results['analysis_info']['end_time'] = datetime.now().isoformat()
            return analysis_results
    
    def _load_and_preprocess_data(self) -> Dict[str, Any]:
        """Load and preprocess data via adapter if available; fallback to legacy OASIS flow."""
        data_summary = {
            'assessments_loaded': [],
            'total_subjects': 0,
            'baseline_subjects': 0,
            'preprocessing_steps': []
        }
        
        # Try dataset adapter first
        if ADAPTERS_AVAILABLE:
            try:
                adapter = get_adapter(self.config)
                if adapter and adapter.is_available():
                    self.logger.info(f"   üîå Using dataset adapter: {adapter.__class__.__name__}")
                    self.combined_data = adapter.load_combined()
                    summary = adapter.data_summary()
                    data_summary.update(summary)
                    # Done
                    return data_summary
                else:
                    self.logger.info("   ‚ÑπÔ∏è No suitable dataset adapter available; falling back to legacy loader")
            except Exception as e:
                self.logger.warning(f"   ‚ö†Ô∏è Adapter loading failed: {e}; using legacy loader")
        
        # Legacy OASIS loader (existing benchmark approach)
        try:
            data_path = "./training_data/oasis/"
            cross_df = pd.read_csv(f"{data_path}oasis_cross-sectional.csv")
            long_df = pd.read_csv(f"{data_path}oasis_longitudinal.csv")
            
            self.logger.info(f"   Loaded cross-sectional: {cross_df.shape}")
            self.logger.info(f"   Loaded longitudinal: {long_df.shape}")
            
            cross_df = cross_df.rename(columns={'ID': 'Subject_ID', 'M/F': 'Gender', 'Educ': 'EDUC'})
            long_df = long_df.rename(columns={'Subject ID': 'Subject_ID', 'M/F': 'Gender'})
            
            common_cols = list(set(cross_df.columns) & set(long_df.columns))
            self.logger.info(f"   Common columns: {len(common_cols)}")
            
            cross_common = cross_df[common_cols]
            long_common = long_df[common_cols]
            
            self.combined_data = pd.concat([cross_common, long_common], ignore_index=True)
            self.logger.info(f"   üîó Combined dataset: {self.combined_data.shape}")
            
            initial_subjects = len(self.combined_data)
            
            if 'CDR' in self.combined_data.columns:
                before_cdr = len(self.combined_data)
                self.combined_data = self.combined_data.dropna(subset=['CDR'])
                after_cdr = len(self.combined_data)
                self.logger.info(f"   üéØ Dropped {before_cdr - after_cdr} rows missing CDR: {after_cdr}/{before_cdr} subjects retained")
                cdr_distribution = self.combined_data['CDR'].value_counts().sort_index()
                self.logger.info(f"   üìà CDR distribution: {dict(cdr_distribution)}")
            
            if 'SES' in self.combined_data.columns and self.combined_data['SES'].isnull().any():
                from sklearn.impute import SimpleImputer
                mode_imputer = SimpleImputer(strategy='most_frequent')
                self.combined_data[['SES']] = mode_imputer.fit_transform(self.combined_data[['SES']])
                self.logger.info("   üîß Imputed SES missing values using mode")
                
            if 'MMSE' in self.combined_data.columns and self.combined_data['MMSE'].isnull().any():
                from sklearn.impute import SimpleImputer
                median_imputer = SimpleImputer(strategy='median')
                self.combined_data[['MMSE']] = median_imputer.fit_transform(self.combined_data[['MMSE']])
                self.logger.info("   üîß Imputed MMSE missing values using median")
            
            data_summary['assessments_loaded'] = [
                {'type': 'brain_imaging_data', 'files': 2, 'records': len(self.combined_data)},
                {'type': 'clinical_data', 'files': 2, 'records': len(self.combined_data)}
            ]
            data_summary['total_subjects'] = len(self.combined_data)
            data_summary['baseline_subjects'] = len(self.combined_data)
            data_summary['preprocessing_steps'] = [
                "Combined cross-sectional + longitudinal datasets (benchmark approach)",
                "Harmonized column names between datasets", 
                "Applied gentle imputation for missing values",
                f"Retained {len(self.combined_data)}/{initial_subjects} subjects"
            ]
            
            self.logger.info(f"   ‚úÖ BENCHMARK DATA LOADING: {len(self.combined_data)} subjects ready for analysis")
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Benchmark data loading failed: {e}")
            self.combined_data = pd.DataFrame()
            data_summary['error'] = str(e)
        
        return data_summary
    
    def _load_assessment_files(self, file_paths: List[str], assessment_type: str) -> pd.DataFrame:
        """Load files for a specific assessment type"""
        dataframes = []
        
        # Check if sampling is enabled
        use_sampling = self.config.get('analysis', {}).get('use_sampling', False)
        sample_size = self.config.get('analysis', {}).get('analysis_sample_size', 5000)
        
        for file_path in file_paths:
            try:
                # Load data with proper filtering
                df = pd.read_csv(file_path, low_memory=False)
                total_loaded = len(df)
                
                # Apply MemTrax-specific filtering (based on human scripts)
                if 'MemTrax' in os.path.basename(file_path) or assessment_type == 'cognitive_data':
                    # Filter to collected status only (as done in human scripts)
                    if 'Status' in df.columns:
                        df = df[df['Status'] == 'Collected'].copy()
                        self.logger.info(f"     MemTrax filtered: {len(df):,} collected of {total_loaded:,} total records")
                    
                    # Convert key numeric columns
                    numeric_cols = ['CorrectPCT', 'CorrectResponsesRT', 'CorrectResponsesPCT', 'CorrectRejectionsPCT']
                    for col in numeric_cols:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    # Convert test dates
                    if 'StatusDateTime' in df.columns:
                        df['test_date'] = pd.to_datetime(df['StatusDateTime'], errors='coerce')
                        df = df.dropna(subset=['test_date'])
                        self.logger.info(f"     MemTrax with valid dates: {len(df):,} records")
                
                # Apply sampling after filtering if needed
                if use_sampling and len(df) > sample_size:
                    df = df.sample(n=sample_size, random_state=42)
                    self.logger.info(f"     {assessment_type} file: {os.path.basename(file_path)} - {len(df):,} of {total_loaded:,} records (sampled after filtering)")
                else:
                    self.logger.info(f"     {assessment_type} file: {os.path.basename(file_path)} - {len(df):,} records")
                
                dataframes.append(df)
            except Exception as e:
                self.logger.warning(f"Could not load {file_path}: {e}")
        
        if dataframes:
            combined_df = pd.concat(dataframes, ignore_index=True)
            return combined_df
        return None
    
    def _combine_assessment_datasets(self) -> pd.DataFrame:
        """Combine all assessment datasets using common subject identifier"""
        # Find common subject identifier
        subject_id_candidates = ['SubjectCode', 'subject_id', 'participant_id', 'Code', 'ID']
        
        # Find the common subject column across all datasets
        common_subject_col = None
        for candidate in subject_id_candidates:
            if all(candidate in df.columns for df in self.assessment_data.values()):
                common_subject_col = candidate
                break
        
        if not common_subject_col:
            # Try to find any common column
            all_columns = [set(df.columns) for df in self.assessment_data.values()]
            common_columns = set.intersection(*all_columns)
            subject_columns = [col for col in common_columns if any(term in col.lower() for term in ['subject', 'id', 'code'])]
            if subject_columns:
                common_subject_col = subject_columns[0]
        
        if not common_subject_col:
            raise ValueError("Could not find common subject identifier across datasets")
        
        # Merge all datasets efficiently
        combined = None
        assessment_names = list(self.assessment_data.keys())
        
        # First, validate data structure and check for potential Cartesian joins
        subject_counts = {}
        duplicate_counts = {}
        
        for assessment_type, df in self.assessment_data.items():
            unique_subjects = df[common_subject_col].nunique()
            total_records = len(df)
            duplicate_records = total_records - unique_subjects
            
            subject_counts[assessment_type] = unique_subjects
            duplicate_counts[assessment_type] = duplicate_records
            
            self.logger.info(f"   {assessment_type}: {unique_subjects:,} unique subjects, {total_records:,} total records")
            
            if duplicate_records > 0:
                duplication_rate = (duplicate_records / total_records) * 100
                self.logger.warning(f"      ‚ö†Ô∏è {duplicate_records:,} duplicate subjects ({duplication_rate:.1f}% duplication rate)")
                
                if duplication_rate > 50:
                    self.logger.error(f"      üö® HIGH DUPLICATION RISK: {duplication_rate:.1f}% duplication could cause Cartesian joins")
        
        # Warn about potential Cartesian join risk
        total_possible_combinations = 1
        for count in subject_counts.values():
            total_possible_combinations *= count
            
        if total_possible_combinations > 1000000:  # 1M records
            self.logger.warning(f"   ‚ö†Ô∏è CARTESIAN JOIN RISK: Potential {total_possible_combinations:,} record combinations")
            self.logger.warning(f"   üí° Using inner joins to reduce risk, but verify subject ID consistency")
        
        # Merge datasets one by one with progress tracking and deduplication
        for i, (assessment_type, df) in enumerate(self.assessment_data.items()):
            if combined is None:
                # Start with first dataset, but deduplicate first
                combined = self._deduplicate_subjects(df, common_subject_col, assessment_type)
                self.logger.info(f"   Starting with {assessment_type}: {len(combined):,} records")
            else:
                before_merge = len(combined)
                
                # Deduplicate the merging dataset first
                df_deduplicated = self._deduplicate_subjects(df, common_subject_col, assessment_type)
                
                # Perform the merge
                combined = combined.merge(
                    df_deduplicated,
                    on=common_subject_col,
                    how='inner',
                    suffixes=('', f'_{assessment_type}')
                )
                after_merge = len(combined)
                self.logger.info(f"   Merged {assessment_type}: {before_merge:,} ‚Üí {after_merge:,} records")
                
                # Critical safety checks for Cartesian joins
                growth_factor = after_merge / before_merge if before_merge > 0 else 1
                
                if growth_factor > 10:
                    self.logger.error(f"   üö® CRITICAL ERROR: Cartesian join detected!")
                    self.logger.error(f"   üìä Data explosion: {before_merge:,} ‚Üí {after_merge:,} records ({growth_factor:.1f}x growth)")
                    self.logger.error(f"   üí° This indicates duplicate or mismatched subject IDs between datasets")
                    self.logger.error(f"   üõë Stopping merge to prevent memory explosion")
                    
                    # Log diagnostic information
                    self.logger.error(f"   üîç Debug info:")
                    self.logger.error(f"      - Common subject column: {common_subject_col}")
                    self.logger.error(f"      - Current dataset unique subjects: {combined[common_subject_col].nunique():,}")
                    self.logger.error(f"      - Merging dataset unique subjects: {df[common_subject_col].nunique():,}")
                    self.logger.error(f"      - Expected max records after merge: {combined[common_subject_col].nunique() * df[common_subject_col].nunique():,}")
                    
                    raise ValueError(f"Cartesian join detected: {growth_factor:.1f}x data explosion. Check subject ID consistency between datasets.")
                
                elif growth_factor > 2:
                    self.logger.warning(f"   ‚ö†Ô∏è Large merge growth detected: {growth_factor:.1f}x increase")
                    self.logger.warning(f"   üí° This may indicate data quality issues or multiple records per subject")
                    
                elif growth_factor < 0.1:
                    self.logger.warning(f"   ‚ö†Ô∏è Very few matches found: {after_merge:,} records from {before_merge:,}")
                    self.logger.warning(f"   üí° This may indicate mismatched subject IDs between datasets")
        
        self.logger.info(f"   üìä Combined dataset: {len(combined)} subjects with data from {len(self.assessment_data)} assessment types")
        return combined
    
    def _deduplicate_subjects(self, df: pd.DataFrame, subject_col: str, assessment_type: str) -> pd.DataFrame:
        """Deduplicate subjects in dataset, keeping most recent or complete record"""
        original_count = len(df)
        unique_subjects = df[subject_col].nunique()
        duplicate_count = original_count - unique_subjects
        
        if duplicate_count == 0:
            return df  # No duplicates
            
        self.logger.info(f"      üìä Deduplicating {assessment_type}: {original_count:,} ‚Üí {unique_subjects:,} records")
        
        # Strategy: Keep the most recent record for each subject (if date available)
        # Otherwise, keep the record with most complete data
        
        date_columns = [col for col in df.columns if any(date_term in col.lower() for date_term in ['date', 'time', 'created', 'updated', 'timestamp'])]
        
        if date_columns:
            # Use most recent record based on date
            date_col = date_columns[0]
            try:
                df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
                df_sorted = df.sort_values([subject_col, date_col], ascending=[True, False])
                deduplicated = df_sorted.groupby(subject_col).first().reset_index()
                self.logger.info(f"         ‚úÖ Deduplication by most recent {date_col}")
            except:
                # Fall back to completeness-based deduplication
                deduplicated = self._deduplicate_by_completeness(df, subject_col)
        else:
            # Use completeness-based deduplication
            deduplicated = self._deduplicate_by_completeness(df, subject_col)
            
        return deduplicated
    
    def _deduplicate_by_completeness(self, df: pd.DataFrame, subject_col: str) -> pd.DataFrame:
        """Keep the record with most complete data for each subject"""
        # Calculate completeness score for each record
        df['_completeness_score'] = df.notna().sum(axis=1)
        
        # Sort by subject and completeness score (highest first)
        df_sorted = df.sort_values([subject_col, '_completeness_score'], ascending=[True, False])
        
        # Keep first record for each subject (most complete)
        deduplicated = df_sorted.groupby(subject_col).first().reset_index()
        
        # Remove the temporary completeness score column
        if '_completeness_score' in deduplicated.columns:
            deduplicated = deduplicated.drop('_completeness_score', axis=1)
            
        self.logger.info(f"         ‚úÖ Deduplication by data completeness")
        return deduplicated
    
    def _detect_data_type(self) -> str:
        """Intelligently detect whether data is surveillance or individual assessment data"""
        # Check for surveillance-specific columns
        surveillance_indicators = ['LocationAbbr', 'LocationDesc', 'Data_Value', 'Topic', 
                                  'Class', 'Question', 'YearStart', 'YearEnd', 
                                  'Stratification1', 'StratificationCategory1']
        
        # Check for individual assessment columns
        individual_indicators = ['Subject_ID', 'subject_id', 'SubjectCode', 'ID', 
                                'MMSE', 'CDR', 'Age', 'Gender', 'EDUC', 
                                'MemTrax', 'ECOG', 'MoCA']
        
        if self.combined_data is None:
            return 'unknown'
        
        cols = self.combined_data.columns.tolist()
        
        # Count indicators
        surveillance_count = sum(1 for ind in surveillance_indicators if ind in cols)
        individual_count = sum(1 for ind in individual_indicators if ind in cols)
        
        # Decision logic
        if surveillance_count >= 5 and 'Data_Value' in cols:
            return 'surveillance'
        elif individual_count >= 3:
            return 'individual'
        elif 'Data_Value' in cols and 'Topic' in cols:
            return 'surveillance'
        else:
            # Default to individual if unclear
            return 'individual'
    
    def _identify_assessment_types(self) -> List[str]:
        """Identify what types of assessments are available in the data"""
        return list(self.assessment_data.keys())
    
    def _analyze_assessment_type(self, assessment_type: str) -> Dict[str, Any]:
        """Analyze a specific assessment type"""
        assessment_results = {
            'assessment_type': assessment_type,
            'descriptive_stats': {},
            'distribution_analysis': {},
            'quality_metrics': {}
        }
        
        # Get the data for this assessment type
        if assessment_type in self.assessment_data:
            df = self.assessment_data[assessment_type]
            
            # Find numeric columns (likely assessment scores)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # Calculate descriptive statistics
            for col in numeric_cols:
                col_data = df[col].dropna()
                if len(col_data) > 0:
                    assessment_results['descriptive_stats'][col] = {
                        'mean': float(col_data.mean()),
                        'median': float(col_data.median()),
                        'std': float(col_data.std()),
                        'min': float(col_data.min()),
                        'max': float(col_data.max()),
                        'sample_size': len(col_data),
                        'missing_percent': float((df[col].isna().sum() / len(df)) * 100)
                    }
        
        return assessment_results
    
    def _analyze_cross_assessment_correlations(self) -> Dict[str, Any]:
        """Analyze correlations between different assessment types"""
        correlation_results = {
            'primary_correlations': {},
            'assessment_pairs': {},
            'clinical_significance': {}
        }
        
        if self.combined_data is None:
            return correlation_results
        
        # Get numeric columns from each assessment type
        assessment_columns = {}
        for assessment_type in self.assessment_data.keys():
            # Find columns that likely belong to this assessment
            matching_cols = [col for col in self.combined_data.columns 
                           if assessment_type.lower() in col.lower() or
                           any(pattern in col.lower() for pattern in self._get_assessment_patterns(assessment_type))]
            assessment_columns[assessment_type] = [col for col in matching_cols 
                                                 if self.combined_data[col].dtype in ['int64', 'float64']]
        
        # Calculate correlations between assessment pairs
        assessment_types = list(assessment_columns.keys())
        for i, assessment_a in enumerate(assessment_types):
            for j, assessment_b in enumerate(assessment_types[i+1:], i+1):
                pair_key = f"{assessment_a}_vs_{assessment_b}"
                correlation_results['assessment_pairs'][pair_key] = self._calculate_assessment_pair_correlations(
                    assessment_columns[assessment_a], assessment_columns[assessment_b], pair_key
                )
        
        # Flatten correlations for primary results
        for pair_key, pair_results in correlation_results['assessment_pairs'].items():
            correlation_results['primary_correlations'].update(pair_results)
        
        # Multiple testing correction (FDR Benjamini‚ÄìHochberg)
        try:
            pvals = [d.get('p_value', 1.0) for d in correlation_results['primary_correlations'].values()]
            keys = list(correlation_results['primary_correlations'].keys())
            if STATSMODELS_AVAILABLE and len(pvals) > 0:
                reject, pvals_corrected, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')
                for k, adj_p, rej in zip(keys, pvals_corrected, reject):
                    correlation_results['primary_correlations'][k]['p_value_fdr'] = float(adj_p)
                    correlation_results['primary_correlations'][k]['significant_fdr'] = bool(rej)
            else:
                # If statsmodels not available, copy unadjusted to adjusted for transparency
                for k in keys:
                    correlation_results['primary_correlations'][k]['p_value_fdr'] = correlation_results['primary_correlations'][k].get('p_value', 1.0)
                    correlation_results['primary_correlations'][k]['significant_fdr'] = correlation_results['primary_correlations'][k].get('p_value', 1.0) < 0.05
        except Exception:
            pass
        
        # Calculate clinical significance
        significant_correlations = sum(1 for corr_data in correlation_results['primary_correlations'].values()
                                     if corr_data.get('p_value_fdr', corr_data.get('p_value', 1)) < 0.05)
        total_correlations = len(correlation_results['primary_correlations'])
        
        if total_correlations > 0:
            correlation_results['clinical_significance'] = {
                'significant_correlations': significant_correlations,
                'total_correlations': total_correlations,
                'significance_rate': significant_correlations / total_correlations,
                'multiple_comparison_threshold': 0.05 / total_correlations,
                'adjustment': 'fdr_bh' if STATSMODELS_AVAILABLE else 'none'
            }
        
        return correlation_results
    
    def _get_assessment_patterns(self, assessment_type: str) -> List[str]:
        """Get search patterns for a specific assessment type"""
        # This should ideally come from configuration
        pattern_map = {
            'cognitive_data': ['memtrax', 'cognitive', 'memory', 'attention', 'rt', 'reaction', 'correct'],
            'ecog_data': ['ecog', 'everyday', 'qid', 'self', 'informant'],
            'demographic_data': ['age', 'education', 'gender', 'demo'],
            'medical_data': ['medical', 'diagnosis', 'condition']
        }
        return pattern_map.get(assessment_type, [assessment_type.lower()])
    
    def _calculate_assessment_pair_correlations(self, cols_a: List[str], cols_b: List[str], pair_key: str) -> Dict[str, Any]:
        """Calculate correlations between two assessment types"""
        pair_correlations = {}
        
        # Intelligent correlation limit based on computational feasibility
        max_cols_per_group = self._calculate_correlation_limit(len(cols_a), len(cols_b))
        
        for col_a in cols_a[:max_cols_per_group]:
            for col_b in cols_b[:max_cols_per_group]:
                try:
                    data_a = self.combined_data[col_a].dropna()
                    data_b = self.combined_data[col_b].dropna()
                    
                    # Get common indices
                    common_indices = data_a.index.intersection(data_b.index)
                    if len(common_indices) > 20:  # Minimum sample size
                        common_a = data_a[common_indices]
                        common_b = data_b[common_indices]
                        
                        corr_coef, corr_p = pearsonr(common_a, common_b)
                        
                        correlation_key = f"{col_a}_vs_{col_b}"
                        pair_correlations[correlation_key] = {
                            'correlation_coefficient': corr_coef,
                            'p_value': corr_p,
                            'sample_size': len(common_indices),
                            'effect_size': self._interpret_correlation_effect_size(abs(corr_coef)),
                            'assessment_pair': pair_key
                        }
                except Exception:
                    continue
        
        return pair_correlations
    
    def _has_self_informant_data(self) -> bool:
        """Check if dataset has both self and informant report data"""
        if self.combined_data is None:
            return False
        
        # Look for columns that might represent self vs informant reports
        self_indicators = ['self', 'participant', 'subject']
        informant_indicators = ['informant', 'partner', 'caregiver', 'family']
        
        columns = [col.lower() for col in self.combined_data.columns]
        
        has_self = any(any(indicator in col for indicator in self_indicators) for col in columns)
        has_informant = any(any(indicator in col for indicator in informant_indicators) for col in columns)
        
        return has_self and has_informant
    
    def _analyze_self_informant_differences(self) -> Dict[str, Any]:
        """Analyze differences between self and informant reports (generic)"""
        self_informant_results = {
            'self_informant_available': True,
            'correlation_analysis': {},
            'discrepancy_analysis': {},
            'domain_analysis': {}
        }
        
        # Find self and informant columns
        all_columns = self.combined_data.columns.tolist()
        
        self_cols = [col for col in all_columns if any(term in col.lower() for term in ['self', 'participant'])]
        informant_cols = [col for col in all_columns if any(term in col.lower() for term in ['informant', 'partner', 'caregiver'])]
        
        # Filter to numeric columns only
        numeric_self_cols = [col for col in self_cols if self.combined_data[col].dtype in ['int64', 'float64']]
        numeric_informant_cols = [col for col in informant_cols if self.combined_data[col].dtype in ['int64', 'float64']]
        
        if len(numeric_self_cols) > 0 and len(numeric_informant_cols) > 0:
            # Calculate correlation between self and informant total scores
            if len(numeric_self_cols) > 1:
                self_total = self.combined_data[numeric_self_cols].sum(axis=1, skipna=False)
            else:
                self_total = self.combined_data[numeric_self_cols[0]]
            
            if len(numeric_informant_cols) > 1:
                informant_total = self.combined_data[numeric_informant_cols].sum(axis=1, skipna=False)
            else:
                informant_total = self.combined_data[numeric_informant_cols[0]]
            
            # Correlation analysis
            valid_data = (~self_total.isna()) & (~informant_total.isna())
            if valid_data.sum() > 10:
                corr_coef, corr_p = pearsonr(self_total[valid_data], informant_total[valid_data])
                self_informant_results['correlation_analysis'] = {
                    'correlation_coefficient': corr_coef,
                    'p_value': corr_p,
                    'sample_size': int(valid_data.sum())
                }
            
            # Discrepancy analysis
            discrepancy = informant_total - self_total
            self_informant_results['discrepancy_analysis'] = {
                'mean_discrepancy': float(discrepancy.mean()),
                'std_discrepancy': float(discrepancy.std()),
                'positive_discrepancy_percent': float((discrepancy > 0).mean() * 100),
                'large_discrepancy_percent': float((abs(discrepancy) > discrepancy.std()).mean() * 100)
            }
        else:
            self_informant_results['self_informant_available'] = False
            self.logger.warning("   ‚ö†Ô∏è Could not identify both self and informant measures")
        
        return self_informant_results
    
    def _analyze_cognitive_performance(self) -> Dict[str, Any]:
        """Analyze cognitive performance patterns (generic)"""
        cognitive_results = {
            'performance_metrics': {},
            'distribution_analysis': {},
            'efficiency_analysis': {}
        }
        
        if self.combined_data is None:
            return cognitive_results
        
        # Find performance-related columns
        performance_indicators = ['rt', 'reaction_time', 'response_time', 'accuracy', 'correct', 'percent', 'score']
        performance_cols = []
        
        for col in self.combined_data.columns:
            if any(indicator in col.lower() for indicator in performance_indicators):
                if self.combined_data[col].dtype in ['int64', 'float64']:
                    performance_cols.append(col)
        
        # Analyze each performance metric
        for col in performance_cols:
            col_data = self.combined_data[col].dropna()
            if len(col_data) > 0:
                cognitive_results['performance_metrics'][col] = {
                    'mean': float(col_data.mean()),
                    'median': float(col_data.median()),
                    'std': float(col_data.std()),
                    'min': float(col_data.min()),
                    'max': float(col_data.max()),
                    'sample_size': len(col_data)
                }
        
        return cognitive_results
    
    def _get_baseline_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Filter to baseline timepoints only"""
        timepoint_cols = ['TimepointCode', 'timepoint', 'visit', 'session']
        baseline_values = ['m00', 'baseline', '0', 'visit_1', 'session_1', 'bl']
        
        for col in timepoint_cols:
            if col in df.columns:
                for baseline_val in baseline_values:
                    baseline_data = df[df[col] == baseline_val]
                    if len(baseline_data) > 0:
                        self.logger.info(f"   üìÖ Using baseline timepoint: {col} == {baseline_val}")
                        return baseline_data
        
        self.logger.info("   üìÖ No timepoint column found, using all data as baseline")
        return df
    
    def _clean_and_validate_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and validate the dataset using notebook's gentle approach"""
        original_size = len(df)
        
        # Use notebook's approach: only drop rows missing target variable (CDR), impute the rest
        before_missing = len(df)
        
        # Only drop rows missing CDR (our target variable)  
        if 'CDR' in df.columns:
            initial_cdr_missing = df['CDR'].isnull().sum()
            self.logger.info(f"   üìä CDR missing analysis: {initial_cdr_missing}/{len(df)} rows missing CDR values")
            
            # Debug: show CDR value distribution BEFORE dropping
            cdr_distribution = df['CDR'].value_counts().sort_index()
            self.logger.info(f"   üìà CDR distribution before cleanup: {dict(cdr_distribution)}")
            
            if initial_cdr_missing > 0:
                df = df.dropna(subset=['CDR'])
                self.logger.info(f"   üéØ Dropped {initial_cdr_missing} rows missing CDR (target variable)")
                
                # Show final CDR distribution
                final_cdr_distribution = df['CDR'].value_counts().sort_index()
                self.logger.info(f"   üìà Final CDR distribution: {dict(final_cdr_distribution)}")
        else:
            self.logger.warning("   ‚ùå No CDR column found in dataset")
        
        # Apply notebook's imputation strategy for other missing values
        if 'SES' in df.columns and df['SES'].isnull().any():
            from sklearn.impute import SimpleImputer
            mode_imputer = SimpleImputer(strategy='most_frequent')
            df[['SES']] = mode_imputer.fit_transform(df[['SES']])
            self.logger.info("   üîß Imputed SES missing values using mode")
            
        if 'MMSE' in df.columns and df['MMSE'].isnull().any():
            from sklearn.impute import SimpleImputer
            median_imputer = SimpleImputer(strategy='median')
            df[['MMSE']] = median_imputer.fit_transform(df[['MMSE']])
            self.logger.info("   üîß Imputed MMSE missing values using median")
        
        self.logger.info(f"   ‚úÖ Gentle data cleaning: {len(df)}/{before_missing} subjects retained (notebook approach)")
        return df
    
    def _interpret_correlation_effect_size(self, correlation: float) -> str:
        """Interpret correlation coefficient effect size"""
        if correlation >= 0.7:
            return "large"
        elif correlation >= 0.5:
            return "medium"
        elif correlation >= 0.3:
            return "small"
        else:
            return "negligible"
    
    def _generate_clinical_insights(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate clinical insights from analysis results"""
        insights = {
            'key_findings': [],
            'clinical_implications': [],
            'novel_discoveries': [],
            'limitations': []
        }
        
        # Analyze correlation findings
        correlations = analysis_results.get('correlation_analysis', {}).get('primary_correlations', {})
        significant_corrs = [name for name, data in correlations.items() 
                           if data.get('p_value_fdr', data.get('p_value', 1)) < 0.05]
        
        if significant_corrs:
            insights['key_findings'].append(
                f"Found {len(significant_corrs)} significant cross-assessment correlations (FDR-adjusted)"
            )
        
        # Check for novel findings
        clinical_sig = analysis_results.get('correlation_analysis', {}).get('clinical_significance', {})
        if clinical_sig.get('significance_rate', 0) > 0.5:
            insights['novel_discoveries'].append(
                "High rate of significant correlations suggests strong cross-assessment relationships"
            )
        
        # Add limitations
        data_summary = analysis_results.get('data_summary', {})
        sample_size = data_summary.get('baseline_subjects', 0)
        
        if sample_size < 100:
            insights['limitations'].append("Small sample size may limit generalizability")
        
        insights['limitations'].append("Cross-sectional analysis - causality cannot be determined")
        
        return insights
    
    def _generate_statistical_summary(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive statistical summary"""
        summary = {
            'sample_characteristics': {},
            'primary_findings': {},
            'recommendations': []
        }
        
        # Sample characteristics
        data_summary = analysis_results.get('data_summary', {})
        summary['sample_characteristics'] = {
            'total_subjects': data_summary.get('baseline_subjects', 0),
            'assessments_analyzed': len(data_summary.get('assessments_loaded', [])),
            'data_completeness': 'Good' if data_summary.get('baseline_subjects', 0) > 100 else 'Limited'
        }
        
        # Primary findings
        correlations = analysis_results.get('correlation_analysis', {}).get('primary_correlations', {})
        significant_correlations = [name for name, data in correlations.items() 
                                 if data.get('p_value_fdr', data.get('p_value', 1)) < 0.05]
        
        summary['primary_findings'] = {
            'significant_cross_assessment_correlations': len(significant_correlations),
            'total_correlations_tested': len(correlations)
        }
        
        # Recommendations
        if len(significant_correlations) > 0:
            summary['recommendations'].append("Strong evidence for cross-assessment relationships (FDR-adjusted)")
            summary['recommendations'].append("Consider validation in independent sample")
        
        if data_summary.get('baseline_subjects', 0) < 200:
            summary['recommendations'].append("Increase sample size for more robust findings")
        
        summary['recommendations'].append("Explore longitudinal changes in assessment relationships")
        
        return summary
    
    def _create_analysis_visualizations(self, analysis_results: Dict[str, Any]):
        """Create visualizations automatically based on analysis type and available data"""
        self.logger.info("   üé® Creating visualizations...")
        
        os.makedirs('outputs/visualizations', exist_ok=True)
        
        try:
            # Detect analysis type and create appropriate visualizations
            if self._is_surveillance_analysis(analysis_results):
                self.logger.info("   üìä Creating surveillance analysis visualizations...")
                self._create_surveillance_visualizations(analysis_results)
            else:
                self.logger.info("   üß† Creating individual assessment visualizations...")
                self._create_assessment_visualizations(analysis_results)
            
        except Exception as e:
            self.logger.warning(f"Error creating visualizations: {e}")
    
    def _is_surveillance_analysis(self, analysis_results: Dict[str, Any]) -> bool:
        """Detect if this is surveillance vs individual assessment analysis"""
        return (analysis_results.get('temporal_analysis') is not None or 
                analysis_results.get('geographic_analysis') is not None)
    
    def _create_surveillance_visualizations(self, analysis_results: Dict[str, Any]):
        """Create surveillance-specific visualizations"""
        
        # 1. Temporal trends (if available)
        temporal_analysis = analysis_results.get('temporal_analysis', {})
        if temporal_analysis:
            self._create_temporal_trends_plot(temporal_analysis)
        
        # 2. Geographic patterns (if available)
        geographic_analysis = analysis_results.get('geographic_analysis', {})
        if geographic_analysis:
            self._create_geographic_patterns_plot(geographic_analysis)
        
        # 3. Health topic analysis (if available)
        topic_analysis = analysis_results.get('topic_analysis', {})
        if topic_analysis:
            self._create_topic_analysis_plot(topic_analysis)
        
        # 4. Risk stratification (if available)
        risk_scores = analysis_results.get('risk_scores', {})
        if risk_scores:
            self._create_risk_stratification_plot(risk_scores)
        
        # 5. Surveillance dashboard
        self._create_surveillance_dashboard(analysis_results)
    
    def _create_assessment_visualizations(self, analysis_results: Dict[str, Any]):
        """Create individual assessment-specific visualizations"""
        
        # Create correlation matrix visualization (if available)
        correlations = analysis_results.get('correlation_analysis', {}).get('primary_correlations', {})
        if correlations:
            self._create_correlation_matrix_plot(analysis_results)
        
        # Create self-informant comparison plot (if available)
        if analysis_results.get('self_informant_comparison', {}).get('self_informant_available'):
            self._create_self_informant_plot(analysis_results)
        
        # Create performance distribution plots (if data available)
        self._create_performance_distribution_plots(analysis_results)
    
    def _create_correlation_matrix_plot(self, analysis_results: Dict[str, Any]):
        """Create correlation matrix heatmap"""
        correlations = analysis_results.get('correlation_analysis', {}).get('primary_correlations', {})
        
        if not correlations:
            return
        
        # Extract correlation data
        corr_names = []
        corr_values = []
        p_values = []
        p_values_fdr = []
        
        for name, data in correlations.items():
            # Clean up names for display
            display_name = name.replace('_vs_', ' vs ').replace('_', ' ')
            if len(display_name) > 40:  # Truncate long names
                display_name = display_name[:37] + "..."
            corr_names.append(display_name)
            corr_values.append(data.get('correlation_coefficient', 0))
            p_values.append(data.get('p_value', 1))
            p_values_fdr.append(data.get('p_value_fdr', data.get('p_value', 1)))
        
        if len(corr_values) > 0:
            fig, ax = plt.subplots(figsize=(12, max(6, len(corr_names) * 0.3)))
            
            # Create horizontal bar plot
            colors = ['red' if p < 0.05 else 'lightblue' for p in p_values_fdr]
            bars = ax.barh(range(len(corr_values)), corr_values, color=colors, alpha=0.7)
            
            ax.set_yticks(range(len(corr_names)))
            ax.set_yticklabels(corr_names, fontsize=9)
            ax.set_xlabel('Correlation Coefficient')
            ax.set_title('Cross-Assessment Correlations (FDR-adjusted)', fontweight='bold', fontsize=14)
            ax.grid(axis='x', alpha=0.3)
            ax.axvline(x=0, color='black', linestyle='-', alpha=0.5)
            
            # Add significance indicators
            for i, (bar, p_val) in enumerate(zip(bars, p_values_fdr)):
                if p_val < 0.05:
                    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
                           '*', fontsize=12, fontweight='bold', va='center')
            
            plt.tight_layout()
            plt.savefig('outputs/visualizations/cross_assessment_correlations.png', dpi=300, bbox_inches='tight')
            plt.close()
    
    def _create_self_informant_plot(self, analysis_results: Dict[str, Any]):
        """Create self vs informant comparison plot"""
        self_informant = analysis_results.get('self_informant_comparison', {})
        
        if not self_informant.get('self_informant_available'):
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Plot 1: Correlation info
        corr_data = self_informant.get('correlation_analysis', {})
        if corr_data:
            correlation = corr_data.get('correlation_coefficient', 0)
            p_value = corr_data.get('p_value', 1)
            
            ax1.text(0.5, 0.6, f'Correlation: r = {correlation:.3f}', 
                    ha='center', va='center', transform=ax1.transAxes, fontsize=14)
            ax1.text(0.5, 0.4, f'p-value: {p_value:.4f}', 
                    ha='center', va='center', transform=ax1.transAxes, fontsize=12)
            
            significance = "Significant" if p_value < 0.05 else "Not Significant"
            ax1.text(0.5, 0.2, significance, 
                    ha='center', va='center', transform=ax1.transAxes, 
                    fontsize=12, fontweight='bold',
                    color='green' if p_value < 0.05 else 'red')
        
        ax1.set_title('Self-Informant Correlation', fontweight='bold')
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)
        ax1.axis('off')
        
        # Plot 2: Discrepancy analysis
        discrepancy = self_informant.get('discrepancy_analysis', {})
        if discrepancy:
            pos_disc_pct = discrepancy.get('positive_discrepancy_percent', 50)
            
            categories = ['Informant Higher', 'Agreement', 'Self Higher']
            # Simple approximation for visualization
            percentages = [pos_disc_pct, 20, 100 - pos_disc_pct - 20]
            colors = ['orange', 'green', 'blue']
            
            ax2.pie(percentages, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)
            ax2.set_title('Report Discrepancy Patterns', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/self_informant_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_performance_distribution_plots(self, analysis_results: Dict[str, Any]):
        """Create cognitive performance distribution plots"""
        cognitive = analysis_results.get('cognitive_performance_analysis', {})
        performance_metrics = cognitive.get('performance_metrics', {})
        
        if not performance_metrics:
            return
        
        # Intelligent metrics selection based on available data
        all_metrics = list(performance_metrics.keys())
        max_displayable_metrics = self._calculate_optimal_display_count(len(all_metrics), 'performance_metrics')
        metrics = all_metrics[:max_displayable_metrics]
        if not metrics:
            return
        
        cols = min(3, len(metrics))
        rows = (len(metrics) + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
        if len(metrics) == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes if len(metrics) > 1 else [axes]
        else:
            axes = axes.flatten()
        
        for i, metric in enumerate(metrics):
            if i < len(axes):
                # Create histogram for this metric
                if metric in self.combined_data.columns:
                    data = self.combined_data[metric].dropna()
                    if len(data) > 0:
                        axes[i].hist(data, bins=30, alpha=0.7, edgecolor='black')
                        axes[i].set_xlabel(metric.replace('_', ' ').title())
                        axes[i].set_ylabel('Frequency')
                        axes[i].set_title(f'{metric.replace("_", " ").title()} Distribution')
                        axes[i].grid(alpha=0.3)
        
        # Hide empty subplots
        for i in range(len(metrics), len(axes)):
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/cognitive_performance_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _save_analysis_results(self, results: Dict[str, Any]):
        """Save analysis results to file"""
        output_file = "outputs/cognitive_analysis_results.json"
        
        try:
            # Make results JSON serializable
            serializable_results = self._make_serializable(results)
            
            with open(output_file, 'w') as f:
                json.dump(serializable_results, f, indent=2)
            
            self.logger.info(f"üìÅ Analysis results saved to: {output_file}")
            
        except Exception as e:
            self.logger.error(f"Error saving results: {e}")
    
    def _make_serializable(self, obj):
        """Convert non-serializable objects for JSON"""
        try:
            import numpy as np
            import pandas as pd
        except Exception:
            np = None
            pd = None
        if isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        # datetime-like
        elif hasattr(obj, 'isoformat'):
            return obj.isoformat()
        # numpy types
        if np is not None:
            if isinstance(obj, (np.integer,)):
                return int(obj)
            if isinstance(obj, (np.floating,)):
                return float(obj)
            if isinstance(obj, (np.bool_,)):
                return bool(obj)
            if isinstance(obj, (np.ndarray,)):
                return obj.tolist()
            if isinstance(obj, np.dtype):
                return str(obj)
        # pandas types
        if pd is not None:
            if isinstance(obj, pd.Timestamp):
                return obj.isoformat()
            if isinstance(obj, pd.Series):
                return obj.to_list()
            if isinstance(obj, pd.DataFrame):
                return obj.to_dict(orient='list')
        # pandas/numpy missing
        try:
            if pd is not None and pd.isna(obj):
                return None
        except Exception:
            pass
        return obj
    
    def print_analysis_summary(self, results: Dict[str, Any]):
        """Print formatted analysis summary"""
        print("\n" + "="*80)
        print("üß† COGNITIVE ANALYSIS SUMMARY")
        print("="*80)
        
        # Analysis info
        analysis_info = results.get('analysis_info', {})
        experiment_name = analysis_info.get('experiment_name', 'Unknown')
        print(f"\nüìä EXPERIMENT: {experiment_name}")
        
        # Data summary
        data_summary = results.get('data_summary', {})
        print(f"\nüìä DATA SUMMARY:")
        print(f"   Total subjects analyzed: {data_summary.get('baseline_subjects', 0):,}")
        
        assessments = data_summary.get('assessments_loaded', [])
        print(f"   Assessment types loaded: {len(assessments)}")
        for assessment in assessments:
            print(f"   - {assessment.get('type', 'Unknown')}: {assessment.get('files', 0)} files, {assessment.get('records', 0)} records")
        
        # Correlation findings
        correlations = results.get('correlation_analysis', {}).get('primary_correlations', {})
        if correlations:
            significant_corrs = [name for name, data in correlations.items() 
                               if data.get('p_value_fdr', data.get('p_value', 1)) < 0.05]
            
            print(f"\nüîó CORRELATION ANALYSIS:")
            print(f"   Total correlations tested: {len(correlations)}")
            print(f"   Significant correlations: {len(significant_corrs)}")
            
            if significant_corrs:
                print(f"\n   üìà TOP SIGNIFICANT CORRELATIONS:")
                # Show top 5 strongest correlations
                sorted_corrs = sorted(significant_corrs, 
                                    key=lambda x: abs(correlations[x].get('correlation_coefficient', 0)), 
                                    reverse=True)[:5]
                for corr_name in sorted_corrs:
                    corr_data = correlations[corr_name]
                    r = corr_data.get('correlation_coefficient', 0)
                    p = corr_data.get('p_value_fdr', corr_data.get('p_value', 1))
                    n = corr_data.get('sample_size', 0)
                    effect = corr_data.get('effect_size', 'unknown')
                    print(f"      {corr_name}: r={r:.3f}, p={p:.4f}, n={n}, effect={effect}")
        
        # Self-informant analysis
        self_informant = results.get('self_informant_comparison', {})
        if self_informant.get('self_informant_available'):
            print(f"\nüë• SELF-INFORMANT ANALYSIS:")
            corr_analysis = self_informant.get('correlation_analysis', {})
            if corr_analysis:
                r = corr_analysis.get('correlation_coefficient', 0)
                p = corr_analysis.get('p_value', 1)
                print(f"   Self-informant correlation: r={r:.3f}, p={p:.4f}")
        
        # F1-FOCUSED CDR Prediction Results
        cdr_prediction = results.get('advanced_cdr_prediction', {})
        if cdr_prediction and 'best_model' in cdr_prediction:
            best_model = cdr_prediction['best_model']
            if best_model:
                print(f"\nüéØ F1-FOCUSED CDR PREDICTION RESULTS:")
                print(f"   ü•á Best Model: {best_model.get('name', 'Unknown')}")
                
                # Prioritize F1 scores in display
                if 'test_f1_weighted' in best_model:
                    print(f"   üî• F1-Score (Weighted): {best_model['test_f1_weighted']:.3f}")
                    print(f"   üî• F1-Score (Macro): {best_model['test_f1_macro']:.3f}")
                    print(f"   üî• Precision (Weighted): {best_model['test_precision']:.3f}")
                    print(f"   üî• Recall (Weighted): {best_model['test_recall']:.3f}")
                    print(f"   üìà Accuracy: {best_model['test_accuracy']:.3f}")
                
                # Clinical acceptability
                test_f1 = best_model.get('test_f1_weighted', 0)
                test_precision = best_model.get('test_precision', 0)
                test_recall = best_model.get('test_recall', 0)
                clinically_acceptable = test_f1 >= 0.75 and test_precision >= 0.75 and test_recall >= 0.75
                print(f"   üè• Clinical Acceptability: {'‚úÖ APPROVED' if clinically_acceptable else '‚ùå NEEDS IMPROVEMENT'}")
                
                # F1-focused insights
                cdr_insights = cdr_prediction.get('clinical_insights', [])
                if cdr_insights:
                    print(f"   üí° F1-Focused Clinical Insights:")
                    for insight in cdr_insights[:3]:  # Show top 3 insights
                        # Clean up insight formatting for display
                        clean_insight = insight.replace('üèÜ ', '').replace('‚úÖ ', '').replace('üéØ ', '').replace('‚ö†Ô∏è ', '')
                        print(f"      ‚Ä¢ {clean_insight}")

        # General Clinical insights
        insights = results.get('clinical_insights', {})
        key_findings = insights.get('key_findings', [])
        if key_findings:
            print(f"\nüí° ADDITIONAL KEY FINDINGS:")
            for i, finding in enumerate(key_findings, 1):
                print(f"   {i}. {finding}")
        
        # Recommendations
        statistical_summary = results.get('statistical_summary', {})
        recommendations = statistical_summary.get('recommendations', [])
        if recommendations:
            print(f"\nüìã RECOMMENDATIONS:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print("\n" + "="*80)
        print("‚úÖ Cognitive analysis complete!")
        print("üìÅ Results saved to: outputs/cognitive_analysis_results.json")
        print("üé® Visualizations saved to: outputs/visualizations/")
        print("="*80)
    
    def _has_memtrax_and_outcomes_data(self) -> bool:
        """Check if we have MemTrax data and cognitive outcome variables for prediction"""
        if self.combined_data is None or self.combined_data.empty:
            return False
            
        # Check for MemTrax variables
        memtrax_vars = [col for col in self.combined_data.columns if any(pattern.lower() in col.lower() for pattern in ['memtrax', 'reaction_time', 'response_time', 'correctresponsesrt', 'correctpct', 'accuracy'])]
        
        # Check for cognitive outcome variables  
        outcome_vars = [col for col in self.combined_data.columns if any(pattern.lower() in col.lower() for pattern in ['diagnosis', 'cognitive_status', 'impairment', 'dementia', 'mci', 'cdr', 'mmse', 'moca'])]
        
        has_memtrax = len(memtrax_vars) > 0
        has_outcomes = len(outcome_vars) > 0
        
        self.logger.info(f"   MemTrax variables found: {len(memtrax_vars)}")  
        self.logger.info(f"   Outcome variables found: {len(outcome_vars)}")
        
        return has_memtrax and has_outcomes
    
    def _analyze_memtrax_predictive_power(self) -> Dict[str, Any]:
        """Analyze MemTrax predictive power for cognitive impairment detection"""
        prediction_results = {
            'analysis_type': 'memtrax_cognitive_impairment_prediction',
            'memtrax_variables': [],
            'outcome_variables': [],
            'model_performance': {},
            'predictive_insights': []
        }
        
        try:
            # Find MemTrax and outcome variables
            memtrax_vars = [col for col in self.combined_data.columns if any(pattern.lower() in col.lower() for pattern in ['memtrax', 'correctresponsesrt', 'correctpct', 'reaction_time', 'accuracy'])]
            outcome_vars = [col for col in self.combined_data.columns if any(pattern.lower() in col.lower() for pattern in ['diagnosis', 'cognitive_status', 'cdr', 'mmse', 'moca', 'impairment'])]
            
            if not memtrax_vars or not outcome_vars:
                prediction_results['error'] = 'Insufficient variables for predictive modeling'
                return prediction_results
            
            prediction_results['memtrax_variables'] = memtrax_vars
            prediction_results['outcome_variables'] = outcome_vars
            
            # Analyze each outcome variable
            for outcome_var in outcome_vars[:2]:  # Limit to prevent excessive analysis
                self.logger.info(f"   Analyzing MemTrax prediction for: {outcome_var}")
                
                # Get clean data for this analysis
                analysis_data = self.combined_data[memtrax_vars + [outcome_var]].dropna()
                
                if len(analysis_data) < 50:  # Need minimum sample
                    continue
                
                # Correlation-based predictive analysis
                outcome_analysis = self._analyze_memtrax_outcome_relationship(analysis_data, memtrax_vars, outcome_var)
                prediction_results['model_performance'][outcome_var] = outcome_analysis
            
            # Generate predictive insights
            prediction_results['predictive_insights'] = self._generate_memtrax_predictive_insights(prediction_results)
            
        except Exception as e:
            prediction_results['error'] = f"MemTrax prediction analysis failed: {str(e)}"
            self.logger.error(f"MemTrax prediction analysis error: {e}")
        
        return prediction_results
    
    def _analyze_memtrax_outcome_relationship(self, data: pd.DataFrame, memtrax_vars: List[str], outcome_var: str) -> Dict[str, Any]:
        """Analyze relationship between MemTrax variables and cognitive outcome"""
        relationship_analysis = {
            'outcome_variable': outcome_var,
            'sample_size': len(data),
            'memtrax_correlations': {},
            'clinical_significance': 'unknown'
        }
        
        try:
            # Calculate correlations between MemTrax variables and outcome
            for memtrax_var in memtrax_vars:
                if memtrax_var in data.columns and outcome_var in data.columns:
                    correlation = data[memtrax_var].corr(pd.to_numeric(data[outcome_var], errors='coerce'))
                    
                    if not np.isnan(correlation):
                        relationship_analysis['memtrax_correlations'][memtrax_var] = {
                            'correlation': float(correlation),
                            'abs_correlation': float(abs(correlation))
                        }
            
            # Determine strongest predictor
            if relationship_analysis['memtrax_correlations']:
                strongest_predictor = max(
                    relationship_analysis['memtrax_correlations'].items(),
                    key=lambda x: x[1]['abs_correlation']
                )
                relationship_analysis['strongest_predictor'] = {
                    'variable': strongest_predictor[0],
                    'correlation': strongest_predictor[1]['correlation']
                }
                
                # Assess clinical significance
                max_correlation = abs(strongest_predictor[1]['correlation'])
                if max_correlation >= 0.5:
                    relationship_analysis['clinical_significance'] = 'strong'
                elif max_correlation >= 0.3:
                    relationship_analysis['clinical_significance'] = 'moderate'
                elif max_correlation >= 0.1:
                    relationship_analysis['clinical_significance'] = 'weak'
                else:
                    relationship_analysis['clinical_significance'] = 'minimal'
        
        except Exception as e:
            relationship_analysis['error'] = str(e)
        
        return relationship_analysis
    
    def _generate_memtrax_predictive_insights(self, prediction_results: Dict[str, Any]) -> List[str]:
        """Generate insights about MemTrax predictive capabilities"""
        insights = []
        
        try:
            model_performance = prediction_results.get('model_performance', {})
            
            if not model_performance:
                insights.append("Insufficient data for MemTrax predictive modeling")
                return insights
            
            # Analyze overall predictive performance
            strong_predictors = []
            moderate_predictors = []
            
            for outcome, analysis in model_performance.items():
                significance = analysis.get('clinical_significance', 'unknown')
                strongest = analysis.get('strongest_predictor', {})
                
                if significance == 'strong':
                    strong_predictors.append(f"{strongest.get('variable', 'unknown')} for {outcome}")
                elif significance == 'moderate':
                    moderate_predictors.append(f"{strongest.get('variable', 'unknown')} for {outcome}")
            
            if strong_predictors:
                insights.append(f"MemTrax shows strong predictive power: {', '.join(strong_predictors)}")
            
            if moderate_predictors:
                insights.append(f"MemTrax shows moderate predictive power: {', '.join(moderate_predictors)}")
            
            if not strong_predictors and not moderate_predictors:
                insights.append("MemTrax shows limited predictive power for cognitive impairment in this dataset")
            
            # Add sample size context
            total_analyses = len(model_performance)
            if total_analyses > 0:
                avg_sample = sum(analysis.get('sample_size', 0) for analysis in model_performance.values()) / total_analyses
                insights.append(f"Predictive analysis based on average {avg_sample:.0f} subjects per outcome")
        
        except Exception as e:
            insights.append(f"Error generating MemTrax predictive insights: {str(e)}")
        
        return insights
    
    def _add_cognitive_impairment_labels(self) -> bool:
        """Add cognitive impairment labels based on medical history QID codes"""
        try:
            # Look for medical data in our combined dataset
            medical_cols = [col for col in self.combined_data.columns if col.startswith('QID1-')]
            
            if not medical_cols:
                self.logger.warning("No medical QID columns found for cognitive impairment labeling")
                return False
            
            # Cognitive condition QID codes (based on human scripts)
            cognitive_qids = [
                'QID1-5',   # Dementia
                'QID1-12',  # Alzheimer's Disease
                'QID1-13',  # Mild Cognitive Impairment (MCI)
                'QID1-22',  # Frontotemporal Dementia (FTD)
                'QID1-23',  # Lewy Body Disease (LBD)
            ]
            
            self.logger.info(f"   Found {len(medical_cols)} medical QID columns, checking for cognitive conditions...")
            
            # Create cognitive impairment labels
            cognitive_impairment = []
            cognitive_details = []
            
            for idx, row in self.combined_data.iterrows():
                has_cognitive_condition = False
                conditions_found = []
                
                for qid in cognitive_qids:
                    if qid in self.combined_data.columns:
                        try:
                            value = pd.to_numeric(row[qid], errors='coerce')
                            if value == 1:
                                has_cognitive_condition = True
                                conditions_found.append(qid)
                        except:
                            if str(row[qid]).strip() == '1':
                                has_cognitive_condition = True
                                conditions_found.append(qid)
                
                cognitive_impairment.append(1 if has_cognitive_condition else 0)
                cognitive_details.append(','.join(conditions_found) if conditions_found else 'None')
            
            # Add to dataset
            self.combined_data['CognitiveImpairment'] = cognitive_impairment
            self.combined_data['CognitiveDiagnosisCodes'] = cognitive_details
            
            # Log summary
            impaired_count = sum(cognitive_impairment)
            total_count = len(cognitive_impairment)
            
            self.logger.info(f"   Cognitive impairment labeling complete:")
            self.logger.info(f"   - Cognitively impaired: {impaired_count:,} ({impaired_count/total_count*100:.1f}%)")
            self.logger.info(f"   - Cognitively normal: {total_count-impaired_count:,} ({(total_count-impaired_count)/total_count*100:.1f}%)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to add cognitive impairment labels: {e}")
            return False
    
    def _advanced_cdr_prediction(self) -> Dict[str, Any]:
        """Advanced CDR prediction using state-of-the-art ML techniques"""
        prediction_results = {
            'analysis_type': 'advanced_cdr_prediction',
            'models_tested': [],
            'best_model': {},
            'feature_importance': {},
            'performance_metrics': {},
            'clinical_insights': []
        }
        
        try:
            # Check if we have CDR column
            if 'CDR' not in self.combined_data.columns:
                prediction_results['error'] = 'CDR column not found in dataset'
                return prediction_results
            
            self.logger.info("üß† Running advanced CDR prediction analysis with benchmark-optimized hyperparameters...")
            
            # Enhanced data preprocessing - Use benchmark approach for optimal performance
            df = self.combined_data.copy()
            
            # BENCHMARK FIX: Harmonize column names from both datasets
            if 'ID' in df.columns and 'Subject_ID' not in df.columns:
                df = df.rename(columns={'ID': 'Subject_ID'})
            if 'Subject ID' in df.columns:
                df = df.rename(columns={'Subject ID': 'Subject_ID'})
            if 'M/F' in df.columns:
                df = df.rename(columns={'M/F': 'Gender'})
            if 'Educ' in df.columns:
                df = df.rename(columns={'Educ': 'EDUC'})
            
            # Apply advanced enhancements if available
            if ENHANCEMENTS_AVAILABLE:
                self.logger.info("   üöÄ Applying advanced feature engineering enhancements...")
                try:
                    df = integrate_enhancements(self, df)
                    self.logger.info("   ‚úÖ Successfully applied brain volume normalization and feature enhancements")
                except Exception as e:
                    self.logger.warning(f"   ‚ö†Ô∏è Enhancement application failed: {e}, continuing with standard features")
            
            # Remove rows with missing CDR
            before_cdr_drop = len(df)
            df = df.dropna(subset=['CDR'])
            after_cdr_drop = len(df)
            self.logger.info(f"   üéØ Dropped {before_cdr_drop - after_cdr_drop} rows missing CDR: {after_cdr_drop}/{before_cdr_drop} subjects retained")
            
            # Handle missing values intelligently
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if 'CDR' in numeric_cols:
                numeric_cols.remove('CDR')
            
            # Smart imputation strategy (matching successful benchmarks)
            if len(numeric_cols) > 0:
                # Filter to columns that have at least some non-null values
                imputable_cols = [col for col in numeric_cols if df[col].notna().any()]
                if imputable_cols:
                    # Use different strategies for different types of variables
                    for col in imputable_cols:
                        if 'SES' in col.upper():
                            # Use mode for socioeconomic status (categorical-like)
                            mode_imputer = SimpleImputer(strategy='most_frequent')
                            df[[col]] = mode_imputer.fit_transform(df[[col]])
                        else:
                            # Use median for other numeric variables
                            median_imputer = SimpleImputer(strategy='median')
                            df[[col]] = median_imputer.fit_transform(df[[col]])
                    self.logger.info(f"   Applied targeted imputation to {len(imputable_cols)} columns")
                
                # Drop columns that are entirely null
                null_cols = [col for col in numeric_cols if df[col].isna().all()]
                if null_cols:
                    df = df.drop(columns=null_cols)
                    self.logger.info(f"   Dropped {len(null_cols)} entirely null columns: {null_cols}")
            
            # Handle categorical variables with one-hot encoding
            categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
            if categorical_cols:
                # Store CDR values before one-hot encoding
                cdr_values = df['CDR'].copy() if 'CDR' in df.columns else None
                df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
                # Restore CDR if it was categorical and got encoded
                if cdr_values is not None and 'CDR' not in df.columns:
                    df['CDR'] = cdr_values
            
            # Prepare features and target with comprehensive data leakage prevention
            target_col = 'CDR'
            
            # Detect potential leakage columns (same name, similar names, high correlation)
            leakage_columns = self._detect_data_leakage(df, target_col)
            
            if leakage_columns:
                self.logger.warning(f"   üö® LEAKAGE DETECTED: Excluding {len(leakage_columns)} columns to prevent data leakage:")
                for col in leakage_columns:
                    self.logger.warning(f"       - {col}")
                    
            X = df.drop(leakage_columns + [target_col], axis=1)
            y = df[target_col]
            
            self.logger.info(f"   ‚úÖ Using {X.shape[1]} legitimate features (excluded {len(leakage_columns)} leakage-prone columns)")
            
            # BENCHMARK FIX: Convert CDR to integer for classification with proper sequential mapping
            # Map CDR values to sequential classes: 0.0->0, 0.5->1, 1.0->2, 2.0->3
            y_classes = y.copy()
            y_classes = y_classes.replace(0.0, 0)  # Map 0.0 to class 0
            y_classes = y_classes.replace(0.5, 1)  # Map 0.5 to class 1
            y_classes = y_classes.replace(1.0, 2)  # Map 1.0 to class 2
            y_classes = y_classes.replace(2.0, 3)  # Map 2.0 to class 3
            y_encoded = y_classes.astype(int)
            
            # BENCHMARK FIX: Apply benchmark preprocessing - exclude CDR=2.0 (severe cases) to match published results
            severe_mask = (y == 2.0)  # Exclude exactly CDR=2.0 cases (5 subjects)
            n_severe = int(severe_mask.sum())
            if n_severe > 0:
                self.logger.info(f"   üìä BENCHMARK APPROACH: Excluding {n_severe} severe CDR=2.0 cases (matches 603 subjects)")
                self.logger.info(f"   üí° This replicates published benchmark for direct performance comparison")
                # Filter all datasets
                X = X.loc[~severe_mask]
                y = y[~severe_mask]
                # Re-encode y after filtering - use LabelEncoder for proper sequential mapping
                # This ensures classes are 0, 1, 2 regardless of original CDR values
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                y_encoded = le.fit_transform(y)  # Will map to sequential 0, 1, 2
                self.logger.info(f"   üìä Classes mapped: {dict(zip(le.classes_, le.transform(le.classes_)))}")
                
                self.logger.info(f"   üéØ Final dataset: {len(X)} subjects (target: 603 for benchmark match)")
            else:
                self.logger.info(f"   üìä No severe CDR=2.0 cases found to exclude")
            
            # Reset indices to avoid alignment issues
            X = X.reset_index(drop=True)
            y_encoded = pd.Series(y_encoded).reset_index(drop=True).values
            
            if len(X) < 50:
                prediction_results['error'] = f'Insufficient data for ML: only {len(X)} samples'
                return prediction_results
            
            self.logger.info(f"   Dataset prepared: {len(X)} samples, {X.shape[1]} features")
            self.logger.info(f"   CDR distribution: {dict(zip(*np.unique(y_encoded, return_counts=True)))}")
            
            # Standardize features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Use notebook's approach: full dataset cross-validation (no train/test split for main evaluation)
            # Split only for final test score reporting
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
            )
            
            # Define models to test with optimized hyperparameters (based on successful benchmarks)
            models = {
                'GradientBoosting': GradientBoostingClassifier(
                    random_state=42,
                    learning_rate=0.15,
                    max_depth=50,
                    n_estimators=96,
                    subsample=0.95,
                    min_samples_split=0.15,
                    min_samples_leaf=5,
                    max_features='log2'
                ),
                'RandomForest': RandomForestClassifier(
                    random_state=42, 
                    n_estimators=200,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2
                )
            }
            
            # Add XGBoost if available with optimized parameters  
            if XGBOOST_AVAILABLE:
                models['XGBoost'] = XGBClassifier(
                    random_state=42, 
                    eval_metric='mlogloss',
                    learning_rate=0.1,
                    max_depth=8,
                    n_estimators=100
                )
            
            # Test each model with F1-focused evaluation
            best_f1_score = 0
            best_model_name = None
            best_model = None
            
            # First test individual models
            for name, model in models.items():
                try:
                    # F1-FOCUSED EVALUATION: Use comprehensive clinical evaluation if available
                    if self.clinical_evaluator:
                        self.logger.info(f"   üéØ {name}: F1-focused clinical evaluation...")
                        
                        # Get comprehensive metrics using clinical evaluator
                        X_df = pd.DataFrame(X_scaled, columns=[f'feature_{i}' for i in range(X_scaled.shape[1])])
                        y_series = pd.Series(y_encoded)
                        
                        comprehensive_metrics = self.clinical_evaluator.evaluate_model_comprehensive(
                            model, X_df, y_series, cv=5
                        )
                        
                        # Extract key metrics
                        f1_weighted = comprehensive_metrics['f1_weighted_mean']
                        f1_macro = comprehensive_metrics['f1_macro_mean']
                        precision_weighted = comprehensive_metrics['precision_weighted_mean']
                        recall_weighted = comprehensive_metrics['recall_weighted_mean']
                        accuracy_mean = comprehensive_metrics['accuracy_mean']
                        clinical_quality = comprehensive_metrics['clinical_quality_score']
                        
                        model_results = {
                            'name': name,
                            'f1_weighted': f1_weighted,
                            'f1_macro': f1_macro,
                            'precision_weighted': precision_weighted,
                            'recall_weighted': recall_weighted,
                            'accuracy_mean': accuracy_mean,
                            'clinical_quality_score': clinical_quality,
                            'clinically_acceptable': comprehensive_metrics['clinically_acceptable'],
                            'comprehensive_metrics': comprehensive_metrics
                        }
                        
                        # Log F1-focused results prominently
                        self.logger.info(f"   üìä {name} CLINICAL EVALUATION:")
                        self.logger.info(f"      üéØ F1 (Weighted): {f1_weighted:.3f} ¬± {comprehensive_metrics['f1_weighted_std']:.3f}")
                        self.logger.info(f"      üéØ F1 (Macro): {f1_macro:.3f} ¬± {comprehensive_metrics['f1_macro_std']:.3f}")
                        self.logger.info(f"      üéØ Precision: {precision_weighted:.3f} ¬± {comprehensive_metrics['precision_weighted_std']:.3f}")
                        self.logger.info(f"      üéØ Recall: {recall_weighted:.3f} ¬± {comprehensive_metrics['recall_weighted_std']:.3f}")
                        self.logger.info(f"      üéØ Accuracy: {accuracy_mean:.3f} ¬± {comprehensive_metrics['accuracy_std']:.3f}")
                        self.logger.info(f"      üè• Clinical Quality Score: {clinical_quality:.3f}")
                        self.logger.info(f"      üè• Clinically Acceptable: {'‚úÖ Yes' if comprehensive_metrics['clinically_acceptable'] else '‚ùå No'}")
                        
                        # Track best model based on F1-weighted score (primary clinical metric)
                        if f1_weighted > best_f1_score:
                            best_f1_score = f1_weighted
                            best_model_name = name
                            best_model = model
                        
                    else:
                        # Fallback to basic evaluation
                        cv_scores = cross_val_score(model, X_scaled, y_encoded, cv=10, scoring='f1_weighted')
                        mean_f1 = cv_scores.mean()
                        
                        # Additional train/test evaluation
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
                        test_f1 = f1_score(y_test, y_pred, average='weighted')
                        
                        model_results = {
                            'name': name,
                            'f1_weighted': mean_f1,
                            'f1_weighted_std': cv_scores.std(),
                            'test_f1': test_f1,
                            'cv_scores': cv_scores.tolist()
                        }
                        
                        self.logger.info(f"   {name}: F1={mean_f1:.3f}¬±{cv_scores.std():.3f}, Test F1={test_f1:.3f}")
                        
                        # Track best model
                        if mean_f1 > best_f1_score:
                            best_f1_score = mean_f1
                            best_model_name = name
                            best_model = model
                    
                    prediction_results['models_tested'].append(model_results)
                        
                except Exception as e:
                    self.logger.warning(f"   {name} failed: {e}")
            
            # Test ensemble model if enhancements are available
            if ENHANCEMENTS_AVAILABLE and best_f1_score > 0:
                try:
                    self.logger.info("   üéØ Testing advanced ensemble model...")
                    enhancer = EnhancedCDRPredictor(logger=self.logger)
                    
                    # Apply correlation-based feature selection
                    X_selected = enhancer.apply_correlation_based_feature_selection(X, y, threshold=0.05)
                    X_selected_scaled = scaler.fit_transform(X_selected)
                    
                    # Split for ensemble
                    X_train_ens, X_test_ens, y_train_ens, y_test_ens = train_test_split(
                        X_selected_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
                    )
                    
                    # Create and test ensemble
                    ensemble_results = enhancer.create_ensemble_model(X_train_ens, y_train_ens, X_test_ens, y_test_ens)
                    
                    # Add to results
                    ensemble_model_results = {
                        'name': 'Ensemble',
                        'cv_mean': ensemble_results['ensemble_cv_mean'],
                        'cv_std': ensemble_results['ensemble_cv_std'],
                        'test_accuracy': ensemble_results.get('ensemble_test_accuracy', 0),
                        'cv_scores': []  # Not available for ensemble
                    }
                    
                    prediction_results['models_tested'].append(ensemble_model_results)
                    
                    # Check if ensemble is best (using F1 as primary metric)
                    ensemble_f1 = ensemble_results.get('ensemble_f1_score', ensemble_results['ensemble_cv_mean'])
                    if ensemble_f1 > best_f1_score:
                        best_f1_score = ensemble_f1
                        best_model_name = 'Ensemble'
                        best_model = ensemble_results['model']
                        self.logger.info(f"   ‚úÖ Ensemble is new best model: F1={ensemble_f1:.3f}")
                    
                except Exception as e:
                    self.logger.warning(f"   Ensemble model failed: {e}")
            
            if best_model is not None:
                # F1-FOCUSED evaluation of best model
                # Use correct feature dimensions based on model type
                if best_model_name == 'Ensemble':
                    # Ensemble was trained on selected features
                    y_pred = best_model.predict(X_test_ens)
                    y_true = y_test_ens
                else:
                    # Individual models were trained on full feature set
                    y_pred = best_model.predict(X_test)
                    y_true = y_test
                
                # Calculate comprehensive F1-focused metrics
                test_accuracy = accuracy_score(y_true, y_pred)
                test_f1_weighted = f1_score(y_true, y_pred, average='weighted')
                test_f1_macro = f1_score(y_true, y_pred, average='macro')
                test_precision = precision_score(y_true, y_pred, average='weighted')
                test_recall = recall_score(y_true, y_pred, average='weighted')
                
                classification_report_dict = classification_report(y_true, y_pred, output_dict=True)
                
                prediction_results['best_model'] = {
                    'name': best_model_name,
                    'f1_weighted': best_f1_score,  # Cross-validation F1 score
                    'test_f1_weighted': test_f1_weighted,  # Test set F1 score
                    'test_f1_macro': test_f1_macro,
                    'test_precision': test_precision,
                    'test_recall': test_recall,
                    'test_accuracy': test_accuracy,
                    'classification_report': classification_report_dict
                }
                
                # Feature importance (if available)
                importance_scores = None
                feature_names = None
                
                if hasattr(best_model, 'feature_importances_'):
                    importance_scores = best_model.feature_importances_
                    feature_names = list(X_selected.columns if best_model_name == 'Ensemble' else X.columns)
                elif hasattr(best_model, 'estimators_') and best_model_name == 'Ensemble':
                    # For ensemble models, get feature importance from first estimator
                    try:
                        importance_scores = best_model.estimators_[0][1].feature_importances_
                        feature_names = list(X_selected.columns)
                    except:
                        pass
                
                if importance_scores is not None and feature_names is not None:
                    # Get top 10 most important features
                    importance_df = pd.DataFrame({
                        'feature': feature_names,
                        'importance': importance_scores
                    }).sort_values('importance', ascending=False)
                    
                    # Intelligently determine how many features to show
                    total_features = len(importance_df)
                    max_features_to_show = self._calculate_optimal_display_count(total_features, 'feature_importance')
                    top_features = importance_df.head(max_features_to_show)
                    
                    prediction_results['feature_importance'] = top_features.to_dict('records')
                    
                    # Show fewer features in log for readability
                    log_features_count = min(5, len(top_features))
                    self.logger.info(f"   Top {log_features_count} predictive features:")
                    for _, row in top_features.head(log_features_count).iterrows():
                        self.logger.info(f"      {row['feature']}: {row['importance']:.3f}")
                
                # F1-FOCUSED Clinical insights
                insights = []
                test_f1_weighted = prediction_results['best_model']['test_f1_weighted']
                test_f1_macro = prediction_results['best_model']['test_f1_macro']
                test_precision = prediction_results['best_model']['test_precision']
                test_recall = prediction_results['best_model']['test_recall']
                test_accuracy = prediction_results['best_model']['test_accuracy']
                
                # F1-based clinical assessment (more important than accuracy for medical applications)
                if test_f1_weighted > 0.85:
                    insights.append("üèÜ EXCELLENT F1 PERFORMANCE: Weighted F1 > 0.85 - Clinical deployment ready")
                elif test_f1_weighted > 0.80:
                    insights.append("‚úÖ STRONG F1 PERFORMANCE: Weighted F1 > 0.80 - Clinically acceptable")
                elif test_f1_weighted > 0.75:
                    insights.append("üéØ GOOD F1 PERFORMANCE: Weighted F1 > 0.75 - Shows clinical promise")
                else:
                    insights.append("‚ö†Ô∏è LIMITED F1 PERFORMANCE: Consider feature engineering or data augmentation")
                
                # Precision-Recall balance analysis
                if test_precision > 0.80 and test_recall > 0.80:
                    insights.append("‚öñÔ∏è BALANCED PRECISION-RECALL: Low false positives AND low false negatives")
                elif test_precision > 0.80:
                    insights.append("üéØ HIGH PRECISION: Low false positive rate - conservative predictions")
                elif test_recall > 0.80:
                    insights.append("üîç HIGH RECALL: Low false negative rate - good case detection")
                else:
                    insights.append("üìä MODERATE PRECISION-RECALL: Room for improvement in clinical decision balance")
                
                # Class balance analysis
                if abs(test_f1_weighted - test_f1_macro) < 0.05:
                    insights.append("üéØ WELL-BALANCED CLASS PERFORMANCE: Similar performance across all CDR levels")
                else:
                    insights.append("‚ö†Ô∏è CLASS IMBALANCE DETECTED: Some CDR levels predicted better than others")
                
                if len(X.columns) > 20:
                    insights.append("üìä HIGH-DIMENSIONAL ANALYSIS: Feature selection may improve interpretability")
                
                prediction_results['clinical_insights'] = insights
                
                # F1-FOCUSED Enhanced logging (F1 scores first!)
                self.logger.info(f"\nüèÜ ===== FINAL F1-FOCUSED CLINICAL EVALUATION =====")
                self.logger.info(f"ü•á Best Model: {best_model_name}")
                self.logger.info(f"üéØ PRIMARY METRICS (Clinical Focus):")
                self.logger.info(f"   üî• F1-Score (Weighted): {test_f1_weighted:.3f}")
                self.logger.info(f"   üî• F1-Score (Macro): {test_f1_macro:.3f}")
                self.logger.info(f"   üî• Precision (Weighted): {test_precision:.3f}")
                self.logger.info(f"   üî• Recall (Weighted): {test_recall:.3f}")
                self.logger.info(f"üìä SECONDARY METRICS:")
                self.logger.info(f"   üìà Accuracy: {test_accuracy:.3f}")
                self.logger.info(f"   üìà CV F1-Score: {best_f1_score:.3f}")
                
                # Per-class F1 performance (clinical decision support)
                classification_report_data = prediction_results['best_model']['classification_report']
                self.logger.info(f"üè• PER-CLASS CLINICAL PERFORMANCE:")
                for class_name, metrics in classification_report_data.items():
                    if class_name.isdigit():
                        cdr_value = {0: '0.0', 1: '0.5', 2: '1.0'}.get(int(class_name), class_name)
                        f1 = metrics.get('f1-score', 0)
                        precision = metrics.get('precision', 0)
                        recall = metrics.get('recall', 0)
                        support = metrics.get('support', 0)
                        self.logger.info(f"   CDR {cdr_value}: F1={f1:.3f}, Prec={precision:.3f}, Rec={recall:.3f} (n={int(support)})")
                
                # Clinical acceptability assessment
                clinically_acceptable = test_f1_weighted >= 0.75 and test_precision >= 0.75 and test_recall >= 0.75
                self.logger.info(f"üè• CLINICAL ACCEPTABILITY: {'‚úÖ APPROVED' if clinically_acceptable else '‚ùå NEEDS IMPROVEMENT'}")
                self.logger.info(f"üí° F1-FOCUSED INSIGHTS:")
                for insight in insights:
                    self.logger.info(f"   {insight}")
                
            else:
                prediction_results['error'] = 'No models successfully trained'
                
        except Exception as e:
            prediction_results['error'] = f"Advanced CDR prediction failed: {str(e)}"
            self.logger.error(f"Advanced CDR prediction error: {e}")
        
        return prediction_results
    
    def _detect_data_leakage(self, df: pd.DataFrame, target_col: str, correlation_threshold: float = 0.95) -> List[str]:
        """
        Comprehensive data leakage detection system
        
        Detects various forms of data leakage:
        1. Same column names (exact matches)
        2. Similar column names (fuzzy matching) 
        3. Perfect or near-perfect correlations
        4. Future information (temporal leakage)
        """
        leakage_columns = []
        
        # Legitimate predictors that should NOT be considered leakage even with high correlation
        legitimate_predictors = ['MMSE', 'Age', 'EDUC', 'SES', 'eTIV', 'nWBV', 'ASF', 'Gender', 'Gender_M', 'Gender_F', 'Hand']
        
        try:
            # 1. Exact name matches (case-insensitive)
            exact_matches = [col for col in df.columns if col != target_col and target_col.upper() in col.upper()]
            leakage_columns.extend(exact_matches)
            
            # 2. Fuzzy name matching for variations
            import difflib
            target_variations = [target_col.lower(), target_col.upper(), f"{target_col}_", f"_{target_col}"]
            for col in df.columns:
                if col != target_col:
                    for variation in target_variations:
                        if variation in col.lower() or difflib.SequenceMatcher(None, col.lower(), target_col.lower()).ratio() > 0.8:
                            if col not in leakage_columns:
                                leakage_columns.append(col)
            
            # 3. High correlation detection (only for numeric columns, excluding legitimate predictors)
            if target_col in df.columns and df[target_col].dtype in ['int64', 'float64']:
                
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                for col in numeric_cols:
                    if col != target_col and col not in leakage_columns:
                        # Skip legitimate predictors
                        if any(legit in col for legit in legitimate_predictors):
                            continue
                            
                        # Calculate correlation, handling NaN values
                        try:
                            corr_data = df[[col, target_col]].dropna()
                            if len(corr_data) > 10:  # Need sufficient data points
                                correlation = corr_data[col].corr(corr_data[target_col])
                                if abs(correlation) > correlation_threshold:
                                    leakage_columns.append(col)
                                    self.logger.warning(f"   High correlation detected: {col} vs {target_col} = {correlation:.3f}")
                        except:
                            pass
            
            # 4. Temporal leakage detection (columns that might contain future information)
            # Skip legitimate predictors from temporal analysis
            temporal_keywords = ['future', 'outcome', 'result', 'final', 'end', 'discharge', 'follow', 'post']
            for col in df.columns:
                if col != target_col and any(keyword in col.lower() for keyword in temporal_keywords):
                    # Skip legitimate predictors
                    if not any(legit in col for legit in legitimate_predictors):
                        if col not in leakage_columns:
                            leakage_columns.append(col)
            
            # 5. Log leakage detection summary
            if leakage_columns:
                self.logger.warning(f"   üìä LEAKAGE ANALYSIS: Found {len(leakage_columns)} potentially leaky features")
                leakage_types = {
                    'name_based': [col for col in exact_matches],
                    'correlation_based': [col for col in leakage_columns if col not in exact_matches],
                    'temporal_based': [col for col in leakage_columns if any(kw in col.lower() for kw in temporal_keywords)]
                }
                for leak_type, cols in leakage_types.items():
                    if cols:
                        self.logger.warning(f"       {leak_type}: {cols}")
            else:
                self.logger.info(f"   ‚úÖ LEAKAGE CHECK: No obvious data leakage detected")
                        
        except Exception as e:
            self.logger.error(f"Error in leakage detection: {e}")
            # Fallback to basic name-based detection
            leakage_columns = [col for col in df.columns if col != target_col and target_col.upper() in col.upper()]
        
        return leakage_columns
    
    def _run_surveillance_analysis(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Run specialized analysis for surveillance data (like BRFSS)"""
        self.logger.info("   üî¨ Analyzing surveillance data patterns...")
        
        df = self.combined_data
        
        # 1. Temporal Analysis
        if 'YearStart' in df.columns:
            self.logger.info("   üìà Performing temporal trend analysis...")
            temporal_analysis = {}
            
            # Group by year and calculate trends
            yearly_stats = df.groupby('YearStart')['Data_Value'].agg(['mean', 'median', 'std', 'count'])
            temporal_analysis['yearly_trends'] = yearly_stats.to_dict()
            
            # Detect trend direction
            years = yearly_stats.index.values
            values = yearly_stats['mean'].values
            if len(years) > 2:
                from scipy import stats
                slope, intercept, r_value, p_value, std_err = stats.linregress(years, values)
                temporal_analysis['trend'] = {
                    'direction': 'increasing' if slope > 0 else 'decreasing',
                    'slope': float(slope),
                    'r_squared': float(r_value**2),
                    'p_value': float(p_value)
                }
            
            analysis_results['temporal_analysis'] = temporal_analysis
        
        # 2. Geographic Analysis
        if 'LocationAbbr' in df.columns:
            self.logger.info("   üó∫Ô∏è Performing geographic analysis...")
            geographic_analysis = {}
            
            # State-level statistics
            state_stats = df.groupby('LocationAbbr')['Data_Value'].agg(['mean', 'median', 'std', 'count'])
            geographic_analysis['state_statistics'] = state_stats.nlargest(10, 'mean').to_dict()
            geographic_analysis['high_risk_states'] = state_stats.nlargest(5, 'mean').index.tolist()
            geographic_analysis['low_risk_states'] = state_stats.nsmallest(5, 'mean').index.tolist()
            
            analysis_results['geographic_analysis'] = geographic_analysis
        
        # 3. Topic/Health Category Analysis
        if 'Topic' in df.columns:
            self.logger.info("   üè• Analyzing health topics...")
            topic_analysis = {}
            
            # Topic statistics
            topic_stats = df.groupby('Topic')['Data_Value'].agg(['mean', 'median', 'count'])
            topic_analysis['topic_statistics'] = topic_stats.nlargest(10, 'count').to_dict()
            
            # Focus on cognitive-related topics
            cognitive_topics = df[df['Topic'].str.contains('Cognitive|Dementia|Alzheimer', case=False, na=False)]
            if not cognitive_topics.empty:
                topic_analysis['cognitive_metrics'] = {
                    'records': len(cognitive_topics),
                    'mean_value': float(cognitive_topics['Data_Value'].mean()),
                    'median_value': float(cognitive_topics['Data_Value'].median())
                }
            
            analysis_results['topic_analysis'] = topic_analysis
        
        # 4. Predictive Modeling for Surveillance Data
        self.logger.info("   üéØ Building predictive models for population health...")
        predictive_results = self._build_surveillance_predictive_models(df)
        analysis_results['predictive_models'] = predictive_results
        
        # 5. Risk Score Calculation
        if 'LocationAbbr' in df.columns and 'Data_Value' in df.columns:
            self.logger.info("   üìä Calculating population risk scores...")
            risk_scores = self._calculate_population_risk_scores(df)
            analysis_results['risk_scores'] = risk_scores
        
        # 6. Stratification Analysis
        if 'Stratification1' in df.columns:
            self.logger.info("   üë• Analyzing demographic stratifications...")
            strat_analysis = {}
            strat_stats = df.groupby('Stratification1')['Data_Value'].agg(['mean', 'median', 'std', 'count'])
            strat_analysis['stratification_statistics'] = strat_stats.to_dict()
            analysis_results['stratification_analysis'] = strat_analysis
        
        return analysis_results
    
    def _build_surveillance_predictive_models(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Build predictive models for surveillance data"""
        results = {}
        
        try:
            # Create feature matrix from pivoted data
            if 'LocationAbbr' in df.columns and 'YearStart' in df.columns and 'Topic' in df.columns:
                # Pivot to create features
                feature_matrix = df.pivot_table(
                    index=['LocationAbbr', 'YearStart'],
                    columns='Topic',
                    values='Data_Value',
                    aggfunc='mean'
                ).reset_index()
                
                # Identify target (cognitive decline if available)
                cognitive_cols = [col for col in feature_matrix.columns 
                                if 'Cognitive' in str(col) or 'Dementia' in str(col)]
                
                if cognitive_cols:
                    target_col = cognitive_cols[0]
                    feature_cols = [col for col in feature_matrix.columns 
                                  if col not in ['LocationAbbr', 'YearStart', target_col]]
                    
                    # Prepare data
                    valid_data = feature_matrix.dropna(subset=[target_col])
                    X = valid_data[feature_cols].fillna(valid_data[feature_cols].median())
                    y = valid_data[target_col]
                    
                    if len(X) > 30:  # Need enough data for modeling
                        from sklearn.model_selection import train_test_split
                        from sklearn.ensemble import RandomForestRegressor
                        from sklearn.metrics import r2_score, mean_absolute_error
                        
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, y, test_size=0.2, random_state=42
                        )
                        
                        # Train model
                        model = RandomForestRegressor(n_estimators=100, random_state=42)
                        model.fit(X_train, y_train)
                        
                        # Evaluate
                        y_pred = model.predict(X_test)
                        r2 = r2_score(y_test, y_pred)
                        mae = mean_absolute_error(y_test, y_pred)
                        
                        # Feature importance
                        feature_importance = pd.DataFrame({
                            'feature': feature_cols,
                            'importance': model.feature_importances_
                        }).sort_values('importance', ascending=False)
                        
                        results = {
                            'model_type': 'RandomForest',
                            'target': target_col,
                            'r2_score': float(r2),
                            'mae': float(mae),
                            'top_features': feature_importance.head(10).to_dict(),
                            'sample_size': len(X)
                        }
                        
                        self.logger.info(f"      ‚úÖ Predictive model R¬≤={r2:.3f}, MAE={mae:.2f}")
        
        except Exception as e:
            self.logger.warning(f"      ‚ö†Ô∏è Could not build predictive models: {e}")
        
        return results
    
    def _calculate_population_risk_scores(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate risk scores for populations"""
        risk_scores = {}
        
        try:
            # Calculate composite risk score by location
            location_scores = df.groupby('LocationAbbr')['Data_Value'].agg(['mean', 'std', 'count'])
            
            # Normalize scores
            location_scores['risk_score'] = (
                (location_scores['mean'] - location_scores['mean'].min()) / 
                (location_scores['mean'].max() - location_scores['mean'].min())
            )
            
            risk_scores = {
                'high_risk': location_scores.nlargest(5, 'risk_score').to_dict(),
                'low_risk': location_scores.nsmallest(5, 'risk_score').to_dict(),
                'median_risk': float(location_scores['risk_score'].median())
            }
            
        except Exception as e:
            self.logger.warning(f"      ‚ö†Ô∏è Could not calculate risk scores: {e}")
        
        return risk_scores
    
    def _get_risk_color_palette(self):
        """Get intelligent color palette for risk visualization"""
        return {
            'high': '#d32f2f',      # Red for high risk
            'low': '#388e3c',       # Green for low risk  
            'neutral': '#42a5f5',   # Blue for neutral
            'secondary': '#ff9800'   # Orange for secondary categories
        }
    
    def _get_topic_color_palette(self):
        """Get intelligent color palette for topic analysis"""
        return {
            'prevalence': '#ff7043',    # Orange-red for prevalence
            'sample_size': '#29b6f6',   # Light blue for sample sizes
            'cognitive': '#ab47bc',     # Purple for cognitive metrics
            'health': '#66bb6a'         # Green for health metrics
        }
    
    def _calculate_optimal_display_count(self, total_items, item_type='items'):
        """Intelligently determine optimal number of items to display"""
        if item_type == 'states':
            # For states, show enough to capture patterns but not overwhelm
            return min(total_items, max(8, int(total_items * 0.6)))
        elif item_type == 'topics':
            # For topics, focus on most significant ones
            return min(total_items, max(6, int(total_items * 0.4)))
        elif item_type == 'performance_metrics':
            # For performance metrics, balance detail with readability
            return min(total_items, max(4, min(8, int(total_items * 0.75))))
        elif item_type == 'feature_importance':
            # For feature importance, show meaningful subset
            return min(total_items, max(5, min(15, int(total_items * 0.8))))
        else:
            # General case
            return min(total_items, max(5, int(total_items * 0.5)))
    
    def _calculate_text_truncation_length(self, text_list, max_width_chars=40):
        """Intelligently determine optimal text truncation length"""
        if not text_list:
            return max_width_chars
        
        avg_length = sum(len(text) for text in text_list) / len(text_list)
        max_length = max(len(text) for text in text_list)
        
        # If average is reasonable, use a bit more than average
        if avg_length <= max_width_chars * 0.7:
            return int(avg_length * 1.3)
        else:
            # If text is long, use adaptive truncation
            return max(20, max_width_chars)
    
    def _calculate_figure_size(self, num_items, plot_type='bar'):
        """Intelligently calculate figure size based on content"""
        if plot_type == 'geographic':
            width = max(12, min(20, num_items * 1.2))
            height = 8
        elif plot_type == 'topic_horizontal':
            width = max(14, min(18, num_items * 0.8))
            height = max(8, min(12, num_items * 0.6))
        elif plot_type == 'dashboard':
            width = 20
            height = 12
        else:
            width = max(10, min(16, num_items * 0.8))
            height = max(6, min(10, num_items * 0.5))
        
        return (width, height)
    
    def _calculate_correlation_limit(self, cols_a_count, cols_b_count):
        """Intelligently calculate correlation analysis limits to prevent computational explosion"""
        total_combinations = cols_a_count * cols_b_count
        
        # Scale limits based on total combinations to prevent exponential growth
        if total_combinations <= 25:  # Small dataset
            return min(cols_a_count, cols_b_count)
        elif total_combinations <= 100:  # Medium dataset  
            return min(10, max(cols_a_count, cols_b_count))
        elif total_combinations <= 400:  # Large dataset
            return min(8, max(cols_a_count, cols_b_count))
        else:  # Very large dataset
            return min(5, max(cols_a_count, cols_b_count))
    
    def _create_temporal_trends_plot(self, temporal_analysis):
        """Create temporal trend visualization"""
        trends = temporal_analysis.get('yearly_trends', {})
        if not trends or not trends.get('mean'):
            self.logger.warning("      ‚ö†Ô∏è No temporal trends data available for plotting")
            return
        
        try:
            years = sorted([int(year) for year in trends['mean'].keys()])
            means = [trends['mean'][str(year)] for year in years]
            stds = [trends['std'][str(year)] for year in years] if 'std' in trends else [0] * len(years)
            counts = [trends['count'][str(year)] for year in years] if 'count' in trends else [1] * len(years)
        except (KeyError, ValueError) as e:
            self.logger.warning(f"      ‚ö†Ô∏è Error parsing temporal trends data: {e}")
            return
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # Main trend plot
        ax1.errorbar(years, means, yerr=stds, marker='o', linewidth=2, 
                    markersize=8, capsize=5, capthick=2, color='steelblue')
        ax1.set_xlabel('Year')
        ax1.set_ylabel('Mean Data Value (%)')
        ax1.set_title('Population Health Surveillance: Temporal Trends', fontsize=16, pad=20)
        ax1.grid(True, alpha=0.3)
        
        # Add trend line
        z = np.polyfit(years, means, 1)
        p = np.poly1d(z)
        ax1.plot(years, p(years), "r--", alpha=0.8, linewidth=2, 
                label=f'Trend: {z[0]:+.2f}%/year')
        
        # Add R¬≤ and p-value
        trend_info = temporal_analysis.get('trend', {})
        r_squared = trend_info.get('r_squared', 0)
        p_value = trend_info.get('p_value', 1)
        significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
        
        ax1.text(0.05, 0.95, f'R¬≤ = {r_squared:.3f}\np = {p_value:.4f} {significance}', 
                 transform=ax1.transAxes, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        ax1.legend()
        
        # Sample size plot
        ax2.bar(years, counts, alpha=0.7, color='lightcoral', edgecolor='black')
        ax2.set_xlabel('Year')
        ax2.set_ylabel('Number of Records')
        ax2.set_title('Sample Sizes by Year')
        ax2.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/temporal_trends.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("      ‚úÖ Temporal trends plot saved")
    
    def _create_geographic_patterns_plot(self, geographic_analysis):
        """Create geographic risk pattern visualization"""
        state_stats = geographic_analysis.get('state_statistics', {})
        high_risk = geographic_analysis.get('high_risk_states', [])
        low_risk = geographic_analysis.get('low_risk_states', [])
        
        if not state_stats or not state_stats.get('mean'):
            self.logger.warning("      ‚ö†Ô∏è No geographic patterns data available for plotting")
            return
        
        # Intelligently determine number of states to show based on data
        all_states = list(state_stats['mean'].keys())
        num_states = min(len(all_states), max(8, len(high_risk) + len(low_risk)))  # At least 8, or enough to show risk states
        states = all_states[:num_states]
        means = [state_stats['mean'][state] for state in states]
        stds = [state_stats['std'][state] for state in states] if 'std' in state_stats else [0] * len(states)
        
        # Intelligent color coding based on risk categories
        risk_colors = self._get_risk_color_palette()
        colors = []
        for state in states:
            if state in high_risk:
                colors.append(risk_colors['high'])
            elif state in low_risk:
                colors.append(risk_colors['low'])
            else:
                colors.append(risk_colors['neutral'])
        
        # Dynamic figure sizing based on number of states
        fig_width = max(12, min(20, num_states * 1.2))
        fig, ax = plt.subplots(1, 1, figsize=(fig_width, 8))
        
        bars = ax.bar(range(len(states)), means, yerr=stds, capsize=4,
                     color=colors, alpha=0.7, edgecolor='black')
        ax.set_xlabel('State')
        ax.set_ylabel('Mean Data Value (%)')
        ax.set_title('Geographic Risk Patterns Across States', fontsize=16, pad=20)
        ax.set_xticks(range(len(states)))
        ax.set_xticklabels(states, rotation=45)
        ax.grid(True, alpha=0.3, axis='y')
        
        # Intelligent legend creation based on actual data
        from matplotlib.patches import Patch
        legend_elements = []
        if any(state in high_risk for state in states):
            legend_elements.append(Patch(facecolor=risk_colors['high'], alpha=0.7, label='High Risk States'))
        if any(state in low_risk for state in states):
            legend_elements.append(Patch(facecolor=risk_colors['low'], alpha=0.7, label='Low Risk States'))
        if any(state not in high_risk and state not in low_risk for state in states):
            legend_elements.append(Patch(facecolor=risk_colors['neutral'], alpha=0.7, label='Other States'))
        
        if legend_elements:
            ax.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/geographic_patterns.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("      ‚úÖ Geographic patterns plot saved")
    
    def _create_topic_analysis_plot(self, topic_analysis):
        """Create health topic analysis visualization"""
        topic_stats = topic_analysis.get('topic_statistics', {})
        cognitive_metrics = topic_analysis.get('cognitive_metrics', {})
        
        if not topic_stats:
            return
        
        # Intelligently determine number of topics to show
        all_topics = list(topic_stats['mean'].keys())
        num_topics = self._calculate_optimal_display_count(len(all_topics), 'topics')
        topics = all_topics[:num_topics]
        means = [topic_stats['mean'][topic] for topic in topics]
        counts = [topic_stats['count'][topic] for topic in topics] if 'count' in topic_stats else [1] * len(topics)
        
        # Intelligent text truncation based on actual topic lengths
        truncation_length = self._calculate_text_truncation_length(topics)
        short_topics = [topic[:truncation_length] + '...' if len(topic) > truncation_length else topic for topic in topics]
        
        # Dynamic figure sizing based on number of topics
        fig_size = self._calculate_figure_size(num_topics, 'topic_horizontal')
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=fig_size)
        
        # Get intelligent colors
        colors = self._get_topic_color_palette()
        
        # Topic prevalence
        bars = ax1.barh(range(len(short_topics)), means, alpha=0.7, 
                        color=colors['prevalence'], edgecolor='black')
        ax1.set_xlabel('Mean Prevalence (%)')
        ax1.set_ylabel('Health Topics')
        ax1.set_title('Health Topic Prevalence Rates', fontsize=14)
        ax1.set_yticks(range(len(short_topics)))
        ax1.set_yticklabels(short_topics)
        ax1.grid(True, alpha=0.3, axis='x')
        
        # Sample sizes
        ax2.barh(range(len(short_topics)), counts, alpha=0.7,
                color=colors['sample_size'], edgecolor='black')
        ax2.set_xlabel('Number of Records')
        ax2.set_ylabel('Health Topics')
        ax2.set_title('Sample Sizes by Topic', fontsize=14)
        ax2.set_yticks(range(len(short_topics)))
        ax2.set_yticklabels(short_topics)
        ax2.grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/topic_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Cognitive health specific plot
        if cognitive_metrics and cognitive_metrics.get('records', 0) > 0:
            # Dynamic figure sizing for cognitive metrics
            fig_size = self._calculate_figure_size(3, 'cognitive_summary')
            fig, ax = plt.subplots(1, 1, figsize=fig_size)
            
            records = cognitive_metrics.get('records', 0)
            mean_value = cognitive_metrics.get('mean_value', 0)
            median_value = cognitive_metrics.get('median_value', 0)
            
            categories = ['Records\n(thousands)', 'Mean Value', 'Median Value']
            values = [records / 1000, mean_value, median_value]
            
            # Intelligent color selection
            topic_colors = self._get_topic_color_palette()
            colors = [topic_colors['cognitive'], topic_colors['health'], topic_colors['sample_size']]
            
            bars = ax.bar(categories, values, color=colors, alpha=0.7, edgecolor='black')
            ax.set_title('Cognitive Health Analysis Summary', fontsize=16, pad=20)
            ax.set_ylabel('Value')
            ax.grid(True, alpha=0.3, axis='y')
            
            plt.tight_layout()
            plt.savefig('outputs/visualizations/cognitive_health_summary.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        self.logger.info("      ‚úÖ Topic analysis plots saved")
    
    def _create_risk_stratification_plot(self, risk_scores):
        """Create risk stratification visualization"""
        high_risk = risk_scores.get('high_risk', {})
        low_risk = risk_scores.get('low_risk', {})
        
        if not high_risk or not low_risk:
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # High-risk states
        hr_states = list(high_risk['mean'].keys())
        hr_means = list(high_risk['mean'].values())
        hr_risks = list(high_risk['risk_score'].values())
        
        bars1 = ax1.bar(hr_states, hr_means, alpha=0.7, color='red', edgecolor='black')
        ax1.set_xlabel('High-Risk States')
        ax1.set_ylabel('Mean Data Value (%)')
        ax1.set_title('High-Risk States', fontsize=14, pad=15)
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3, axis='y')
        
        # Low-risk states
        lr_states = list(low_risk['mean'].keys())
        lr_means = list(low_risk['mean'].values())
        
        bars2 = ax2.bar(lr_states, lr_means, alpha=0.7, color='green', edgecolor='black')
        ax2.set_xlabel('Low-Risk States')
        ax2.set_ylabel('Mean Data Value (%)')
        ax2.set_title('Low-Risk States', fontsize=14, pad=15)
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/risk_stratification.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("      ‚úÖ Risk stratification plot saved")
    
    def _create_surveillance_dashboard(self, analysis_results):
        """Create comprehensive surveillance dashboard"""
        # Dynamic dashboard sizing
        fig_size = self._calculate_figure_size(8, 'dashboard')
        fig = plt.figure(figsize=fig_size)
        
        # Get key metrics
        total_subjects = analysis_results.get('data_summary', {}).get('total_subjects', 0)
        temporal = analysis_results.get('temporal_analysis', {})
        geographic = analysis_results.get('geographic_analysis', {})
        topic = analysis_results.get('topic_analysis', {})
        
        # Title
        fig.suptitle('Population Health Surveillance Dashboard', fontsize=24, fontweight='bold', y=0.95)
        
        # Key metrics panels
        ax1 = plt.subplot(2, 4, 1)
        ax1.text(0.5, 0.7, f'{total_subjects:,}', ha='center', va='center',
                fontsize=24, fontweight='bold', color='darkblue')
        ax1.text(0.5, 0.3, 'Total Records\nAnalyzed', ha='center', va='center',
                fontsize=12, fontweight='bold')
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)
        ax1.axis('off')
        
        # Temporal trend summary
        ax2 = plt.subplot(2, 4, 2)
        trend = temporal.get('trend', {})
        r_squared = trend.get('r_squared', 0)
        direction = trend.get('direction', 'unknown')
        ax2.text(0.5, 0.7, f'R¬≤ = {r_squared:.3f}', ha='center', va='center',
                fontsize=18, fontweight='bold', color='green' if r_squared > 0.5 else 'orange')
        ax2.text(0.5, 0.3, f'Temporal Trend\n({direction})', ha='center', va='center',
                fontsize=12, fontweight='bold')
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)
        ax2.axis('off')
        
        # Geographic summary
        ax3 = plt.subplot(2, 4, 3)
        high_risk_count = len(geographic.get('high_risk_states', []))
        ax3.text(0.5, 0.7, f'{high_risk_count}', ha='center', va='center',
                fontsize=24, fontweight='bold', color='red')
        ax3.text(0.5, 0.3, 'High-Risk\nStates', ha='center', va='center',
                fontsize=12, fontweight='bold')
        ax3.set_xlim(0, 1)
        ax3.set_ylim(0, 1)
        ax3.axis('off')
        
        # Cognitive health records
        ax4 = plt.subplot(2, 4, 4)
        cog_records = topic.get('cognitive_metrics', {}).get('records', 0)
        ax4.text(0.5, 0.7, f'{cog_records:,}', ha='center', va='center',
                fontsize=20, fontweight='bold', color='purple')
        ax4.text(0.5, 0.3, 'Cognitive Health\nRecords', ha='center', va='center',
                fontsize=12, fontweight='bold')
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.axis('off')
        
        # Mini plots in bottom row
        if temporal.get('yearly_trends') and temporal['yearly_trends'].get('mean'):
            ax5 = plt.subplot(2, 4, 5)
            try:
                trends = temporal['yearly_trends']
                years = sorted([int(y) for y in trends['mean'].keys()])
                means = [trends['mean'][str(y)] for y in years]
                ax5.plot(years, means, 'o-', linewidth=2, markersize=6, color='steelblue')
                ax5.set_title('Temporal Trend', fontweight='bold')
                ax5.grid(True, alpha=0.3)
            except (KeyError, ValueError) as e:
                self.logger.warning(f"      ‚ö†Ô∏è Error creating mini temporal plot: {e}")
                ax5 = plt.subplot(2, 4, 5)
                ax5.text(0.5, 0.5, 'Temporal data\nprocessing error', ha='center', va='center')
                ax5.set_title('Temporal Trend', fontweight='bold')
                ax5.axis('off')
        
        # Mini geographic plot
        if geographic.get('state_statistics'):
            ax6 = plt.subplot(2, 4, 6)
            all_states = list(geographic['state_statistics']['mean'].keys())
            display_count = self._calculate_optimal_display_count(len(all_states), 'states')
            states = all_states[:min(display_count, 8)]  # Limit for dashboard space
            means = [geographic['state_statistics']['mean'][s] for s in states]
            
            # Use intelligent colors
            high_risk = geographic.get('high_risk_states', [])
            low_risk = geographic.get('low_risk_states', [])
            risk_colors = self._get_risk_color_palette()
            colors = []
            for s in states:
                if s in high_risk:
                    colors.append(risk_colors['high'])
                elif s in low_risk:
                    colors.append(risk_colors['low'])
                else:
                    colors.append(risk_colors['neutral'])
                    
            ax6.bar(range(len(states)), means, color=colors, alpha=0.7)
            ax6.set_title('Geographic Patterns', fontweight='bold')
            ax6.set_xticks(range(len(states)))
            ax6.set_xticklabels(states, rotation=45, fontsize=8)
        
        # Analysis summary text
        ax7 = plt.subplot(2, 4, (7, 8))
        summary_text = f"""
SURVEILLANCE ANALYSIS SUMMARY:

‚Ä¢ Population health surveillance dataset analyzed
‚Ä¢ {total_subjects:,} total health records processed
‚Ä¢ {high_risk_count} high-risk geographic regions identified  
‚Ä¢ {cog_records:,} cognitive health indicators analyzed
‚Ä¢ Temporal trend: {direction} pattern (R¬≤={r_squared:.3f})
‚Ä¢ Geographic health disparities detected and quantified
‚Ä¢ Multi-modal health surveillance indicators processed
‚Ä¢ Population-level risk stratification completed
‚Ä¢ Predictive modeling framework applied

This analysis provides comprehensive surveillance insights
for population health monitoring and intervention planning.
        """
        ax7.text(0.05, 0.95, summary_text, ha='left', va='top', fontsize=11,
                transform=ax7.transAxes)
        ax7.set_xlim(0, 1)
        ax7.set_ylim(0, 1)
        ax7.axis('off')
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.92)
        plt.savefig('outputs/visualizations/surveillance_dashboard.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("      ‚úÖ Surveillance dashboard saved")


def main():
    """Test the cognitive analysis agent"""
    agent = CognitiveAnalysisAgent()
    results = agent.run_complete_analysis()
    agent.print_analysis_summary(results)
    return results


if __name__ == "__main__":
    main()