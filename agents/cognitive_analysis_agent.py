#!/usr/bin/env python3
"""
Cognitive Analysis Agent
========================

Generalizable autonomous agent for analyzing relationships between cognitive assessments
in Alzheimer's research. Works with any cognitive assessment combination through 
configuration-driven analysis.

Key Features:
- Dataset agnostic: Works with any Alzheimer's dataset structure
- Assessment flexible: Supports any cognitive assessment combination
- Configuration-driven: All specifics defined in config files
- Extensible: Easy plugin architecture for custom analyses
- Automatically optimizes decision thresholds for imbalanced datasets
- Warns when default 0.5 threshold severely underperforms
- Recognizes performance ceilings and advises when to stop model optimization
- Recommends appropriate model types based on data characteristics
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr, spearmanr, mannwhitneyu, ttest_ind, norm
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
import json
import yaml
import os
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import logging
import warnings
warnings.filterwarnings('ignore')

# Import enhancement module if available
try:
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from cognitive_agent_enhancements import EnhancedCDRPredictor, integrate_enhancements
    ENHANCEMENTS_AVAILABLE = True
except ImportError:
    ENHANCEMENTS_AVAILABLE = False

# Import F1-focused clinical evaluation system
try:
    sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'improvements'))
    from clinical_evaluation_metrics import ClinicalEvaluator
    F1_EVALUATION_AVAILABLE = True
except ImportError:
    F1_EVALUATION_AVAILABLE = False

# Enhanced data merging with Cartesian join prevention
try:
    from enhanced_data_merging import EnhancedDataMerger, smart_merge_datasets
    ENHANCED_MERGING_AVAILABLE = True
except ImportError:
    ENHANCED_MERGING_AVAILABLE = False
# Multiple-testing correction
try:
    from statsmodels.stats.multitest import multipletests
    STATSMODELS_AVAILABLE = True
except Exception:
    STATSMODELS_AVAILABLE = False

# Dataset adapters
try:
    sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'core'))
    from datasets import get_adapter
    ADAPTERS_AVAILABLE = True
except Exception:
    ADAPTERS_AVAILABLE = False

class CognitiveAnalysisAgent:
    """
    Generalizable analysis agent for cognitive assessment correlation studies
    
    This agent is completely dataset and assessment agnostic. It discovers
    cognitive assessments from your data based on configuration patterns
    and performs comprehensive analysis on any combination of:
    - Self-report questionnaires
    - Digital cognitive assessments  
    - Informant reports
    - Performance measures
    - Biomarkers
    
    All analysis types and variable mappings are defined in configuration
    files, making it easy to adapt to any Alzheimer's research dataset.
    
    IMPORTANT: This agent follows strict ML methodology guidelines:
    - Always uses train/test split for final evaluation
    - Never evaluates models on training data
    - Applies proper cross-validation for hyperparameter selection
    - Prevents data leakage in feature selection and preprocessing
    - Automatically optimizes decision thresholds for imbalanced datasets
    - Warns when default 0.5 threshold severely underperforms
    - Recognizes performance ceilings and advises when to stop model optimization
    - Recommends appropriate model types based on data characteristics
    """
    
    def __init__(self, config_path: str = "config/config.yaml",
                 data_dict_path: str = "config/data_dictionary.json",
                 discovery_results_path: str = "outputs/dataset_discovery_results.json"):
        
        self.config_path = config_path
        self.data_dict_path = data_dict_path
        self.discovery_results_path = discovery_results_path
        
        # Load configurations
        self.config = self._load_config()
        self.data_dict = self._load_data_dictionary()
        self.discovery_results = self._load_discovery_results()
        
        # Initialize data containers
        self.assessment_data = {}  # Dictionary of loaded assessment datasets
        self.combined_data = None
        
        # Setup logging
        self._setup_logging()
        
        # ML methodology validation flags and tracking
        self.enforce_ml_best_practices = True
        self.ml_validation_warnings = []
        
        # Get experiment configuration
        self.experiment_config = self.config.get('experiment', {})
        self.analysis_config = self.config.get('analysis', {})
        
        self.logger.info(f"ðŸ§  Cognitive Analysis Agent initialized for experiment: {self.experiment_config.get('name', 'Unknown')}")
    
    def _load_config(self) -> Dict[str, Any]:
        """Load main configuration file"""
        try:
            with open(self.config_path, 'r') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"Warning: Could not load config {self.config_path}: {e}")
            return {}
    
    def _load_data_dictionary(self) -> Dict[str, Any]:
        """Load data dictionary for variable mapping"""
        try:
            with open(self.data_dict_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Warning: Could not load data dictionary {self.data_dict_path}: {e}")
            return {}
    
    def _load_discovery_results(self) -> Dict[str, Any]:
        """Load dataset discovery results"""
        try:
            with open(self.discovery_results_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Warning: Could not load discovery results {self.discovery_results_path}: {e}")
            return {}
    
    def _setup_logging(self):
        """Setup logging system"""
        log_config = self.config.get('logging', {})
        log_level = getattr(logging, log_config.get('level', 'INFO').upper())
        
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def _validate_ml_methodology(self, action: str, details: Dict[str, Any] = None) -> bool:
        """
        Validate ML methodology to prevent common issues like training set evaluation.
        
        Args:
            action: The ML action being performed (e.g., 'train', 'evaluate', 'cross_validate')
            details: Additional details about the action
            
        Returns:
            bool: True if methodology is valid, False otherwise
        """
        if not self.enforce_ml_best_practices:
            return True
            
        valid = True
        warning_msg = None
        
        if action == 'evaluate_on_training':
            warning_msg = "âš ï¸ ML METHODOLOGY WARNING: Evaluating model on training data detected! This inflates metrics by 0.05-0.15 AUC."
            valid = False
            
        elif action == 'no_test_split':
            warning_msg = "âš ï¸ ML METHODOLOGY WARNING: No train/test split detected! Always use holdout test set for final evaluation."
            valid = False
            
        elif action == 'feature_selection_before_split':
            warning_msg = "âš ï¸ ML METHODOLOGY WARNING: Feature selection before train/test split causes data leakage!"
            valid = False
            
        elif action == 'full_data_calibration':
            warning_msg = "âš ï¸ ML METHODOLOGY WARNING: Model calibration on full dataset detected! Calibrate only on training data."
            valid = False
            
        elif action == 'suboptimal_threshold':
            # This is informational, not necessarily invalid
            if details and details.get('improvement_possible', 0) > 0.20:
                warning_msg = f"âš ï¸ THRESHOLD WARNING: Default 0.5 threshold severely underperforms! Sensitivity can improve from {details.get('default_sensitivity', 0):.1%} to {details.get('optimal_sensitivity', 0):.1%} with threshold optimization."
                valid = True  # Not invalid, just suboptimal
            
        elif action == 'train_test_split':
            # This is good practice, just log it
            valid = True
            
        elif action == 'cross_validate':
            # This is good practice, just log it  
            valid = True
            
        if warning_msg:
            self.logger.warning(warning_msg)
            self.ml_validation_warnings.append(warning_msg)
            if details:
                self.logger.warning(f"   Details: {details}")
                
        return valid
    
    def _validate_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        Enhanced data quality validation based on learnings from analysis sessions.
        
        Args:
            data: DataFrame to validate
            
        Returns:
            Dictionary with validation results and recommendations
        """
        validation_results = {
            'gender_coding_valid': True,
            'age_calculation_valid': True,
            'memtrax_filtering_applied': False,
            'data_dictionary_consistent': True,
            'warnings': [],
            'recommendations': []
        }
        
        # 1. Gender coding validation
        if 'Gender' in data.columns or 'Profile.Gender' in data.columns:
            gender_col = 'Gender' if 'Gender' in data.columns else 'Profile.Gender'
            unique_genders = data[gender_col].unique()
            
            # Check for expected coding (0=Male, 1=Female per data dictionary)
            if set(unique_genders).issubset({0, 1, '0', '1'}):
                validation_results['gender_coding_valid'] = True
            elif set(unique_genders).issubset({1, 2, '1', '2'}):
                validation_results['gender_coding_valid'] = False
                validation_results['warnings'].append(
                    "Gender coding appears inverted (1=Male, 2=Female). Data dictionary specifies 0=Male, 1=Female."
                )
                validation_results['recommendations'].append(
                    "Verify gender coding against data dictionary: BHR-ALL-EXT_Mem_2022/DataDictionary/DataDictionary.csv"
                )
        
        # 2. Age calculation validation
        age_columns = ['Age_Baseline', 'Age', 'age']
        timepoint_columns = ['TimepointCode', 'timepoint']
        
        if any(col in data.columns for col in age_columns) and any(col in data.columns for col in timepoint_columns):
            validation_results['age_calculation_valid'] = True
        else:
            validation_results['warnings'].append(
                "Age calculation may be incomplete. Primary method: Age_Baseline + TimepointCode (months). Fallback: StatusDateTime - BaselineDate"
            )
            validation_results['recommendations'].append(
                "Implement proper age calculation using Age_Baseline + TimepointCode for longitudinal data"
            )
        
        # 3. MemTrax data filtering validation
        memtrax_indicators = ['CorrectPCT', 'CorrectResponsesRT', 'Status']
        if any(col in data.columns for col in memtrax_indicators):
            if 'Status' in data.columns:
                status_counts = data['Status'].value_counts()
                if 'Collected' in status_counts:
                    validation_results['memtrax_filtering_applied'] = True
                else:
                    validation_results['warnings'].append(
                        "MemTrax data not filtered to 'Collected' status. Apply Ashford filter: Status == 'Collected'"
                    )
                    validation_results['recommendations'].append(
                        "Apply MemTrax quality filter: Status == 'Collected', CorrectPCT >= 0.60, RT in [0.5, 2.5]"
                    )
        
        # 4. Data dictionary consistency check
        if hasattr(self, 'data_dictionary') and self.data_dictionary:
            validation_results['data_dictionary_consistent'] = True
        else:
            validation_results['warnings'].append(
                "Data dictionary not loaded. Always check BHR-ALL-EXT_Mem_2022/DataDictionary/DataDictionary.csv for field definitions"
            )
            validation_results['recommendations'].append(
                "Load and validate against authoritative data dictionary for field definitions"
            )
        
        return validation_results
    
    def _engineer_advanced_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Enhanced feature engineering based on learnings from analysis sessions.
        
        Args:
            data: Input DataFrame
            
        Returns:
            DataFrame with additional engineered features
        """
        enhanced_data = data.copy()
        
        # 1. Hit RT vs All-Click RT features
        if 'CorrectResponsesRT' in data.columns and 'ReactionTimes' in data.columns:
            # Use hit-only RTs for sequence features to avoid diluting signal
            enhanced_data['hit_rt_mean'] = data['CorrectResponsesRT'].mean()
            enhanced_data['hit_rt_std'] = data['CorrectResponsesRT'].std()
            enhanced_data['hit_rt_cv'] = enhanced_data['hit_rt_std'] / (enhanced_data['hit_rt_mean'] + 1e-8)
            
            # All-click RT for comparison
            enhanced_data['all_click_rt_mean'] = data['ReactionTimes'].mean()
            enhanced_data['rt_difference'] = enhanced_data['all_click_rt_mean'] - enhanced_data['hit_rt_mean']
        
        # 2. Age normalization features
        if 'Age_Baseline' in data.columns:
            # Create age bins for normalization
            age_bins = [0, 45, 55, 65, 75, 85, 100]
            age_labels = ['<45', '45-54', '55-64', '65-74', '75-84', '85+']
            enhanced_data['age_bin'] = pd.cut(data['Age_Baseline'], bins=age_bins, labels=age_labels, right=False)
            
            # Age-bin normalized z-scores for cognitive features
            cognitive_features = ['CorrectPCT', 'CorrectResponsesRT', 'CorrectResponsesPCT']
            for feature in cognitive_features:
                if feature in data.columns:
                    # Calculate z-scores within age bins to isolate cognitive signal
                    enhanced_data[f'{feature}_age_normalized'] = enhanced_data.groupby('age_bin')[feature].transform(
                        lambda x: (x - x.mean()) / (x.std() + 1e-8)
                    )
        
        # 3. Signal Detection Theory (SDT) features
        if 'CorrectResponsesPCT' in data.columns and 'CorrectRejectionsPCT' in data.columns:
            hit_rate = data['CorrectResponsesPCT'] / 100.0
            fa_rate = 1.0 - (data['CorrectRejectionsPCT'] / 100.0)
            
            # Avoid extreme values for d' calculation
            hit_rate = np.clip(hit_rate, 0.01, 0.99)
            fa_rate = np.clip(fa_rate, 0.01, 0.99)
            
            # d' (d-prime) - sensitivity measure
            enhanced_data['d_prime'] = (norm.ppf(hit_rate) - norm.ppf(fa_rate))
            
            # c (criterion) - bias measure
            enhanced_data['criterion_c'] = -0.5 * (norm.ppf(hit_rate) + norm.ppf(fa_rate))
        
        # 4. Speed-Accuracy tradeoff features
        if 'CorrectPCT' in data.columns and 'CorrectResponsesRT' in data.columns:
            # Speed-accuracy product (higher is better)
            enhanced_data['speed_accuracy_product'] = (data['CorrectPCT'] / 100.0) * (1.0 / (data['CorrectResponsesRT'] + 1e-8))
            
            # Speed-accuracy ratio
            enhanced_data['speed_accuracy_ratio'] = data['CorrectPCT'] / (data['CorrectResponsesRT'] + 1e-8)
        
        # 5. Education-Accuracy interaction features
        if 'Education' in data.columns and 'CorrectPCT' in data.columns:
            enhanced_data['education_accuracy_interaction'] = data['Education'] * data['CorrectPCT']
            enhanced_data['education_rt_interaction'] = data['Education'] * data.get('CorrectResponsesRT', 0)
        
        # 6. Device/Browser context features (if available)
        device_cols = ['DeviceType', 'Browser', 'OperatingSystem', 'Language']
        for col in device_cols:
            if col in data.columns:
                # One-hot encode device types
                dummies = pd.get_dummies(data[col], prefix=col.lower())
                enhanced_data = pd.concat([enhanced_data, dummies], axis=1)
        
        return enhanced_data
    
    def _detect_performance_plateau(self, model_results: Dict[str, float], 
                                  feature_counts: List[int]) -> Dict[str, Any]:
        """
        Detect performance plateau patterns based on learnings from analysis sessions.
        
        Args:
            model_results: Dictionary of model performance metrics
            feature_counts: List of feature counts used in different experiments
            
        Returns:
            Dictionary with plateau analysis and recommendations
        """
        plateau_analysis = {
            'plateau_detected': False,
            'plateau_confidence': 0.0,
            'evidence': [],
            'recommendations': []
        }
        
        if len(model_results) < 3:
            return plateau_analysis
        
        # Extract AUC values and sort by feature count
        auc_values = []
        feature_nums = []
        
        for name, auc in model_results.items():
            # Extract feature count from model name if possible
            if 'features' in name.lower():
                try:
                    feature_count = int(''.join(filter(str.isdigit, name.split('features')[0])))
                    feature_nums.append(feature_count)
                    auc_values.append(auc)
                except:
                    continue
        
        if len(auc_values) < 3:
            return plateau_analysis
        
        # Sort by feature count
        sorted_data = sorted(zip(feature_nums, auc_values))
        feature_nums, auc_values = zip(*sorted_data)
        
        # Check for plateau pattern (AUC plateau around 0.755-0.760)
        auc_range = max(auc_values) - min(auc_values)
        mean_auc = np.mean(auc_values)
        
        plateau_indicators = 0
        
        # 1. Small performance range despite feature additions
        if auc_range < 0.01:
            plateau_indicators += 1
            plateau_analysis['evidence'].append(
                f"Performance range only {auc_range:.3f} AUC despite {len(feature_nums)} different feature sets"
            )
        
        # 2. Performance around expected plateau (0.755-0.760)
        if 0.750 <= mean_auc <= 0.765:
            plateau_indicators += 1
            plateau_analysis['evidence'].append(
                f"Mean AUC {mean_auc:.3f} is in expected plateau range (0.755-0.760)"
            )
        
        # 3. Diminishing returns from feature engineering
        if len(auc_values) >= 4:
            recent_improvement = auc_values[-1] - auc_values[-4]
            if recent_improvement < 0.005:
                plateau_indicators += 1
                plateau_analysis['evidence'].append(
                    f"Recent feature additions show minimal improvement: {recent_improvement:.3f} AUC"
                )
        
        # Determine plateau confidence
        plateau_analysis['plateau_confidence'] = plateau_indicators / 3.0
        plateau_analysis['plateau_detected'] = plateau_analysis['plateau_confidence'] >= 0.67
        
        if plateau_analysis['plateau_detected']:
            plateau_analysis['recommendations'].extend([
                "Focus on data quality improvements rather than feature engineering",
                "Implement baseline-only subject selection to reduce label noise",
                "Explore within-test dynamics and per-item trajectory features",
                "Consider cross-session stability analysis",
                "Validate label quality and outcome definitions",
                "Try different model architectures or ensemble methods"
            ])
        
        return plateau_analysis
    
    def _generate_improvement_recommendations(self, data: pd.DataFrame, 
                                            model_results: Dict[str, float]) -> Dict[str, Any]:
        """
        Generate comprehensive improvement recommendations based on learnings from analysis sessions.
        
        Args:
            data: Input DataFrame
            model_results: Dictionary of model performance metrics
            
        Returns:
            Dictionary with improvement recommendations organized by category
        """
        recommendations = {
            'data_quality_improvements': [],
            'feature_engineering_improvements': [],
            'model_improvements': [],
            'technical_improvements': [],
            'next_steps': []
        }
        
        # 1. Data Quality Improvements
        recommendations['data_quality_improvements'] = [
            "Implement baseline-only subject selection to reduce label noise",
            "Validate gender coding against data dictionary (0=Male, 1=Female)",
            "Apply proper age calculation: Age_Baseline + TimepointCode (months)",
            "Enforce MemTrax Ashford filter: Status == 'Collected', CorrectPCT >= 0.60, RT in [0.5, 2.5]",
            "Cross-validate labels across multiple sources (self-report, informant, objective)",
            "Check for selection bias in highly educated populations (70%+ college degrees)",
            "Validate against expected MCI prevalence by age group"
        ]
        
        # 2. Feature Engineering Improvements
        recommendations['feature_engineering_improvements'] = [
            "Use hit-only RTs (CorrectResponsesRT) for sequence features, not all clicks",
            "Implement age-bin normalized z-scores to isolate cognitive signal",
            "Add Signal Detection Theory features: d_prime, criterion_c",
            "Create speed-accuracy tradeoff features: product, ratio",
            "Add education-accuracy interaction features",
            "Explore within-test dynamics: per-item response patterns, early vs late performance",
            "Implement cross-session stability analysis",
            "Add device/browser context features (one-hot encoded)",
            "Create composite cognitive scores: RT/(accuracy+0.01)"
        ]
        
        # 3. Model Improvements
        recommendations['model_improvements'] = [
            "Try traditional ML (Logistic Regression, Random Forest) before neural networks",
            "Implement proper hyperparameter optimization",
            "Use ensemble methods with different algorithms",
            "Apply threshold optimization for imbalanced datasets",
            "Consider model calibration for probability outputs",
            "Test different train/test split ratios (70/30, 80/20)",
            "Use stratified sampling to maintain class distribution"
        ]
        
        # 4. Technical Improvements
        recommendations['technical_improvements'] = [
            "Load authoritative data dictionary for field definitions",
            "Implement proper environment management for package installation",
            "Optimize permutation importance calculation (major bottleneck)",
            "Add comprehensive logging for debugging",
            "Implement data validation at each processing step",
            "Create automated data quality reports",
            "Add performance monitoring and alerting"
        ]
        
        # 5. Next Steps (based on current performance)
        if model_results:
            best_auc = max(model_results.values())
            
            if best_auc < 0.70:
                recommendations['next_steps'].extend([
                    "Focus on data quality - current performance suggests fundamental issues",
                    "Verify data preprocessing and feature engineering",
                    "Check for data leakage or methodological issues",
                    "Consider simpler baseline models first"
                ])
            elif 0.70 <= best_auc <= 0.80:
                recommendations['next_steps'].extend([
                    "Performance is in expected range for cognitive assessment data",
                    "Focus on threshold optimization for clinical utility",
                    "Consider data quality improvements over model complexity",
                    "Validate findings with external datasets"
                ])
            elif best_auc > 0.80:
                recommendations['next_steps'].extend([
                    "High performance detected - verify no data leakage",
                    "Check for unrealistic performance indicators",
                    "Validate against clinical expectations",
                    "Consider if performance is too good to be true"
                ])
        
        # Add specific recommendations based on data characteristics
        if 'MemTrax' in str(data.columns):
            recommendations['next_steps'].extend([
                "Leverage rich per-item MemTrax data (50 items per test)",
                "Explore temporal dynamics within 90-120 second tests",
                "Analyze response patterns and fatigue effects",
                "Consider multiple timepoint aggregation strategies"
            ])
        
        return recommendations
    
    def _check_performance_ceiling(self, model_results: Dict[str, float], 
                                  n_samples: int, n_features: int) -> Dict[str, Any]:
        """
        Check if performance has hit a ceiling and provide recommendations.
        
        Args:
            model_results: Dictionary of model names to their AUC scores
            n_samples: Number of samples in dataset
            n_features: Number of features
            
        Returns:
            Dictionary with ceiling analysis and recommendations
        """
        ceiling_analysis = {
            'likely_at_ceiling': False,
            'confidence': 0.0,
            'recommendations': [],
            'evidence': []
        }
        
        if not model_results:
            return ceiling_analysis
            
        # Calculate performance statistics
        auc_values = list(model_results.values())
        best_auc = max(auc_values)
        worst_auc = min(auc_values)
        mean_auc = np.mean(auc_values)
        std_auc = np.std(auc_values)
        
        # Check for performance convergence
        performance_range = best_auc - worst_auc
        
        # Criteria for ceiling detection
        ceiling_indicators = 0
        
        # 1. Tight performance range across different models
        if performance_range < 0.05:
            ceiling_indicators += 1
            ceiling_analysis['evidence'].append(
                f"All models within {performance_range:.3f} AUC range"
            )
        
        # 2. Simple model performs as well as complex
        if 'LogisticRegression' in model_results and 'Ensemble' in model_results:
            lr_auc = model_results['LogisticRegression']
            ens_auc = model_results['Ensemble']
            if abs(lr_auc - ens_auc) < 0.02:
                ceiling_indicators += 1
                ceiling_analysis['evidence'].append(
                    f"Simple LR ({lr_auc:.3f}) â‰ˆ Complex ensemble ({ens_auc:.3f})"
                )
        
        # 3. Low standard deviation
        if std_auc < 0.02:
            ceiling_indicators += 1
            ceiling_analysis['evidence'].append(
                f"Low variance across models (Ïƒ={std_auc:.3f})"
            )
        
        # 4. Check if neural networks would be appropriate
        nn_appropriate = n_samples > 50000 and n_features > 100
        if not nn_appropriate and n_samples < 100000:
            ceiling_analysis['evidence'].append(
                f"Dataset too small for neural networks ({n_samples:,} samples, {n_features} features)"
            )
        
        # Check for plateau pattern (AUC plateau around 0.755-0.760)
        plateau_analysis = self._detect_performance_plateau(model_results, [n_features])
        if plateau_analysis['plateau_detected']:
            ceiling_indicators += 1
            ceiling_analysis['evidence'].extend(plateau_analysis['evidence'])
            ceiling_analysis['recommendations'].extend(plateau_analysis['recommendations'])
        
        # Determine if at ceiling
        if ceiling_indicators >= 2:
            ceiling_analysis['likely_at_ceiling'] = True
            ceiling_analysis['confidence'] = min(ceiling_indicators / 3.0, 1.0)
            
            # Provide recommendations
            ceiling_analysis['recommendations'].extend([
                "âœ… Performance has likely reached its ceiling",
                f"âœ… Best achievable AUC appears to be ~{best_auc:.3f}",
                "ðŸ“Š Focus on data quality rather than model complexity:",
                "   - Obtain better ground truth labels",
                "   - Increase sample size",
                "   - Reduce selection bias",
                "   - Consider external validation",
                "ðŸŽ¯ Optimize decision thresholds for deployment",
                "âš ï¸ Further model optimization unlikely to help"
            ])
            
            # Add realistic performance ceiling guidance
            if best_auc > 0.85:
                ceiling_analysis['recommendations'].extend([
                    "ðŸš¨ HIGH PERFORMANCE WARNING:",
                    "   - AUC >0.85 in cognitive impairment prediction may indicate data leakage",
                    "   - Realistic ceiling for self-reported labels: 0.75-0.80 AUC",
                    "   - Verify no cognitive assessment features are included",
                    "   - Check for temporal leakage or other data issues"
                ])
            
            self.logger.info(f"   ðŸ“Š Performance ceiling detected at AUC={best_auc:.3f}")
        else:
            # Not at ceiling, suggest improvements
            ceiling_analysis['recommendations'].extend([
                "ðŸ“ˆ Performance may still be improvable",
                "Try: ensemble methods, hyperparameter tuning, feature engineering"
            ])
            
            if nn_appropriate:
                ceiling_analysis['recommendations'].append(
                    "Consider: Neural networks might help with this data size"
                )
        
        return ceiling_analysis

    def _detect_potential_leakage(self, feature_names: List[str], target_name: str) -> Dict[str, Any]:
        """
        Detect potential data leakage based on feature names and target.
        
        Args:
            feature_names: List of feature column names
            target_name: Name of the target variable
            
        Returns:
            Dictionary with leakage analysis and recommendations
        """
        leakage_indicators = []
        warnings = []
        recommendations = []
        
        # Check for cognitive assessment features when predicting cognitive impairment
        # These features can cause leakage because they measure the same construct as the target
        cognitive_assessment_keywords = [
            'ecog', 'sp_ecog', 'mmse', 'moca', 'cdr', 'adni', 'cog', 'cognitive',
            'memory', 'attention', 'executive', 'language', 'visuospatial', 'informant'
        ]
        
        target_cognitive_keywords = [
            'mci', 'dementia', 'cognitive', 'impairment', 'cdr', 'adni'
        ]
        
        is_cognitive_target = any(keyword in target_name.lower() for keyword in target_cognitive_keywords)
        
        if is_cognitive_target:
            cognitive_features = []
            for feature in feature_names:
                if any(keyword in feature.lower() for keyword in cognitive_assessment_keywords):
                    cognitive_features.append(feature)
            
            if cognitive_features:
                leakage_indicators.append(f"Found {len(cognitive_features)} cognitive assessment features: {cognitive_features[:5]}")
                warnings.append("Using cognitive assessments as features to predict cognitive impairment may cause leakage")
                recommendations.append("Consider removing cognitive assessment features - this can actually IMPROVE performance by reducing noise")
                recommendations.append("Cognitive assessments measure different constructs than objective performance tests")
        
        # Check for medical history features when predicting medical conditions
        medical_keywords = ['medical', 'history', 'diagnosis', 'condition', 'disease', 'qid']
        target_medical_keywords = ['mci', 'dementia', 'alzheimer', 'cognitive', 'impairment']
        
        is_medical_target = any(keyword in target_name.lower() for keyword in target_medical_keywords)
        
        if is_medical_target:
            medical_features = []
            for feature in feature_names:
                if any(keyword in feature.lower() for keyword in medical_keywords):
                    medical_features.append(feature)
            
            if medical_features:
                leakage_indicators.append(f"Found {len(medical_features)} medical history features: {medical_features[:5]}")
                warnings.append("Using medical history as features to predict medical conditions may cause leakage")
                recommendations.append("Consider using only objective performance measures and demographics")
        
        # Check for informant reports when predicting self-reported conditions
        informant_keywords = ['informant', 'sp_', 'partner', 'family', 'caregiver']
        self_report_keywords = ['self_', 'participant', 'subject']
        
        has_informant_features = any(keyword in ' '.join(feature_names).lower() for keyword in informant_keywords)
        is_self_report_target = any(keyword in target_name.lower() for keyword in self_report_keywords)
        
        if has_informant_features and is_self_report_target:
            leakage_indicators.append("Using informant reports to predict self-reported conditions")
            warnings.append("Informant reports may leak information about the target condition")
            recommendations.append("Consider using informant reports only for validation or as separate targets")
        
        # Check for temporal leakage
        temporal_keywords = ['future', 'later', 'followup', 'm12', 'm24', 'm36']
        has_temporal_features = any(keyword in ' '.join(feature_names).lower() for keyword in temporal_keywords)
        
        if has_temporal_features:
            leakage_indicators.append("Found temporal features that may leak future information")
            warnings.append("Features from future timepoints may cause temporal leakage")
            recommendations.append("Ensure all features are from baseline or earlier timepoints only")
        
        # Overall assessment
        has_leakage_risk = len(leakage_indicators) > 0
        risk_level = "HIGH" if len(leakage_indicators) >= 2 else "MEDIUM" if len(leakage_indicators) == 1 else "LOW"
        
        return {
            'has_leakage_risk': has_leakage_risk,
            'risk_level': risk_level,
            'indicators': leakage_indicators,
            'warnings': warnings,
            'recommendations': recommendations,
            'n_features_checked': len(feature_names)
        }
    
    def run_complete_analysis(self) -> Dict[str, Any]:
        """Execute complete cognitive assessment analysis pipeline - adaptive for different data types"""
        experiment_name = self.experiment_config.get('name', 'Cognitive_Assessment_Analysis')
        self.logger.info(f"ðŸš€ Starting Cognitive Analysis Pipeline: {experiment_name}")
        
        analysis_results = {
            'analysis_info': {
                'agent_type': 'CognitiveAnalysisAgent',
                'experiment_name': experiment_name,
                'start_time': datetime.now().isoformat(),
                'config': self.config
            },
            'data_summary': {},
            'assessment_analysis': {},
            'correlation_analysis': {},
            'self_informant_comparison': {},
            'cognitive_performance_analysis': {},
            'clinical_insights': {},
            'statistical_summary': {}
        }
        
        try:
            # Step 1: Load and preprocess data
            self.logger.info("Step 1: Data loading and preprocessing")
            data_summary = self._load_and_preprocess_data()
            analysis_results['data_summary'] = data_summary
            
            if self.combined_data is None or len(self.combined_data) == 0:
                raise ValueError("No data available for analysis")
            
            # NEW: Detect data type and adapt analysis strategy
            data_type = self._detect_data_type()
            self.logger.info(f"   ðŸ” Detected data type: {data_type}")
            
            if data_type == 'surveillance':
                # Run surveillance-specific analyses for population health data
                self.logger.info("   ðŸ“Š Running surveillance data analysis pipeline")
                analysis_results = self._run_surveillance_analysis(analysis_results)
            else:
                # Original individual-level cognitive assessment analysis
                # Step 2: Assessment-specific analysis
                assessment_types = self._identify_assessment_types()
                for assessment_type in assessment_types:
                    self.logger.info(f"Step 2.{assessment_type}: {assessment_type} analysis")
                    analysis_results['assessment_analysis'][assessment_type] = self._analyze_assessment_type(assessment_type)
            
            if data_type != 'surveillance':
                # Step 3: Cross-assessment correlation analysis (for individual data only)
                self.logger.info("Step 3: Cross-assessment correlation analysis")
                analysis_results['correlation_analysis'] = self._analyze_cross_assessment_correlations()
            
            if data_type != 'surveillance':
                # Step 4: Self vs informant comparison (if available)
                if self._has_self_informant_data():
                    self.logger.info("Step 4: Self vs informant comparison")
                    analysis_results['self_informant_comparison'] = self._analyze_self_informant_differences()
                
                # Step 5: Cognitive performance analysis
                self.logger.info("Step 5: Cognitive performance analysis")
                analysis_results['cognitive_performance_analysis'] = self._analyze_cognitive_performance()
                
                # Step 5b: MemTrax predictive modeling for cognitive impairment
                if self._has_memtrax_and_outcomes_data():
                    self.logger.info("Step 5b: MemTrax cognitive impairment prediction analysis")
                    analysis_results['memtrax_prediction'] = self._analyze_memtrax_predictive_power()
                
                # Step 5c: Advanced CDR prediction (if CDR column exists)
                if 'CDR' in self.combined_data.columns:
                    self.logger.info("Step 5c: Advanced CDR prediction with state-of-the-art ML")
                    analysis_results['advanced_cdr_prediction'] = self._advanced_cdr_prediction()
            
            # Step 6: Generate clinical insights
            self.logger.info("Step 6: Clinical insights generation")
            analysis_results['clinical_insights'] = self._generate_clinical_insights(analysis_results)
            
            # Step 7: Statistical summary
            analysis_results['statistical_summary'] = self._generate_statistical_summary(analysis_results)
            
            # Step 8: Generate improvement recommendations
            self.logger.info("Step 8: Generating improvement recommendations")
            model_results = {}
            if 'assessment_analysis' in analysis_results:
                for assessment_type, results in analysis_results['assessment_analysis'].items():
                    if 'model_performance' in results:
                        for model_name, metrics in results['model_performance'].items():
                            if 'auc' in metrics:
                                model_results[f"{assessment_type}_{model_name}"] = metrics['auc']
            
            analysis_results['improvement_recommendations'] = self._generate_improvement_recommendations(
                self.combined_data, model_results
            )
            
            # Step 9: Create visualizations
            self.logger.info("Step 9: Creating visualizations")
            self._create_analysis_visualizations(analysis_results)
            
            # Step 10: Save results
            analysis_results['analysis_info']['end_time'] = datetime.now().isoformat()
            self._save_analysis_results(analysis_results)
            
            self.logger.info("âœ… Cognitive analysis complete!")
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"Analysis failed: {e}")
            analysis_results['analysis_info']['error'] = str(e)
            analysis_results['analysis_info']['end_time'] = datetime.now().isoformat()
            return analysis_results
    
    def _load_and_preprocess_data(self) -> Dict[str, Any]:
        """Load and preprocess data via adapter if available; fallback to legacy OASIS flow."""
        data_summary = {
            'assessments_loaded': [],
            'total_subjects': 0,
            'baseline_subjects': 0,
            'preprocessing_steps': [],
            'data_quality_validation': {}
        }
        
        # Try dataset adapter first
        if ADAPTERS_AVAILABLE:
            try:
                adapter = get_adapter(self.config)
                if adapter and adapter.is_available():
                    self.logger.info(f"   ðŸ”Œ Using dataset adapter: {adapter.__class__.__name__}")
                    self.combined_data = adapter.load_combined()
                    summary = adapter.data_summary()
                    data_summary.update(summary)
                    
                    # Apply enhanced data quality validation
                    if self.combined_data is not None and len(self.combined_data) > 0:
                        validation_results = self._validate_data_quality(self.combined_data)
                        data_summary['data_quality_validation'] = validation_results
                        
                        # Log validation warnings
                        for warning in validation_results.get('warnings', []):
                            self.logger.warning(f"   âš ï¸ {warning}")
                        
                        # Apply advanced feature engineering
                        self.combined_data = self._engineer_advanced_features(self.combined_data)
                        self.logger.info(f"   ðŸ”§ Applied advanced feature engineering: {self.combined_data.shape[1]} features")
                    
                    return data_summary
                else:
                    self.logger.info("   â„¹ï¸ No suitable dataset adapter available; falling back to legacy loader")
            except Exception as e:
                self.logger.warning(f"   âš ï¸ Adapter loading failed: {e}; using legacy loader")
        
        # Legacy OASIS loader (existing benchmark approach)
        try:
            data_path = "./training_data/oasis/"
            cross_df = pd.read_csv(f"{data_path}oasis_cross-sectional.csv")
            long_df = pd.read_csv(f"{data_path}oasis_longitudinal.csv")
            
            self.logger.info(f"   Loaded cross-sectional: {cross_df.shape}")
            self.logger.info(f"   Loaded longitudinal: {long_df.shape}")
            
            cross_df = cross_df.rename(columns={'ID': 'Subject_ID', 'M/F': 'Gender', 'Educ': 'EDUC'})
            long_df = long_df.rename(columns={'Subject ID': 'Subject_ID', 'M/F': 'Gender'})
            
            common_cols = list(set(cross_df.columns) & set(long_df.columns))
            self.logger.info(f"   Common columns: {len(common_cols)}")
            
            cross_common = cross_df[common_cols]
            long_common = long_df[common_cols]
            
            self.combined_data = pd.concat([cross_common, long_common], ignore_index=True)
            self.logger.info(f"   ðŸ”— Combined dataset: {self.combined_data.shape}")
            
            initial_subjects = len(self.combined_data)
            
            if 'CDR' in self.combined_data.columns:
                before_cdr = len(self.combined_data)
                self.combined_data = self.combined_data.dropna(subset=['CDR'])
                after_cdr = len(self.combined_data)
                self.logger.info(f"   ðŸŽ¯ Dropped {before_cdr - after_cdr} rows missing CDR: {after_cdr}/{before_cdr} subjects retained")
                cdr_distribution = self.combined_data['CDR'].value_counts().sort_index()
                self.logger.info(f"   ðŸ“ˆ CDR distribution: {dict(cdr_distribution)}")
            
            if 'SES' in self.combined_data.columns and self.combined_data['SES'].isnull().any():
                from sklearn.impute import SimpleImputer
                mode_imputer = SimpleImputer(strategy='most_frequent')
                self.combined_data[['SES']] = mode_imputer.fit_transform(self.combined_data[['SES']])
                self.logger.info("   ðŸ”§ Imputed SES missing values using mode")
                
            if 'MMSE' in self.combined_data.columns and self.combined_data['MMSE'].isnull().any():
                from sklearn.impute import SimpleImputer
                median_imputer = SimpleImputer(strategy='median')
                self.combined_data[['MMSE']] = median_imputer.fit_transform(self.combined_data[['MMSE']])
                self.logger.info("   ðŸ”§ Imputed MMSE missing values using median")
            
            data_summary['assessments_loaded'] = [
                {'type': 'brain_imaging_data', 'files': 2, 'records': len(self.combined_data)},
                {'type': 'clinical_data', 'files': 2, 'records': len(self.combined_data)}
            ]
            data_summary['total_subjects'] = len(self.combined_data)
            data_summary['baseline_subjects'] = len(self.combined_data)
            data_summary['preprocessing_steps'] = [
                "Combined cross-sectional + longitudinal datasets (benchmark approach)",
                "Harmonized column names between datasets", 
                "Applied gentle imputation for missing values",
                f"Retained {len(self.combined_data)}/{initial_subjects} subjects"
            ]
            
            # Apply enhanced data quality validation
            validation_results = self._validate_data_quality(self.combined_data)
            data_summary['data_quality_validation'] = validation_results
            
            # Log validation warnings
            for warning in validation_results.get('warnings', []):
                self.logger.warning(f"   âš ï¸ {warning}")
            
            # Apply advanced feature engineering
            self.combined_data = self._engineer_advanced_features(self.combined_data)
            self.logger.info(f"   ðŸ”§ Applied advanced feature engineering: {self.combined_data.shape[1]} features")
            
            self.logger.info(f"   âœ… BENCHMARK DATA LOADING: {len(self.combined_data)} subjects ready for analysis")
            
        except Exception as e:
            self.logger.error(f"   âŒ Benchmark data loading failed: {e}")
            self.combined_data = pd.DataFrame()
            data_summary['error'] = str(e)
        
        return data_summary
    
    def _load_assessment_files(self, file_paths: List[str], assessment_type: str) -> pd.DataFrame:
        """Load files for a specific assessment type"""
        dataframes = []
        
        # Check if sampling is enabled
        use_sampling = self.config.get('analysis', {}).get('use_sampling', False)
        sample_size = self.config.get('analysis', {}).get('analysis_sample_size', 5000)
        
        for file_path in file_paths:
            try:
                # Load data with proper filtering
                df = pd.read_csv(file_path, low_memory=False)
                total_loaded = len(df)
                
                # Apply MemTrax-specific filtering (based on human scripts)
                if 'MemTrax' in os.path.basename(file_path) or assessment_type == 'cognitive_data':
                    # Filter to collected status only (as done in human scripts)
                    if 'Status' in df.columns:
                        df = df[df['Status'] == 'Collected'].copy()
                        self.logger.info(f"     MemTrax filtered: {len(df):,} collected of {total_loaded:,} total records")
                    
                    # Convert key numeric columns
                    numeric_cols = ['CorrectPCT', 'CorrectResponsesRT', 'CorrectResponsesPCT', 'CorrectRejectionsPCT']
                    for col in numeric_cols:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    # Convert test dates
                    if 'StatusDateTime' in df.columns:
                        df['test_date'] = pd.to_datetime(df['StatusDateTime'], errors='coerce')
                        df = df.dropna(subset=['test_date'])
                        self.logger.info(f"     MemTrax with valid dates: {len(df):,} records")
                
                # Apply sampling after filtering if needed
                if use_sampling and len(df) > sample_size:
                    df = df.sample(n=sample_size, random_state=42)
                    self.logger.info(f"     {assessment_type} file: {os.path.basename(file_path)} - {len(df):,} of {total_loaded:,} records (sampled after filtering)")
                else:
                    self.logger.info(f"     {assessment_type} file: {os.path.basename(file_path)} - {len(df):,} records")
                
                dataframes.append(df)
            except Exception as e:
                self.logger.warning(f"Could not load {file_path}: {e}")
        
        if dataframes:
            combined_df = pd.concat(dataframes, ignore_index=True)
            return combined_df
        return None
    
    def _combine_assessment_datasets(self) -> pd.DataFrame:
        """Combine all assessment datasets using common subject identifier"""
        # Find common subject identifier
        subject_id_candidates = ['SubjectCode', 'subject_id', 'participant_id', 'Code', 'ID']
        
        # Find the common subject column across all datasets
        common_subject_col = None
        for candidate in subject_id_candidates:
            if all(candidate in df.columns for df in self.assessment_data.values()):
                common_subject_col = candidate
                break
        
        if not common_subject_col:
            # Try to find any common column
            all_columns = [set(df.columns) for df in self.assessment_data.values()]
            common_columns = set.intersection(*all_columns)
            subject_columns = [col for col in common_columns if any(term in col.lower() for term in ['subject', 'id', 'code'])]
            if subject_columns:
                common_subject_col = subject_columns[0]
        
        if not common_subject_col:
            raise ValueError("Could not find common subject identifier across datasets")
        
        # Merge all datasets efficiently
        combined = None
        assessment_names = list(self.assessment_data.keys())
        
        # First, validate data structure and check for potential Cartesian joins
        subject_counts = {}
        duplicate_counts = {}
        
        for assessment_type, df in self.assessment_data.items():
            unique_subjects = df[common_subject_col].nunique()
            total_records = len(df)
            duplicate_records = total_records - unique_subjects
            
            subject_counts[assessment_type] = unique_subjects
            duplicate_counts[assessment_type] = duplicate_records
            
            self.logger.info(f"   {assessment_type}: {unique_subjects:,} unique subjects, {total_records:,} total records")
            
            if duplicate_records > 0:
                duplication_rate = (duplicate_records / total_records) * 100
                self.logger.warning(f"      âš ï¸ {duplicate_records:,} duplicate subjects ({duplication_rate:.1f}% duplication rate)")
                
                if duplication_rate > 50:
                    self.logger.error(f"      ðŸš¨ HIGH DUPLICATION RISK: {duplication_rate:.1f}% duplication could cause Cartesian joins")
        
        # Warn about potential Cartesian join risk
        total_possible_combinations = 1
        for count in subject_counts.values():
            total_possible_combinations *= count
            
        if total_possible_combinations > 1000000:  # 1M records
            self.logger.warning(f"   âš ï¸ CARTESIAN JOIN RISK: Potential {total_possible_combinations:,} record combinations")
            self.logger.warning(f"   ðŸ’¡ Using inner joins to reduce risk, but verify subject ID consistency")
        
        # Merge datasets one by one with progress tracking and deduplication
        for i, (assessment_type, df) in enumerate(self.assessment_data.items()):
            if combined is None:
                # Start with first dataset, but deduplicate first
                combined = self._deduplicate_subjects(df, common_subject_col, assessment_type)
                self.logger.info(f"   Starting with {assessment_type}: {len(combined):,} records")
            else:
                before_merge = len(combined)
                
                # Deduplicate the merging dataset first
                df_deduplicated = self._deduplicate_subjects(df, common_subject_col, assessment_type)
                
                # Perform the merge using enhanced merging if available
                if ENHANCED_MERGING_AVAILABLE:
                    self.logger.info(f"   ðŸš€ Using enhanced merge strategy for {assessment_type}")
                    try:
                        combined = smart_merge_datasets(
                            combined, df_deduplicated, common_subject_col,
                            df1_name="Combined", df2_name=assessment_type,
                            logger=self.logger
                        )
                    except Exception as e:
                        self.logger.warning(f"   âš ï¸  Enhanced merge failed, falling back to standard merge: {e}")
                        combined = combined.merge(
                            df_deduplicated,
                            on=common_subject_col,
                            how='inner',
                            suffixes=('', f'_{assessment_type}')
                        )
                else:
                    # Standard merge (original logic)
                    combined = combined.merge(
                        df_deduplicated,
                        on=common_subject_col,
                        how='inner',
                        suffixes=('', f'_{assessment_type}')
                    )
                after_merge = len(combined)
                self.logger.info(f"   Merged {assessment_type}: {before_merge:,} â†’ {after_merge:,} records")
                
                # Critical safety checks for Cartesian joins
                growth_factor = after_merge / before_merge if before_merge > 0 else 1
                
                if growth_factor > 10:
                    self.logger.error(f"   ðŸš¨ CRITICAL ERROR: Cartesian join detected!")
                    self.logger.error(f"   ðŸ“Š Data explosion: {before_merge:,} â†’ {after_merge:,} records ({growth_factor:.1f}x growth)")
                    self.logger.error(f"   ðŸ’¡ This indicates duplicate or mismatched subject IDs between datasets")
                    self.logger.error(f"   ðŸ›‘ Stopping merge to prevent memory explosion")
                    
                    # Log diagnostic information
                    self.logger.error(f"   ðŸ” Debug info:")
                    self.logger.error(f"      - Common subject column: {common_subject_col}")
                    self.logger.error(f"      - Current dataset unique subjects: {combined[common_subject_col].nunique():,}")
                    self.logger.error(f"      - Merging dataset unique subjects: {df[common_subject_col].nunique():,}")
                    self.logger.error(f"      - Expected max records after merge: {combined[common_subject_col].nunique() * df[common_subject_col].nunique():,}")
                    
                    raise ValueError(f"Cartesian join detected: {growth_factor:.1f}x data explosion. Check subject ID consistency between datasets.")
                
                elif growth_factor > 2:
                    self.logger.warning(f"   âš ï¸ Large merge growth detected: {growth_factor:.1f}x increase")
                    self.logger.warning(f"   ðŸ’¡ This may indicate data quality issues or multiple records per subject")
                    
                elif growth_factor < 0.1:
                    self.logger.warning(f"   âš ï¸ Very few matches found: {after_merge:,} records from {before_merge:,}")
                    self.logger.warning(f"   ðŸ’¡ This may indicate mismatched subject IDs between datasets")
        
        self.logger.info(f"   ðŸ“Š Combined dataset: {len(combined)} subjects with data from {len(self.assessment_data)} assessment types")
        return combined
    
    def _deduplicate_subjects(self, df: pd.DataFrame, subject_col: str, assessment_type: str) -> pd.DataFrame:
        """Deduplicate subjects in dataset, keeping most recent or complete record"""
        original_count = len(df)
        unique_subjects = df[subject_col].nunique()
        duplicate_count = original_count - unique_subjects
        
        if duplicate_count == 0:
            return df  # No duplicates
            
        self.logger.info(f"      ðŸ“Š Deduplicating {assessment_type}: {original_count:,} â†’ {unique_subjects:,} records")
        
        # Strategy: Keep the most recent record for each subject (if date available)
        # Otherwise, keep the record with most complete data
        
        date_columns = [col for col in df.columns if any(date_term in col.lower() for date_term in ['date', 'time', 'created', 'updated', 'timestamp'])]
        
        if date_columns:
            # Use most recent record based on date
            date_col = date_columns[0]
            try:
                df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
                df_sorted = df.sort_values([subject_col, date_col], ascending=[True, False])
                deduplicated = df_sorted.groupby(subject_col).first().reset_index()
                self.logger.info(f"         âœ… Deduplication by most recent {date_col}")
            except:
                # Fall back to completeness-based deduplication
                deduplicated = self._deduplicate_by_completeness(df, subject_col)
        else:
            # Use completeness-based deduplication
            deduplicated = self._deduplicate_by_completeness(df, subject_col)
            
        return deduplicated
    
    def _deduplicate_by_completeness(self, df: pd.DataFrame, subject_col: str) -> pd.DataFrame:
        """Keep the record with most complete data for each subject"""
        # Calculate completeness score for each record
        df['_completeness_score'] = df.notna().sum(axis=1)
        
        # Sort by subject and completeness score (highest first)
        df_sorted = df.sort_values([subject_col, '_completeness_score'], ascending=[True, False])
        
        # Keep first record for each subject (most complete)
        deduplicated = df_sorted.groupby(subject_col).first().reset_index()
        
        # Remove the temporary completeness score column
        if '_completeness_score' in deduplicated.columns:
            deduplicated = deduplicated.drop('_completeness_score', axis=1)
            
        self.logger.info(f"         âœ… Deduplication by data completeness")
        return deduplicated
    
    def _detect_data_type(self) -> str:
        """Intelligently detect whether data is surveillance or individual assessment data"""
        # Check for surveillance-specific columns
        surveillance_indicators = ['LocationAbbr', 'LocationDesc', 'Data_Value', 'Topic', 
                                  'Class', 'Question', 'YearStart', 'YearEnd', 
                                  'Stratification1', 'StratificationCategory1']
        
        # Check for individual assessment columns
        individual_indicators = ['Subject_ID', 'subject_id', 'SubjectCode', 'ID', 
                                'MMSE', 'CDR', 'Age', 'Gender', 'EDUC', 
                                'MemTrax', 'ECOG', 'MoCA']
        
        if self.combined_data is None:
            return 'unknown'
        
        cols = self.combined_data.columns.tolist()
        
        # Count indicators
        surveillance_count = sum(1 for ind in surveillance_indicators if ind in cols)
        individual_count = sum(1 for ind in individual_indicators if ind in cols)
        
        # Decision logic
        if surveillance_count >= 5 and 'Data_Value' in cols:
            return 'surveillance'
        elif individual_count >= 3:
            return 'individual'
        elif 'Data_Value' in cols and 'Topic' in cols:
            return 'surveillance'
        else:
            # Default to individual if unclear
            return 'individual'
    
    def _identify_assessment_types(self) -> List[str]:
        """Identify what types of assessments are available in the data"""
        return list(self.assessment_data.keys())
    
    def _analyze_assessment_type(self, assessment_type: str) -> Dict[str, Any]:
        """Analyze a specific assessment type"""
        assessment_results = {
            'assessment_type': assessment_type,
            'descriptive_stats': {},
            'distribution_analysis': {},
            'quality_metrics': {}
        }
        
        # Get the data for this assessment type
        if assessment_type in self.assessment_data:
            df = self.assessment_data[assessment_type]
            
            # Find numeric columns (likely assessment scores)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # Calculate descriptive statistics
            for col in numeric_cols:
                col_data = df[col].dropna()
                if len(col_data) > 0:
                    assessment_results['descriptive_stats'][col] = {
                        'mean': float(col_data.mean()),
                        'median': float(col_data.median()),
                        'std': float(col_data.std()),
                        'min': float(col_data.min()),
                        'max': float(col_data.max()),
                        'sample_size': len(col_data),
                        'missing_percent': float((df[col].isna().sum() / len(df)) * 100)
                    }
        
        return assessment_results
    
    def _analyze_cross_assessment_correlations(self) -> Dict[str, Any]:
        """Analyze correlations between different assessment types"""
        correlation_results = {
            'primary_correlations': {},
            'assessment_pairs': {},
            'clinical_significance': {}
        }
        
        if self.combined_data is None:
            return correlation_results
        
        # Get numeric columns from each assessment type
        assessment_columns = {}
        for assessment_type in self.assessment_data.keys():
            # Find columns that likely belong to this assessment
            matching_cols = [col for col in self.combined_data.columns 
                           if assessment_type.lower() in col.lower() or
                           any(pattern in col.lower() for pattern in self._get_assessment_patterns(assessment_type))]
            assessment_columns[assessment_type] = [col for col in matching_cols 
                                                 if self.combined_data[col].dtype in ['int64', 'float64']]
        
        # Calculate correlations between assessment pairs
        assessment_types = list(assessment_columns.keys())
        for i, assessment_a in enumerate(assessment_types):
            for j, assessment_b in enumerate(assessment_types[i+1:], i+1):
                pair_key = f"{assessment_a}_vs_{assessment_b}"
                correlation_results['assessment_pairs'][pair_key] = self._calculate_assessment_pair_correlations(
                    assessment_columns[assessment_a], assessment_columns[assessment_b], pair_key
                )
        
        # Flatten correlations for primary results
        for pair_key, pair_results in correlation_results['assessment_pairs'].items():
            correlation_results['primary_correlations'].update(pair_results)
        
        # Multiple testing correction (FDR Benjaminiâ€“Hochberg)
        try:
            pvals = [d.get('p_value', 1.0) for d in correlation_results['primary_correlations'].values()]
            keys = list(correlation_results['primary_correlations'].keys())
            if STATSMODELS_AVAILABLE and len(pvals) > 0:
                reject, pvals_corrected, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')
                for k, adj_p, rej in zip(keys, pvals_corrected, reject):
                    correlation_results['primary_correlations'][k]['p_value_fdr'] = float(adj_p)
                    correlation_results['primary_correlations'][k]['significant_fdr'] = bool(rej)
            else:
                # If statsmodels not available, copy unadjusted to adjusted for transparency
                for k in keys:
                    correlation_results['primary_correlations'][k]['p_value_fdr'] = correlation_results['primary_correlations'][k].get('p_value', 1.0)
                    correlation_results['primary_correlations'][k]['significant_fdr'] = correlation_results['primary_correlations'][k].get('p_value', 1.0) < 0.05
        except Exception:
            pass
        
        # Calculate clinical significance
        significant_correlations = sum(1 for corr_data in correlation_results['primary_correlations'].values()
                                     if corr_data.get('p_value_fdr', corr_data.get('p_value', 1)) < 0.05)
        total_correlations = len(correlation_results['primary_correlations'])
        
        if total_correlations > 0:
            correlation_results['clinical_significance'] = {
                'significant_correlations': significant_correlations,
                'total_correlations': total_correlations,
                'significance_rate': significant_correlations / total_correlations,
                'multiple_comparison_threshold': 0.05 / total_correlations,
                'adjustment': 'fdr_bh' if STATSMODELS_AVAILABLE else 'none'
            }
        
        return correlation_results
    
    def _get_assessment_patterns(self, assessment_type: str) -> List[str]:
        """Get search patterns for a specific assessment type"""
        # This should ideally come from configuration
        pattern_map = {
            'cognitive_data': ['memtrax', 'cognitive', 'memory', 'attention', 'rt', 'reaction', 'correct'],
            'ecog_data': ['ecog', 'everyday', 'qid', 'self', 'informant'],
            'demographic_data': ['age', 'education', 'gender', 'demo'],
            'medical_data': ['medical', 'diagnosis', 'condition']
        }
        return pattern_map.get(assessment_type, [assessment_type.lower()])
    
    def _calculate_assessment_pair_correlations(self, cols_a: List[str], cols_b: List[str], pair_key: str) -> Dict[str, Any]:
        """Calculate correlations between two assessment types"""
        pair_correlations = {}
        
        # Intelligent correlation limit based on computational feasibility
        max_cols_per_group = self._calculate_correlation_limit(len(cols_a), len(cols_b))
        
        for col_a in cols_a[:max_cols_per_group]:
            for col_b in cols_b[:max_cols_per_group]:
                try:
                    data_a = self.combined_data[col_a].dropna()
                    data_b = self.combined_data[col_b].dropna()
                    
                    # Get common indices
                    common_indices = data_a.index.intersection(data_b.index)
                    if len(common_indices) > 20:  # Minimum sample size
                        common_a = data_a[common_indices]
                        common_b = data_b[common_indices]
                        
                        corr_coef, corr_p = pearsonr(common_a, common_b)
                        
                        correlation_key = f"{col_a}_vs_{col_b}"
                        pair_correlations[correlation_key] = {
                            'correlation_coefficient': corr_coef,
                            'p_value': corr_p,
                            'sample_size': len(common_indices),
                            'effect_size': self._interpret_correlation_effect_size(abs(corr_coef)),
                            'assessment_pair': pair_key
                        }
                except Exception:
                    continue
        
        return pair_correlations
    
    def _has_self_informant_data(self) -> bool:
        """Check if dataset has both self and informant report data"""
        if self.combined_data is None:
            return False
        
        # Look for columns that might represent self vs informant reports
        self_indicators = ['self', 'participant', 'subject']
        informant_indicators = ['informant', 'partner', 'caregiver', 'family']
        
        columns = [col.lower() for col in self.combined_data.columns]
        
        has_self = any(any(indicator in col for indicator in self_indicators) for col in columns)
        has_informant = any(any(indicator in col for indicator in informant_indicators) for col in columns)
        
        return has_self and has_informant
    
    def _analyze_self_informant_differences(self) -> Dict[str, Any]:
        """Analyze differences between self and informant reports (generic)"""
        self_informant_results = {
            'self_informant_available': True,
            'correlation_analysis': {},
            'discrepancy_analysis': {},
            'domain_analysis': {}
        }
        
        # Find self and informant columns
        all_columns = self.combined_data.columns.tolist()
        
        self_cols = [col for col in all_columns if any(term in col.lower() for term in ['self', 'participant'])]
        informant_cols = [col for col in all_columns if any(term in col.lower() for term in ['informant', 'partner', 'caregiver'])]
        
        # Filter to numeric columns only
        numeric_self_cols = [col for col in self_cols if self.combined_data[col].dtype in ['int64', 'float64']]
        numeric_informant_cols = [col for col in informant_cols if self.combined_data[col].dtype in ['int64', 'float64']]
        
        if len(numeric_self_cols) > 0 and len(numeric_informant_cols) > 0:
            # Calculate correlation between self and informant total scores
            if len(numeric_self_cols) > 1:
                self_total = self.combined_data[numeric_self_cols].sum(axis=1, skipna=False)
            else:
                self_total = self.combined_data[numeric_self_cols[0]]
            
            if len(numeric_informant_cols) > 1:
                informant_total = self.combined_data[numeric_informant_cols].sum(axis=1, skipna=False)
            else:
                informant_total = self.combined_data[numeric_informant_cols[0]]
            
            # Correlation analysis
            valid_data = (~self_total.isna()) & (~informant_total.isna())
            if valid_data.sum() > 10:
                corr_coef, corr_p = pearsonr(self_total[valid_data], informant_total[valid_data])
                self_informant_results['correlation_analysis'] = {
                    'correlation_coefficient': corr_coef,
                    'p_value': corr_p,
                    'sample_size': int(valid_data.sum())
                }
            
            # Discrepancy analysis
            discrepancy = informant_total - self_total
            self_informant_results['discrepancy_analysis'] = {
                'mean_discrepancy': float(discrepancy.mean()),
                'std_discrepancy': float(discrepancy.std()),
                'positive_discrepancy_percent': float((discrepancy > 0).mean() * 100),
                'large_discrepancy_percent': float((abs(discrepancy) > discrepancy.std()).mean() * 100)
            }
        else:
            self_informant_results['self_informant_available'] = False
            self.logger.warning("   âš ï¸ Could not identify both self and informant measures")
        
        return self_informant_results
    
    def _analyze_cognitive_performance(self) -> Dict[str, Any]:
        """Analyze cognitive performance patterns (generic)"""
        cognitive_results = {
            'performance_metrics': {},
            'distribution_analysis': {},
            'efficiency_analysis': {}
        }
        
        if self.combined_data is None:
            return cognitive_results
        
        # Find performance-related columns
        performance_indicators = ['rt', 'reaction_time', 'response_time', 'accuracy', 'correct', 'percent', 'score']
        performance_cols = []
        
        for col in self.combined_data.columns:
            if any(indicator in col.lower() for indicator in performance_indicators):
                if self.combined_data[col].dtype in ['int64', 'float64']:
                    performance_cols.append(col)
        
        # Analyze each performance metric
        for col in performance_cols:
            col_data = self.combined_data[col].dropna()
            if len(col_data) > 0:
                cognitive_results['performance_metrics'][col] = {
                    'mean': float(col_data.mean()),
                    'median': float(col_data.median()),
                    'std': float(col_data.std()),
                    'min': float(col_data.min()),
                    'max': float(col_data.max()),
                    'sample_size': len(col_data)
                }
        
        return cognitive_results
    
    def _get_baseline_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Filter to baseline timepoints only"""
        timepoint_cols = ['TimepointCode', 'timepoint', 'visit', 'session']
        baseline_values = ['m00', 'baseline', '0', 'visit_1', 'session_1', 'bl']
        
        for col in timepoint_cols:
            if col in df.columns:
                for baseline_val in baseline_values:
                    baseline_data = df[df[col] == baseline_val]
                    if len(baseline_data) > 0:
                        self.logger.info(f"   ðŸ“… Using baseline timepoint: {col} == {baseline_val}")
                        return baseline_data
        
        self.logger.info("   ðŸ“… No timepoint column found, using all data as baseline")
        return df
    
    def _clean_and_validate_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and validate the dataset using notebook's gentle approach"""
        original_size = len(df)
        
        # Use notebook's approach: only drop rows missing target variable (CDR), impute the rest
        before_missing = len(df)
        
        # Only drop rows missing CDR (our target variable)  
        if 'CDR' in df.columns:
            initial_cdr_missing = df['CDR'].isnull().sum()
            self.logger.info(f"   ðŸ“Š CDR missing analysis: {initial_cdr_missing}/{len(df)} rows missing CDR values")
            
            # Debug: show CDR value distribution BEFORE dropping
            cdr_distribution = df['CDR'].value_counts().sort_index()
            self.logger.info(f"   ðŸ“ˆ CDR distribution before cleanup: {dict(cdr_distribution)}")
            
            if initial_cdr_missing > 0:
                df = df.dropna(subset=['CDR'])
                self.logger.info(f"   ðŸŽ¯ Dropped {initial_cdr_missing} rows missing CDR (target variable)")
                
                # Show final CDR distribution
                final_cdr_distribution = df['CDR'].value_counts().sort_index()
                self.logger.info(f"   ðŸ“ˆ Final CDR distribution: {dict(final_cdr_distribution)}")
        else:
            self.logger.warning("   âŒ No CDR column found in dataset")
        
        # Apply notebook's imputation strategy for other missing values
        if 'SES' in df.columns and df['SES'].isnull().any():
            from sklearn.impute import SimpleImputer
            mode_imputer = SimpleImputer(strategy='most_frequent')
            df[['SES']] = mode_imputer.fit_transform(df[['SES']])
            self.logger.info("   ðŸ”§ Imputed SES missing values using mode")
            
        if 'MMSE' in df.columns and df['MMSE'].isnull().any():
            from sklearn.impute import SimpleImputer
            median_imputer = SimpleImputer(strategy='median')
            df[['MMSE']] = median_imputer.fit_transform(df[['MMSE']])
            self.logger.info("   ðŸ”§ Imputed MMSE missing values using median")
        
        self.logger.info(f"   âœ… Gentle data cleaning: {len(df)}/{before_missing} subjects retained (notebook approach)")
        return df
    
    def _interpret_correlation_effect_size(self, correlation: float) -> str:
        """Interpret correlation coefficient effect size"""
        if correlation >= 0.7:
            return "large"
        elif correlation >= 0.5:
            return "medium"
        elif correlation >= 0.3:
            return "small"
        else:
            return "negligible"
    
    def _generate_clinical_insights(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate clinical insights from analysis results"""
        insights = {
            'key_findings': [],
            'clinical_implications': [],
            'novel_discoveries': [],
            'limitations': []
        }
        
        # Analyze correlation findings
        correlations = analysis_results.get('correlation_analysis', {}).get('primary_correlations', {})
        significant_corrs = [name for name, data in correlations.items() 
                           if data.get('p_value_fdr', data.get('p_value', 1)) < 0.05]
        
        if significant_corrs:
            insights['key_findings'].append(
                f"Found {len(significant_corrs)} significant cross-assessment correlations (FDR-adjusted)"
            )
        
        # Check for novel findings
        clinical_sig = analysis_results.get('correlation_analysis', {}).get('clinical_significance', {})
        if clinical_sig.get('significance_rate', 0) > 0.5:
            insights['novel_discoveries'].append(
                "High rate of significant correlations suggests strong cross-assessment relationships"
            )
        
        # Add limitations
        data_summary = analysis_results.get('data_summary', {})
        sample_size = data_summary.get('baseline_subjects', 0)
        
        if sample_size < 100:
            insights['limitations'].append("Small sample size may limit generalizability")
        
        insights['limitations'].append("Cross-sectional analysis - causality cannot be determined")
        
        return insights
    
    def _generate_statistical_summary(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive statistical summary"""
        summary = {
            'sample_characteristics': {},
            'primary_findings': {},
            'recommendations': []
        }
        
        # Sample characteristics
        data_summary = analysis_results.get('data_summary', {})
        summary['sample_characteristics'] = {
            'total_subjects': data_summary.get('baseline_subjects', 0),
            'assessments_analyzed': len(data_summary.get('assessments_loaded', [])),
            'data_completeness': 'Good' if data_summary.get('baseline_subjects', 0) > 100 else 'Limited'
        }
        
        # Primary findings
        correlations = analysis_results.get('correlation_analysis', {}).get('primary_correlations', {})
        significant_correlations = [name for name, data in correlations.items() 
                                 if data.get('p_value_fdr', data.get('p_value', 1)) < 0.05]
        
        summary['primary_findings'] = {
            'significant_cross_assessment_correlations': len(significant_correlations),
            'total_correlations_tested': len(correlations)
        }
        
        # Recommendations
        if len(significant_correlations) > 0:
            summary['recommendations'].append("Strong evidence for cross-assessment relationships (FDR-adjusted)")
            summary['recommendations'].append("Consider validation in independent sample")
        
        if data_summary.get('baseline_subjects', 0) < 200:
            summary['recommendations'].append("Increase sample size for more robust findings")
        
        summary['recommendations'].append("Explore longitudinal changes in assessment relationships")
        
        return summary
    
    def _create_analysis_visualizations(self, analysis_results: Dict[str, Any]):
        """Create visualizations automatically based on analysis type and available data"""
        self.logger.info("   ðŸŽ¨ Creating visualizations...")
        
        os.makedirs('outputs/visualizations', exist_ok=True)
        
        try:
            # Detect analysis type and create appropriate visualizations
            if self._is_surveillance_analysis(analysis_results):
                self.logger.info("   ðŸ“Š Creating surveillance analysis visualizations...")
                self._create_surveillance_visualizations(analysis_results)
            else:
                self.logger.info("   ðŸ§  Creating individual assessment visualizations...")
                self._create_assessment_visualizations(analysis_results)
            
        except Exception as e:
            self.logger.warning(f"Error creating visualizations: {e}")
    
    def _is_surveillance_analysis(self, analysis_results: Dict[str, Any]) -> bool:
        """Detect if this is surveillance vs individual assessment analysis"""
        return (analysis_results.get('temporal_analysis') is not None or 
                analysis_results.get('geographic_analysis') is not None)
    
    def _create_surveillance_visualizations(self, analysis_results: Dict[str, Any]):
        """Create surveillance-specific visualizations"""
        
        # 1. Temporal trends (if available)
        temporal_analysis = analysis_results.get('temporal_analysis', {})
        if temporal_analysis:
            self._create_temporal_trends_plot(temporal_analysis)
        
        # 2. Geographic patterns (if available)
        geographic_analysis = analysis_results.get('geographic_analysis', {})
        if geographic_analysis:
            self._create_geographic_patterns_plot(geographic_analysis)
        
        # 3. Health topic analysis (if available)
        topic_analysis = analysis_results.get('topic_analysis', {})
        if topic_analysis:
            self._create_topic_analysis_plot(topic_analysis)
        
        # 4. Risk stratification (if available)
        risk_scores = analysis_results.get('risk_scores', {})
        if risk_scores:
            self._create_risk_stratification_plot(risk_scores)
        
        # 5. Surveillance dashboard
        self._create_surveillance_dashboard(analysis_results)
    
    def _create_assessment_visualizations(self, analysis_results: Dict[str, Any]):
        """Create individual assessment-specific visualizations"""
        
        # Create correlation matrix visualization (if available)
        correlations = analysis_results.get('correlation_analysis', {}).get('primary_correlations', {})
        if correlations:
            self._create_correlation_matrix_plot(analysis_results)
        
        # Create self-informant comparison plot (if available)
        if analysis_results.get('self_informant_comparison', {}).get('self_informant_available'):
            self._create_self_informant_plot(analysis_results)
        
        # Create performance distribution plots (if data available)
        self._create_performance_distribution_plots(analysis_results)
    
    def _create_correlation_matrix_plot(self, analysis_results: Dict[str, Any]):
        """Create correlation matrix heatmap"""
        correlations = analysis_results.get('correlation_analysis', {}).get('primary_correlations', {})
        
        if not correlations:
            return
        
        # Extract correlation data
        corr_names = []
        corr_values = []
        p_values = []
        p_values_fdr = []
        
        for name, data in correlations.items():
            # Clean up names for display
            display_name = name.replace('_vs_', ' vs ').replace('_', ' ')
            if len(display_name) > 40:  # Truncate long names
                display_name = display_name[:37] + "..."
            corr_names.append(display_name)
            corr_values.append(data.get('correlation_coefficient', 0))
            p_values.append(data.get('p_value', 1))
            p_values_fdr.append(data.get('p_value_fdr', data.get('p_value', 1)))
        
        if len(corr_values) > 0:
            fig, ax = plt.subplots(figsize=(12, max(6, len(corr_names) * 0.3)))
            
            # Create horizontal bar plot
            colors = ['red' if p < 0.05 else 'lightblue' for p in p_values_fdr]
            bars = ax.barh(range(len(corr_values)), corr_values, color=colors, alpha=0.7)
            
            ax.set_yticks(range(len(corr_names)))
            ax.set_yticklabels(corr_names, fontsize=9)
            ax.set_xlabel('Correlation Coefficient')
            ax.set_title('Cross-Assessment Correlations (FDR-adjusted)', fontweight='bold', fontsize=14)
            ax.grid(axis='x', alpha=0.3)
            ax.axvline(x=0, color='black', linestyle='-', alpha=0.5)
            
            # Add significance indicators
            for i, (bar, p_val) in enumerate(zip(bars, p_values_fdr)):
                if p_val < 0.05:
                    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
                           '*', fontsize=12, fontweight='bold', va='center')
            
            plt.tight_layout()
            plt.savefig('outputs/visualizations/cross_assessment_correlations.png', dpi=300, bbox_inches='tight')
            plt.close()
    
    def _create_self_informant_plot(self, analysis_results: Dict[str, Any]):
        """Create self vs informant comparison plot"""
        self_informant = analysis_results.get('self_informant_comparison', {})
        
        if not self_informant.get('self_informant_available'):
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Plot 1: Correlation info
        corr_data = self_informant.get('correlation_analysis', {})
        if corr_data:
            correlation = corr_data.get('correlation_coefficient', 0)
            p_value = corr_data.get('p_value', 1)
            
            ax1.text(0.5, 0.6, f'Correlation: r = {correlation:.3f}', 
                    ha='center', va='center', transform=ax1.transAxes, fontsize=14)
            ax1.text(0.5, 0.4, f'p-value: {p_value:.4f}', 
                    ha='center', va='center', transform=ax1.transAxes, fontsize=12)
            
            significance = "Significant" if p_value < 0.05 else "Not Significant"
            ax1.text(0.5, 0.2, significance, 
                    ha='center', va='center', transform=ax1.transAxes, 
                    fontsize=12, fontweight='bold',
                    color='green' if p_value < 0.05 else 'red')
        
        ax1.set_title('Self-Informant Correlation', fontweight='bold')
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)
        ax1.axis('off')
        
        # Plot 2: Discrepancy analysis
        discrepancy = self_informant.get('discrepancy_analysis', {})
        if discrepancy:
            pos_disc_pct = discrepancy.get('positive_discrepancy_percent', 50)
            
            categories = ['Informant Higher', 'Agreement', 'Self Higher']
            # Simple approximation for visualization
            percentages = [pos_disc_pct, 20, 100 - pos_disc_pct - 20]
            colors = ['orange', 'green', 'blue']
            
            ax2.pie(percentages, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)
            ax2.set_title('Report Discrepancy Patterns', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/self_informant_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_performance_distribution_plots(self, analysis_results: Dict[str, Any]):
        """Create cognitive performance distribution plots"""
        cognitive = analysis_results.get('cognitive_performance_analysis', {})
        performance_metrics = cognitive.get('performance_metrics', {})
        
        if not performance_metrics:
            return
        
        # Intelligent metrics selection based on available data
        all_metrics = list(performance_metrics.keys())
        max_displayable_metrics = self._calculate_optimal_display_count(len(all_metrics), 'performance_metrics')
        metrics = all_metrics[:max_displayable_metrics]
        if not metrics:
            return
        
        cols = min(3, len(metrics))
        rows = (len(metrics) + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
        if len(metrics) == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes if len(metrics) > 1 else [axes]
        else:
            axes = axes.flatten()
        
        for i, metric in enumerate(metrics):
            if i < len(axes):
                # Create histogram for this metric
                if metric in self.combined_data.columns:
                    data = self.combined_data[metric].dropna()
                    if len(data) > 0:
                        axes[i].hist(data, bins=30, alpha=0.7, edgecolor='black')
                        axes[i].set_xlabel(metric.replace('_', ' ').title())
                        axes[i].set_ylabel('Frequency')
                        axes[i].set_title(f'{metric.replace("_", " ").title()} Distribution')
                        axes[i].grid(alpha=0.3)
        
        # Hide empty subplots
        for i in range(len(metrics), len(axes)):
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/cognitive_performance_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _save_analysis_results(self, results: Dict[str, Any]):
        """Save analysis results to file"""
        output_file = "outputs/cognitive_analysis_results.json"
        
        try:
            # Make results JSON serializable
            serializable_results = self._make_serializable(results)
            
            with open(output_file, 'w') as f:
                json.dump(serializable_results, f, indent=2)
            
            self.logger.info(f"ðŸ“ Analysis results saved to: {output_file}")
            
        except Exception as e:
            self.logger.error(f"Error saving results: {e}")
    
    def _make_serializable(self, obj):
        """Convert non-serializable objects for JSON"""
        try:
            import numpy as np
            import pandas as pd
        except Exception:
            np = None
            pd = None
        if isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        # datetime-like
        elif hasattr(obj, 'isoformat'):
            return obj.isoformat()
        # numpy types
        if np is not None:
            if isinstance(obj, (np.integer,)):
                return int(obj)
            if isinstance(obj, (np.floating,)):
                return float(obj)
            if isinstance(obj, (np.bool_,)):
                return bool(obj)
            if isinstance(obj, (np.ndarray,)):
                return obj.tolist()
            if isinstance(obj, np.dtype):
                return str(obj)
        # pandas types
        if pd is not None:
            if isinstance(obj, pd.Timestamp):
                return obj.isoformat()
            if isinstance(obj, pd.Series):
                return obj.to_list()
            if isinstance(obj, pd.DataFrame):
                return obj.to_dict(orient='list')
        # pandas/numpy missing
        try:
            if pd is not None and pd.isna(obj):
                return None
        except Exception:
            pass
        return obj
    
    def print_analysis_summary(self, results: Dict[str, Any]):
        """Print formatted analysis summary"""
        print("\n" + "="*80)
        print("ðŸ§  COGNITIVE ANALYSIS SUMMARY")
        print("="*80)
        
        # Analysis info
        analysis_info = results.get('analysis_info', {})
        experiment_name = analysis_info.get('experiment_name', 'Unknown')
        print(f"\nðŸ“Š EXPERIMENT: {experiment_name}")
        
        # Data summary
        data_summary = results.get('data_summary', {})
        print(f"\nðŸ“Š DATA SUMMARY:")
        print(f"   Total subjects analyzed: {data_summary.get('baseline_subjects', 0):,}")
        
        assessments = data_summary.get('assessments_loaded', [])
        print(f"   Assessment types loaded: {len(assessments)}")
        for assessment in assessments:
            print(f"   - {assessment.get('type', 'Unknown')}: {assessment.get('files', 0)} files, {assessment.get('records', 0)} records")
        
        # Correlation findings
        correlations = results.get('correlation_analysis', {}).get('primary_correlations', {})
        if correlations:
            significant_corrs = [name for name, data in correlations.items() 
                               if data.get('p_value_fdr', data.get('p_value', 1)) < 0.05]
            
            print(f"\nðŸ”— CORRELATION ANALYSIS:")
            print(f"   Total correlations tested: {len(correlations)}")
            print(f"   Significant correlations: {len(significant_corrs)}")
            
            if significant_corrs:
                print(f"\n   ðŸ“ˆ TOP SIGNIFICANT CORRELATIONS:")
                # Show top 5 strongest correlations
                sorted_corrs = sorted(significant_corrs, 
                                    key=lambda x: abs(correlations[x].get('correlation_coefficient', 0)), 
                                    reverse=True)[:5]
                for corr_name in sorted_corrs:
                    corr_data = correlations[corr_name]
                    r = corr_data.get('correlation_coefficient', 0)
                    p = corr_data.get('p_value_fdr', corr_data.get('p_value', 1))
                    n = corr_data.get('sample_size', 0)
                    effect = corr_data.get('effect_size', 'unknown')
                    print(f"      {corr_name}: r={r:.3f}, p={p:.4f}, n={n}, effect={effect}")
        
        # Self-informant analysis
        self_informant = results.get('self_informant_comparison', {})
        if self_informant.get('self_informant_available'):
            print(f"\nðŸ‘¥ SELF-INFORMANT ANALYSIS:")
            corr_analysis = self_informant.get('correlation_analysis', {})
            if corr_analysis:
                r = corr_analysis.get('correlation_coefficient', 0)
                p = corr_analysis.get('p_value', 1)
                print(f"   Self-informant correlation: r={r:.3f}, p={p:.4f}")
        
        # F1-FOCUSED CDR Prediction Results
        cdr_prediction = results.get('advanced_cdr_prediction', {})
        if cdr_prediction and 'best_model' in cdr_prediction:
            best_model = cdr_prediction['best_model']
            if best_model:
                print(f"\nðŸŽ¯ F1-FOCUSED CDR PREDICTION RESULTS:")
                print(f"   ðŸ¥‡ Best Model: {best_model.get('name', 'Unknown')}")
                
                # Prioritize F1 scores in display
                if 'test_f1_weighted' in best_model:
                    print(f"   ðŸ”¥ F1-Score (Weighted): {best_model['test_f1_weighted']:.3f}")
                    print(f"   ðŸ”¥ F1-Score (Macro): {best_model['test_f1_macro']:.3f}")
                    print(f"   ðŸ”¥ Precision (Weighted): {best_model['test_precision']:.3f}")
                    print(f"   ðŸ”¥ Recall (Weighted): {best_model['test_recall']:.3f}")
                    print(f"   ðŸ“ˆ Accuracy: {best_model['test_accuracy']:.3f}")
                
                # Clinical acceptability
                test_f1 = best_model.get('test_f1_weighted', 0)
                test_precision = best_model.get('test_precision', 0)
                test_recall = best_model.get('test_recall', 0)
                clinically_acceptable = test_f1 >= 0.75 and test_precision >= 0.75 and test_recall >= 0.75
                print(f"   ðŸ¥ Clinical Acceptability: {'âœ… APPROVED' if clinically_acceptable else 'âŒ NEEDS IMPROVEMENT'}")
                
                # F1-focused insights
                cdr_insights = cdr_prediction.get('clinical_insights', [])
                if cdr_insights:
                    print(f"   ðŸ’¡ F1-Focused Clinical Insights:")
                    for insight in cdr_insights[:3]:  # Show top 3 insights
                        # Clean up insight formatting for display
                        clean_insight = insight.replace('ðŸ† ', '').replace('âœ… ', '').replace('ðŸŽ¯ ', '').replace('âš ï¸ ', '')
                        print(f"      â€¢ {clean_insight}")

        # General Clinical insights
        insights = results.get('clinical_insights', {})
        key_findings = insights.get('key_findings', [])
        if key_findings:
            print(f"\nðŸ’¡ ADDITIONAL KEY FINDINGS:")
            for i, finding in enumerate(key_findings, 1):
                print(f"   {i}. {finding}")
        
        # Recommendations
        statistical_summary = results.get('statistical_summary', {})
        recommendations = statistical_summary.get('recommendations', [])
        if recommendations:
            print(f"\nðŸ“‹ RECOMMENDATIONS:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print("\n" + "="*80)
        print("âœ… Cognitive analysis complete!")
        print("ðŸ“ Results saved to: outputs/cognitive_analysis_results.json")
        print("ðŸŽ¨ Visualizations saved to: outputs/visualizations/")
        print("="*80)
    
    def _has_memtrax_and_outcomes_data(self) -> bool:
        """Check if we have MemTrax data and cognitive outcome variables for prediction"""
        if self.combined_data is None or self.combined_data.empty:
            return False
            
        # Check for MemTrax variables
        memtrax_vars = [col for col in self.combined_data.columns if any(pattern.lower() in col.lower() for pattern in ['memtrax', 'reaction_time', 'response_time', 'correctresponsesrt', 'correctpct', 'accuracy'])]
        
        # Check for cognitive outcome variables  
        outcome_vars = [col for col in self.combined_data.columns if any(pattern.lower() in col.lower() for pattern in ['diagnosis', 'cognitive_status', 'impairment', 'dementia', 'mci', 'cdr', 'mmse', 'moca'])]
        
        has_memtrax = len(memtrax_vars) > 0
        has_outcomes = len(outcome_vars) > 0
        
        self.logger.info(f"   MemTrax variables found: {len(memtrax_vars)}")  
        self.logger.info(f"   Outcome variables found: {len(outcome_vars)}")
        
        return has_memtrax and has_outcomes
    
    def _analyze_memtrax_predictive_power(self) -> Dict[str, Any]:
        """Analyze MemTrax predictive power for cognitive impairment detection using Bergeron's proven approach"""
        prediction_results = {
            'analysis_type': 'memtrax_cognitive_impairment_prediction',
            'memtrax_variables': [],
            'outcome_variables': [],
            'model_performance': {},
            'predictive_insights': [],
            'bergeron_style_analysis': {}
        }
        
        try:
            # Find MemTrax and outcome variables
            memtrax_vars = [col for col in self.combined_data.columns if any(pattern.lower() in col.lower() for pattern in ['memtrax', 'correctresponsesrt', 'correctpct', 'reaction_time', 'accuracy'])]
            outcome_vars = [col for col in self.combined_data.columns if any(pattern.lower() in col.lower() for pattern in ['diagnosis', 'cognitive_status', 'cdr', 'mmse', 'moca', 'impairment'])]
            
            if not memtrax_vars or not outcome_vars:
                prediction_results['error'] = 'Insufficient variables for predictive modeling'
                return prediction_results
            
            prediction_results['memtrax_variables'] = memtrax_vars
            prediction_results['outcome_variables'] = outcome_vars
            
            # NEW: Bergeron-style analysis with optimal feature set
            self.logger.info("   ðŸ§  Running Bergeron-style MemTrax + medical features analysis")
            bergeron_results = self._run_bergeron_style_analysis()
            prediction_results['bergeron_style_analysis'] = bergeron_results
            
            # Original correlation-based analysis
            for outcome_var in outcome_vars[:2]:  # Limit to prevent excessive analysis
                self.logger.info(f"   Analyzing MemTrax prediction for: {outcome_var}")
                
                # Get clean data for this analysis
                analysis_data = self.combined_data[memtrax_vars + [outcome_var]].dropna()
                
                if len(analysis_data) < 50:  # Need minimum sample
                    continue
                
                # Correlation-based predictive analysis
                outcome_analysis = self._analyze_memtrax_outcome_relationship(analysis_data, memtrax_vars, outcome_var)
                prediction_results['model_performance'][outcome_var] = outcome_analysis
            
            # Generate predictive insights
            prediction_results['predictive_insights'] = self._generate_memtrax_predictive_insights(prediction_results)
            
        except Exception as e:
            prediction_results['error'] = f"MemTrax prediction analysis failed: {str(e)}"
            self.logger.error(f"MemTrax prediction analysis error: {e}")
        
        return prediction_results
    
    def _analyze_memtrax_outcome_relationship(self, data: pd.DataFrame, memtrax_vars: List[str], outcome_var: str) -> Dict[str, Any]:
        """Analyze relationship between MemTrax variables and cognitive outcome"""
        relationship_analysis = {
            'outcome_variable': outcome_var,
            'sample_size': len(data),
            'memtrax_correlations': {},
            'clinical_significance': 'unknown'
        }
        
        try:
            # Calculate correlations between MemTrax variables and outcome
            for memtrax_var in memtrax_vars:
                if memtrax_var in data.columns and outcome_var in data.columns:
                    correlation = data[memtrax_var].corr(pd.to_numeric(data[outcome_var], errors='coerce'))
                    
                    if not np.isnan(correlation):
                        relationship_analysis['memtrax_correlations'][memtrax_var] = {
                            'correlation': float(correlation),
                            'abs_correlation': float(abs(correlation))
                        }
            
            # Determine strongest predictor
            if relationship_analysis['memtrax_correlations']:
                strongest_predictor = max(
                    relationship_analysis['memtrax_correlations'].items(),
                    key=lambda x: x[1]['abs_correlation']
                )
                relationship_analysis['strongest_predictor'] = {
                    'variable': strongest_predictor[0],
                    'correlation': strongest_predictor[1]['correlation']
                }
                
                # Assess clinical significance
                max_correlation = abs(strongest_predictor[1]['correlation'])
                if max_correlation >= 0.5:
                    relationship_analysis['clinical_significance'] = 'strong'
                elif max_correlation >= 0.3:
                    relationship_analysis['clinical_significance'] = 'moderate'
                elif max_correlation >= 0.1:
                    relationship_analysis['clinical_significance'] = 'weak'
                else:
                    relationship_analysis['clinical_significance'] = 'minimal'
        
        except Exception as e:
            relationship_analysis['error'] = str(e)
        
        return relationship_analysis
    
    def _run_bergeron_style_analysis(self) -> Dict[str, Any]:
        """Run Bergeron-style analysis using proven feature set: MemTrax + medical demographics"""
        bergeron_results = {
            'analysis_type': 'bergeron_style_memtrax_analysis',
            'feature_set': 'MemTrax (2) + Medical Demographics (8)',
            'total_features': 10,
            'model_performance': {},
            'feature_importance': {},
            'insights': []
        }
        
        try:
            # Extract Bergeron's exact features
            bergeron_features = self._extract_bergeron_features()
            
            if bergeron_features is None or len(bergeron_features) < 50:
                bergeron_results['error'] = 'Insufficient data for Bergeron-style analysis'
                return bergeron_results
            
            # Find cognitive impairment target
            target_col = self._find_cognitive_impairment_target()
            if target_col is None:
                bergeron_results['error'] = 'No cognitive impairment target found'
                return bergeron_results
            
            # Prepare data
            analysis_data = bergeron_features.dropna(subset=[target_col])
            if len(analysis_data) < 50:
                bergeron_results['error'] = 'Insufficient clean data for analysis'
                return bergeron_results
            
            # Separate features and target
            feature_cols = [col for col in bergeron_features.columns if col not in ['SubjectCode', target_col]]
            X = analysis_data[feature_cols]
            y = analysis_data[target_col]
            
            # Train/test split
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Train Bergeron-style model (simple ensemble)
            model_performance = self._train_bergeron_model(X_train, X_test, y_train, y_test, feature_cols)
            bergeron_results['model_performance'] = model_performance
            
            # Generate insights
            bergeron_results['insights'] = self._generate_bergeron_insights(model_performance, len(analysis_data))
            
            self.logger.info(f"   âœ… Bergeron-style analysis complete: AUC={model_performance.get('auc', 0):.3f}")
            
        except Exception as e:
            bergeron_results['error'] = f"Bergeron-style analysis failed: {str(e)}"
            self.logger.error(f"Bergeron-style analysis error: {e}")
        
        return bergeron_results
    
    def _extract_bergeron_features(self) -> Optional[pd.DataFrame]:
        """Extract Bergeron's exact 10 features: MemTrax (2) + Medical Demographics (8)"""
        try:
            # Start with subject codes
            subjects = self.combined_data['SubjectCode'].unique() if 'SubjectCode' in self.combined_data.columns else []
            if len(subjects) == 0:
                return None
            
            features = []
            for subject in subjects:
                subject_data = self.combined_data[self.combined_data['SubjectCode'] == subject]
                if len(subject_data) == 0:
                    continue
                
                feat = {'SubjectCode': subject}
                
                # MemTrax features (2)
                memtrax_cols = [col for col in subject_data.columns if any(pattern.lower() in col.lower() for pattern in ['correctpct', 'correctresponsesrt'])]
                
                if len(memtrax_cols) >= 2:
                    # Percent correct
                    pct_cols = [col for col in memtrax_cols if 'pct' in col.lower()]
                    if pct_cols:
                        feat['memtrax_percent_correct'] = subject_data[pct_cols[0]].mean()
                    
                    # Response time
                    rt_cols = [col for col in memtrax_cols if 'rt' in col.lower()]
                    if rt_cols:
                        feat['memtrax_response_time'] = subject_data[rt_cols[0]].mean()
                
                # Medical demographic features (8)
                # Age
                if 'Age' in subject_data.columns:
                    feat['age'] = subject_data['Age'].iloc[0]
                elif 'AgeRange' in subject_data.columns:
                    age_mapping = {'18-24': 21, '25-34': 29.5, '35-44': 39.5, '45-54': 49.5,
                                 '55-64': 59.5, '65-74': 69.5, '75-84': 79.5, '85+': 85}
                    feat['age'] = age_mapping.get(subject_data['AgeRange'].iloc[0], 65)
                else:
                    feat['age'] = 65
                
                # Sex (1 = Male, 0 = Female)
                if 'Gender' in subject_data.columns:
                    feat['sex_male'] = int(subject_data['Gender'].iloc[0] == 1)
                else:
                    feat['sex_male'] = 0
                
                # Education
                if 'Education' in subject_data.columns:
                    feat['education_years'] = subject_data['Education'].iloc[0]
                elif 'YearsEducationUS_Converted' in subject_data.columns:
                    feat['education_years'] = subject_data['YearsEducationUS_Converted'].iloc[0]
                else:
                    feat['education_years'] = 16
                
                # Medical conditions (QID-based)
                qid_mapping = {
                    'QID3': 'hypertension',      # High blood pressure
                    'QID4': 'diabetes',          # Diabetes
                    'QID5': 'hyperlipidemia',    # High cholesterol
                    'QID1-3': 'stroke',          # Stroke
                    'QID6': 'heart_disease'      # Heart disease
                }
                
                for qid, feature_name in qid_mapping.items():
                    if qid in subject_data.columns:
                        feat[feature_name] = int(subject_data[qid].iloc[0] == 1)
                    else:
                        feat[feature_name] = 0
                
                # Only include if we have both MemTrax features
                if 'memtrax_percent_correct' in feat and 'memtrax_response_time' in feat:
                    features.append(feat)
            
            if len(features) < 10:
                return None
            
            return pd.DataFrame(features)
            
        except Exception as e:
            self.logger.error(f"Error extracting Bergeron features: {e}")
            return None
    
    def _find_cognitive_impairment_target(self) -> Optional[str]:
        """Find cognitive impairment target variable"""
        # Look for cognitive impairment indicators
        target_patterns = ['cognitive_impairment', 'cdr', 'mmse', 'moca', 'diagnosis']
        
        for pattern in target_patterns:
            matching_cols = [col for col in self.combined_data.columns if pattern.lower() in col.lower()]
            if matching_cols:
                return matching_cols[0]
        
        # Look for QID-based cognitive conditions
        cognitive_qids = ['QID1-5', 'QID1-12', 'QID1-13', 'QID1-22', 'QID1-23']
        for qid in cognitive_qids:
            if qid in self.combined_data.columns:
                return qid
        
        return None
    
    def _train_bergeron_model(self, X_train, X_test, y_train, y_test, feature_cols) -> Dict[str, Any]:
        """Train Bergeron-style model with simple ensemble"""
        try:
            from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
            from sklearn.linear_model import LogisticRegression
            from sklearn.ensemble import StackingClassifier
            from sklearn.calibration import CalibratedClassifierCV
            from sklearn.metrics import roc_auc_score, classification_report
            
            # Simple ensemble (like Bergeron used)
            base_models = [
                ('logistic', LogisticRegression(random_state=42, max_iter=1000)),
                ('rf', RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)),
                ('gb', GradientBoostingClassifier(random_state=42, max_iter=100))
            ]
            
            # Stacking ensemble
            stack = StackingClassifier(
                estimators=base_models,
                final_estimator=LogisticRegression(random_state=42),
                cv=5
            )
            
            # Calibrated ensemble
            model = CalibratedClassifierCV(stack, cv=3)
            
            # Train
            model.fit(X_train, y_train)
            
            # Evaluate
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            auc = roc_auc_score(y_test, y_pred_proba)
            
            # Feature importance (from Random Forest)
            rf_model = stack.estimators_[1]  # Random Forest
            feature_importance = dict(zip(feature_cols, rf_model.feature_importances_))
            
            return {
                'auc': float(auc),
                'model_type': 'Bergeron-style ensemble',
                'feature_importance': feature_importance,
                'sample_size': len(X_train),
                'test_size': len(X_test),
                'prevalence': float(y_train.mean())
            }
            
        except Exception as e:
            self.logger.error(f"Error training Bergeron model: {e}")
            return {'error': str(e)}
    
    def _generate_bergeron_insights(self, model_performance: Dict[str, Any], sample_size: int) -> List[str]:
        """Generate insights from Bergeron-style analysis"""
        insights = []
        
        try:
            auc = model_performance.get('auc', 0)
            prevalence = model_performance.get('prevalence', 0)
            
            # Performance assessment
            if auc >= 0.80:
                insights.append(f"âœ… Excellent performance: AUC={auc:.3f} using Bergeron's proven feature set")
            elif auc >= 0.70:
                insights.append(f"âœ… Good performance: AUC={auc:.3f} with simple MemTrax + medical features")
            elif auc >= 0.60:
                insights.append(f"âš ï¸ Moderate performance: AUC={auc:.3f} - may indicate label quality issues")
            else:
                insights.append(f"âŒ Low performance: AUC={auc:.3f} - check data quality and labels")
            
            # Feature insights
            feature_importance = model_performance.get('feature_importance', {})
            if feature_importance:
                top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:3]
                insights.append(f"Top predictive features: {', '.join([f[0] for f in top_features])}")
            
            # Sample size context
            insights.append(f"Analysis based on {sample_size} subjects with {prevalence:.1%} cognitive impairment prevalence")
            
            # Bergeron comparison
            if auc >= 0.75:
                insights.append("Performance approaches Bergeron's results (0.91 AUC) - validates MemTrax predictive value")
            else:
                insights.append("Performance below Bergeron's results - likely due to label quality differences (clinical vs self-report)")
            
        except Exception as e:
            insights.append(f"Error generating Bergeron insights: {str(e)}")
        
        return insights
    
    def _generate_memtrax_predictive_insights(self, prediction_results: Dict[str, Any]) -> List[str]:
        """Generate insights about MemTrax predictive capabilities"""
        insights = []
        
        try:
            model_performance = prediction_results.get('model_performance', {})
            
            if not model_performance:
                insights.append("Insufficient data for MemTrax predictive modeling")
                return insights
            
            # Analyze overall predictive performance
            strong_predictors = []
            moderate_predictors = []
            
            for outcome, analysis in model_performance.items():
                significance = analysis.get('clinical_significance', 'unknown')
                strongest = analysis.get('strongest_predictor', {})
                
                if significance == 'strong':
                    strong_predictors.append(f"{strongest.get('variable', 'unknown')} for {outcome}")
                elif significance == 'moderate':
                    moderate_predictors.append(f"{strongest.get('variable', 'unknown')} for {outcome}")
            
            if strong_predictors:
                insights.append(f"MemTrax shows strong predictive power: {', '.join(strong_predictors)}")
            
            if moderate_predictors:
                insights.append(f"MemTrax shows moderate predictive power: {', '.join(moderate_predictors)}")
            
            if not strong_predictors and not moderate_predictors:
                insights.append("MemTrax shows limited predictive power for cognitive impairment in this dataset")
            
            # Add sample size context
            total_analyses = len(model_performance)
            if total_analyses > 0:
                avg_sample = sum(analysis.get('sample_size', 0) for analysis in model_performance.values()) / total_analyses
                insights.append(f"Predictive analysis based on average {avg_sample:.0f} subjects per outcome")
        
        except Exception as e:
            insights.append(f"Error generating MemTrax predictive insights: {str(e)}")
        
        return insights
    
    def _add_cognitive_impairment_labels(self) -> bool:
        """Add cognitive impairment labels based on medical history QID codes"""
        try:
            # Look for medical data in our combined dataset
            medical_cols = [col for col in self.combined_data.columns if col.startswith('QID1-')]
            
            if not medical_cols:
                self.logger.warning("No medical QID columns found for cognitive impairment labeling")
                return False
            
            # Cognitive condition QID codes (based on human scripts)
            cognitive_qids = [
                'QID1-5',   # Dementia
                'QID1-12',  # Alzheimer's Disease
                'QID1-13',  # Mild Cognitive Impairment (MCI)
                'QID1-22',  # Frontotemporal Dementia (FTD)
                'QID1-23',  # Lewy Body Disease (LBD)
            ]
            
            self.logger.info(f"   Found {len(medical_cols)} medical QID columns, checking for cognitive conditions...")
            
            # Create cognitive impairment labels
            cognitive_impairment = []
            cognitive_details = []
            
            for idx, row in self.combined_data.iterrows():
                has_cognitive_condition = False
                conditions_found = []
                
                for qid in cognitive_qids:
                    if qid in self.combined_data.columns:
                        try:
                            value = pd.to_numeric(row[qid], errors='coerce')
                            if value == 1:
                                has_cognitive_condition = True
                                conditions_found.append(qid)
                        except:
                            if str(row[qid]).strip() == '1':
                                has_cognitive_condition = True
                                conditions_found.append(qid)
                
                cognitive_impairment.append(1 if has_cognitive_condition else 0)
                cognitive_details.append(','.join(conditions_found) if conditions_found else 'None')
            
            # Add to dataset
            self.combined_data['CognitiveImpairment'] = cognitive_impairment
            self.combined_data['CognitiveDiagnosisCodes'] = cognitive_details
            
            # Log summary
            impaired_count = sum(cognitive_impairment)
            total_count = len(cognitive_impairment)
            
            self.logger.info(f"   Cognitive impairment labeling complete:")
            self.logger.info(f"   - Cognitively impaired: {impaired_count:,} ({impaired_count/total_count*100:.1f}%)")
            self.logger.info(f"   - Cognitively normal: {total_count-impaired_count:,} ({(total_count-impaired_count)/total_count*100:.1f}%)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to add cognitive impairment labels: {e}")
            return False
    
    def _advanced_cdr_prediction(self) -> Dict[str, Any]:
        """Advanced CDR prediction using state-of-the-art ML techniques"""
        prediction_results = {
            'analysis_type': 'advanced_cdr_prediction',
            'models_tested': [],
            'best_model': {},
            'feature_importance': {},
            'performance_metrics': {},
            'clinical_insights': []
        }
        
        try:
            # Check if we have CDR column
            if 'CDR' not in self.combined_data.columns:
                prediction_results['error'] = 'CDR column not found in dataset'
                return prediction_results
            
            self.logger.info("ðŸ§  Running advanced CDR prediction analysis with benchmark-optimized hyperparameters...")
            
            # Enhanced data preprocessing - Use benchmark approach for optimal performance
            df = self.combined_data.copy()
            
            # BENCHMARK FIX: Harmonize column names from both datasets
            if 'ID' in df.columns and 'Subject_ID' not in df.columns:
                df = df.rename(columns={'ID': 'Subject_ID'})
            if 'Subject ID' in df.columns:
                df = df.rename(columns={'Subject ID': 'Subject_ID'})
            if 'M/F' in df.columns:
                df = df.rename(columns={'M/F': 'Gender'})
            if 'Educ' in df.columns:
                df = df.rename(columns={'Educ': 'EDUC'})
            
            # Apply advanced enhancements if available
            if ENHANCEMENTS_AVAILABLE:
                self.logger.info("   ðŸš€ Applying advanced feature engineering enhancements...")
                try:
                    df = integrate_enhancements(self, df)
                    self.logger.info("   âœ… Successfully applied brain volume normalization and feature enhancements")
                except Exception as e:
                    self.logger.warning(f"   âš ï¸ Enhancement application failed: {e}, continuing with standard features")
            
            # Remove rows with missing CDR
            before_cdr_drop = len(df)
            df = df.dropna(subset=['CDR'])
            after_cdr_drop = len(df)
            self.logger.info(f"   ðŸŽ¯ Dropped {before_cdr_drop - after_cdr_drop} rows missing CDR: {after_cdr_drop}/{before_cdr_drop} subjects retained")
            
            # Handle missing values intelligently
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if 'CDR' in numeric_cols:
                numeric_cols.remove('CDR')
            
            # Smart imputation strategy (matching successful benchmarks)
            if len(numeric_cols) > 0:
                # Filter to columns that have at least some non-null values
                imputable_cols = [col for col in numeric_cols if df[col].notna().any()]
                if imputable_cols:
                    # Use different strategies for different types of variables
                    for col in imputable_cols:
                        if 'SES' in col.upper():
                            # Use mode for socioeconomic status (categorical-like)
                            mode_imputer = SimpleImputer(strategy='most_frequent')
                            df[[col]] = mode_imputer.fit_transform(df[[col]])
                        else:
                            # Use median for other numeric variables
                            median_imputer = SimpleImputer(strategy='median')
                            df[[col]] = median_imputer.fit_transform(df[[col]])
                    self.logger.info(f"   Applied targeted imputation to {len(imputable_cols)} columns")
                
                # Drop columns that are entirely null
                null_cols = [col for col in numeric_cols if df[col].isna().all()]
                if null_cols:
                    df = df.drop(columns=null_cols)
                    self.logger.info(f"   Dropped {len(null_cols)} entirely null columns: {null_cols}")
            
            # Handle categorical variables with one-hot encoding
            categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
            if categorical_cols:
                # Store CDR values before one-hot encoding
                cdr_values = df['CDR'].copy() if 'CDR' in df.columns else None
                df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
                # Restore CDR if it was categorical and got encoded
                if cdr_values is not None and 'CDR' not in df.columns:
                    df['CDR'] = cdr_values
            
            # Prepare features and target with comprehensive data leakage prevention
            target_col = 'CDR'
            
            # Detect potential leakage columns (same name, similar names, high correlation)
            leakage_columns = self._detect_data_leakage(df, target_col)
            
            if leakage_columns:
                self.logger.warning(f"   ðŸš¨ LEAKAGE DETECTED: Excluding {len(leakage_columns)} columns to prevent data leakage:")
                for col in leakage_columns:
                    self.logger.warning(f"       - {col}")
                    
            X = df.drop(leakage_columns + [target_col], axis=1)
            y = df[target_col]
            
            # Enhanced leakage detection using feature names
            feature_names = list(X.columns)
            leakage_analysis = self._detect_potential_leakage(feature_names, target_col)
            
            if leakage_analysis['has_leakage_risk']:
                self.logger.warning(f"   ðŸš¨ POTENTIAL LEAKAGE DETECTED ({leakage_analysis['risk_level']} RISK):")
                for indicator in leakage_analysis['indicators']:
                    self.logger.warning(f"       - {indicator}")
                for warning in leakage_analysis['warnings']:
                    self.logger.warning(f"       âš ï¸ {warning}")
                for rec in leakage_analysis['recommendations']:
                    self.logger.info(f"       ðŸ’¡ {rec}")
            else:
                self.logger.info(f"   âœ… No additional leakage risks detected")
            
            self.logger.info(f"   âœ… Using {X.shape[1]} legitimate features (excluded {len(leakage_columns)} leakage-prone columns)")
            
            # BENCHMARK FIX: Convert CDR to integer for classification with proper sequential mapping
            # Map CDR values to sequential classes: 0.0->0, 0.5->1, 1.0->2, 2.0->3
            y_classes = y.copy()
            y_classes = y_classes.replace(0.0, 0)  # Map 0.0 to class 0
            y_classes = y_classes.replace(0.5, 1)  # Map 0.5 to class 1
            y_classes = y_classes.replace(1.0, 2)  # Map 1.0 to class 2
            y_classes = y_classes.replace(2.0, 3)  # Map 2.0 to class 3
            y_encoded = y_classes.astype(int)
            
            # BENCHMARK FIX: Apply benchmark preprocessing - exclude CDR=2.0 (severe cases) to match published results
            severe_mask = (y == 2.0)  # Exclude exactly CDR=2.0 cases (5 subjects)
            n_severe = int(severe_mask.sum())
            if n_severe > 0:
                self.logger.info(f"   ðŸ“Š BENCHMARK APPROACH: Excluding {n_severe} severe CDR=2.0 cases (matches 603 subjects)")
                self.logger.info(f"   ðŸ’¡ This replicates published benchmark for direct performance comparison")
                # Filter all datasets
                X = X.loc[~severe_mask]
                y = y[~severe_mask]
                # Re-encode y after filtering - use LabelEncoder for proper sequential mapping
                # This ensures classes are 0, 1, 2 regardless of original CDR values
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                y_encoded = le.fit_transform(y)  # Will map to sequential 0, 1, 2
                self.logger.info(f"   ðŸ“Š Classes mapped: {dict(zip(le.classes_, le.transform(le.classes_)))}")
                
                self.logger.info(f"   ðŸŽ¯ Final dataset: {len(X)} subjects (target: 603 for benchmark match)")
            else:
                self.logger.info(f"   ðŸ“Š No severe CDR=2.0 cases found to exclude")
            
            # Reset indices to avoid alignment issues
            X = X.reset_index(drop=True)
            y_encoded = pd.Series(y_encoded).reset_index(drop=True).values
            
            if len(X) < 50:
                prediction_results['error'] = f'Insufficient data for ML: only {len(X)} samples'
                return prediction_results
            
            self.logger.info(f"   Dataset prepared: {len(X)} samples, {X.shape[1]} features")
            self.logger.info(f"   CDR distribution: {dict(zip(*np.unique(y_encoded, return_counts=True)))}")
            
            # Standardize features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # PROPER ML METHODOLOGY: Always use train/test split for final evaluation
            # This prevents overfitting and provides honest performance metrics
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
            )
            
            # Log proper methodology usage
            self.logger.info(f"   âœ… Proper train/test split: Train={len(X_train)}, Test={len(X_test)} samples")
            self._validate_ml_methodology('train_test_split', {'train_size': len(X_train), 'test_size': len(X_test)})
            
            # Define models to test with optimized hyperparameters (based on successful benchmarks)
            models = {
                'GradientBoosting': GradientBoostingClassifier(
                    random_state=42,
                    learning_rate=0.15,
                    max_depth=50,
                    n_estimators=96,
                    subsample=0.95,
                    min_samples_split=0.15,
                    min_samples_leaf=5,
                    max_features='log2'
                ),
                'RandomForest': RandomForestClassifier(
                    random_state=42, 
                    n_estimators=200,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2
                )
            }
            
            # Add XGBoost if available with optimized parameters  
            if XGBOOST_AVAILABLE:
                models['XGBoost'] = XGBClassifier(
                    random_state=42, 
                    eval_metric='mlogloss',
                    learning_rate=0.1,
                    max_depth=8,
                    n_estimators=100
                )
            
            # Test each model with F1-focused evaluation
            best_f1_score = 0
            best_model_name = None
            best_model = None
            
            # First test individual models
            for name, model in models.items():
                try:
                    # F1-FOCUSED EVALUATION: Use comprehensive clinical evaluation if available
                    if hasattr(self, 'clinical_evaluator') and self.clinical_evaluator:
                        self.logger.info(f"   ðŸŽ¯ {name}: F1-focused clinical evaluation...")
                        
                        # Get comprehensive metrics using clinical evaluator
                        X_df = pd.DataFrame(X_scaled, columns=[f'feature_{i}' for i in range(X_scaled.shape[1])])
                        y_series = pd.Series(y_encoded)
                        
                        comprehensive_metrics = self.clinical_evaluator.evaluate_model_comprehensive(
                            model, X_df, y_series, cv=5
                        )
                        
                        # Extract key metrics
                        f1_weighted = comprehensive_metrics['f1_weighted_mean']
                        f1_macro = comprehensive_metrics['f1_macro_mean']
                        precision_weighted = comprehensive_metrics['precision_weighted_mean']
                        recall_weighted = comprehensive_metrics['recall_weighted_mean']
                        accuracy_mean = comprehensive_metrics['accuracy_mean']
                        clinical_quality = comprehensive_metrics['clinical_quality_score']
                        
                        model_results = {
                            'name': name,
                            'f1_weighted': f1_weighted,
                            'f1_macro': f1_macro,
                            'precision_weighted': precision_weighted,
                            'recall_weighted': recall_weighted,
                            'accuracy_mean': accuracy_mean,
                            'clinical_quality_score': clinical_quality,
                            'clinically_acceptable': comprehensive_metrics['clinically_acceptable'],
                            'comprehensive_metrics': comprehensive_metrics
                        }
                        
                        # Log F1-focused results prominently
                        self.logger.info(f"   ðŸ“Š {name} CLINICAL EVALUATION:")
                        self.logger.info(f"      ðŸŽ¯ F1 (Weighted): {f1_weighted:.3f} Â± {comprehensive_metrics['f1_weighted_std']:.3f}")
                        self.logger.info(f"      ðŸŽ¯ F1 (Macro): {f1_macro:.3f} Â± {comprehensive_metrics['f1_macro_std']:.3f}")
                        self.logger.info(f"      ðŸŽ¯ Precision: {precision_weighted:.3f} Â± {comprehensive_metrics['precision_weighted_std']:.3f}")
                        self.logger.info(f"      ðŸŽ¯ Recall: {recall_weighted:.3f} Â± {comprehensive_metrics['recall_weighted_std']:.3f}")
                        self.logger.info(f"      ðŸŽ¯ Accuracy: {accuracy_mean:.3f} Â± {comprehensive_metrics['accuracy_std']:.3f}")
                        self.logger.info(f"      ðŸ¥ Clinical Quality Score: {clinical_quality:.3f}")
                        self.logger.info(f"      ðŸ¥ Clinically Acceptable: {'âœ… Yes' if comprehensive_metrics['clinically_acceptable'] else 'âŒ No'}")
                        
                        # Track best model based on F1-weighted score (primary clinical metric)
                        if f1_weighted > best_f1_score:
                            best_f1_score = f1_weighted
                            best_model_name = name
                            best_model = model
                        
                    else:
                        # Fallback to basic evaluation - FIXED to use proper methodology
                        # IMPORTANT: Cross-validation ONLY on training set for model selection
                        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')
                        mean_f1 = cv_scores.mean()
                        
                        # Validate ML methodology
                        self._validate_ml_methodology('cross_validate', {'data': 'training_only', 'cv_folds': 5})
                        
                        # Train final model on full training set
                        model.fit(X_train, y_train)
                        
                        # Evaluate ONLY on held-out test set
                        y_pred = model.predict(X_test)
                        test_f1 = f1_score(y_test, y_pred, average='weighted')
                        
                        model_results = {
                            'name': name,
                            'f1_weighted': mean_f1,
                            'f1_weighted_std': cv_scores.std(),
                            'test_f1': test_f1,
                            'cv_scores': cv_scores.tolist()
                        }
                        
                        self.logger.info(f"   {name}: F1={mean_f1:.3f}Â±{cv_scores.std():.3f}, Test F1={test_f1:.3f}")
                        
                        # Track best model
                        if mean_f1 > best_f1_score:
                            best_f1_score = mean_f1
                            best_model_name = name
                            best_model = model
                    
                    prediction_results['models_tested'].append(model_results)
                        
                except Exception as e:
                    self.logger.warning(f"   {name} failed: {e}")
            
            # Test ensemble model if enhancements are available
            if ENHANCEMENTS_AVAILABLE and best_f1_score > 0:
                try:
                    self.logger.info("   ðŸŽ¯ Testing advanced ensemble model...")
                    enhancer = EnhancedCDRPredictor(logger=self.logger)
                    
                    # Apply correlation-based feature selection
                    X_selected = enhancer.apply_correlation_based_feature_selection(X, y, threshold=0.05)
                    X_selected_scaled = scaler.fit_transform(X_selected)
                    
                    # Split for ensemble
                    X_train_ens, X_test_ens, y_train_ens, y_test_ens = train_test_split(
                        X_selected_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
                    )
                    
                    # Create and test ensemble
                    ensemble_results = enhancer.create_ensemble_model(X_train_ens, y_train_ens, X_test_ens, y_test_ens)
                    
                    # Add to results
                    ensemble_model_results = {
                        'name': 'Ensemble',
                        'cv_mean': ensemble_results['ensemble_cv_mean'],
                        'cv_std': ensemble_results['ensemble_cv_std'],
                        'test_accuracy': ensemble_results.get('ensemble_test_accuracy', 0),
                        'cv_scores': []  # Not available for ensemble
                    }
                    
                    prediction_results['models_tested'].append(ensemble_model_results)
                    
                    # Check if ensemble is best (using F1 as primary metric)
                    ensemble_f1 = ensemble_results.get('ensemble_f1_score', ensemble_results['ensemble_cv_mean'])
                    if ensemble_f1 > best_f1_score:
                        best_f1_score = ensemble_f1
                        best_model_name = 'Ensemble'
                        best_model = ensemble_results['model']
                        self.logger.info(f"   âœ… Ensemble is new best model: F1={ensemble_f1:.3f}")
                    
                except Exception as e:
                    self.logger.warning(f"   Ensemble model failed: {e}")
            
            # Check for performance ceiling
            if prediction_results['models_tested']:
                # Collect all model performances for ceiling analysis
                model_performances = {}
                for model_result in prediction_results['models_tested']:
                    model_name = model_result['model']
                    # Use F1 score as the performance metric
                    if 'f1_weighted' in model_result:
                        model_performances[model_name] = model_result['f1_weighted']
                    elif 'cv_mean' in model_result:
                        model_performances[model_name] = model_result['cv_mean']
                
                # Analyze for performance ceiling
                n_samples = len(y_train)
                n_features = X.shape[1]
                ceiling_analysis = self._check_performance_ceiling(
                    model_performances, n_samples, n_features
                )
                
                prediction_results['performance_ceiling_analysis'] = ceiling_analysis
                
                # Log ceiling analysis results
                if ceiling_analysis['likely_at_ceiling']:
                    self.logger.info("\n   ðŸ“Š PERFORMANCE CEILING ANALYSIS:")
                    for evidence in ceiling_analysis['evidence']:
                        self.logger.info(f"      - {evidence}")
                    self.logger.info("\n   ðŸ“ RECOMMENDATIONS:")
                    for rec in ceiling_analysis['recommendations'][:5]:  # Show top 5 recommendations
                        self.logger.info(f"      {rec}")
            
            if best_model is not None:
                # F1-FOCUSED evaluation of best model
                # Use correct feature dimensions based on model type
                if best_model_name == 'Ensemble':
                    # Ensemble was trained on selected features
                    y_pred = best_model.predict(X_test_ens)
                    y_true = y_test_ens
                else:
                    # Individual models were trained on full feature set
                    y_pred = best_model.predict(X_test)
                    y_true = y_test
                
                # Calculate comprehensive F1-focused metrics
                test_accuracy = accuracy_score(y_true, y_pred)
                test_f1_weighted = f1_score(y_true, y_pred, average='weighted')
                test_f1_macro = f1_score(y_true, y_pred, average='macro')
                test_precision = precision_score(y_true, y_pred, average='weighted')
                test_recall = recall_score(y_true, y_pred, average='weighted')
                
                classification_report_dict = classification_report(y_true, y_pred, output_dict=True)
                
                # THRESHOLD OPTIMIZATION - Critical for imbalanced datasets
                optimal_thresholds = {}
                if hasattr(best_model, 'predict_proba'):
                    try:
                        self.logger.info("   ðŸŽ¯ Performing threshold optimization...")
                        
                        # Get probabilities for positive class
                        if best_model_name == 'Ensemble' and 'X_test_ens' in locals():
                            y_proba = best_model.predict_proba(X_test_ens)
                            y_true_thresh = y_test_ens
                        else:
                            y_proba = best_model.predict_proba(X_test)
                            y_true_thresh = y_test
                        
                        # Check if this is binary or multiclass
                        n_classes = len(np.unique(y_true_thresh))
                        
                        # Only perform threshold optimization for binary classification
                        if n_classes == 2:
                            # Handle binary case
                            if len(y_proba.shape) > 1 and y_proba.shape[1] > 1:
                                y_proba_positive = y_proba[:, 1]
                            else:
                                y_proba_positive = y_proba
                            
                            # Find optimal thresholds for different objectives
                            from sklearn.metrics import roc_curve, precision_recall_curve
                            
                            # ROC-based optimization
                            fpr, tpr, thresholds_roc = roc_curve(y_true_thresh, y_proba_positive)
                            
                            # Youden's J statistic (balanced)
                            j_scores = tpr - fpr
                            optimal_idx = np.argmax(j_scores)
                            optimal_thresholds['youden'] = {
                                'threshold': float(thresholds_roc[optimal_idx]),
                                'sensitivity': float(tpr[optimal_idx]),
                                'specificity': float(1 - fpr[optimal_idx])
                            }
                            
                            # High sensitivity for screening (80% target)
                            target_sensitivity = 0.80
                            idx_80 = np.argmin(np.abs(tpr - target_sensitivity))
                            optimal_thresholds['screening_80'] = {
                                'threshold': float(thresholds_roc[idx_80]),
                                'sensitivity': float(tpr[idx_80]),
                                'specificity': float(1 - fpr[idx_80])
                            }
                            
                            # Default threshold comparison
                            y_pred_default = (y_proba_positive >= 0.5).astype(int)
                            default_sensitivity = recall_score(y_true_thresh, y_pred_default, average='binary')
                            
                            optimal_thresholds['default'] = {
                                'threshold': 0.5,
                                'sensitivity': float(default_sensitivity),
                                'warning': 'Default threshold often inappropriate for imbalanced data!'
                            }
                            
                            # Log threshold optimization results
                            self.logger.info(f"   ðŸ“Š Threshold Analysis:")
                            self.logger.info(f"      Default (0.5): Sensitivity={default_sensitivity:.1%}")
                            self.logger.info(f"      Optimal (Youden): Threshold={optimal_thresholds['youden']['threshold']:.3f}, Sensitivity={optimal_thresholds['youden']['sensitivity']:.1%}")
                            self.logger.info(f"      Screening (80%): Threshold={optimal_thresholds['screening_80']['threshold']:.3f}, Sensitivity={optimal_thresholds['screening_80']['sensitivity']:.1%}")
                            
                            # Add ML best practice warning if needed
                            if default_sensitivity < 0.50 and optimal_thresholds['youden']['sensitivity'] > default_sensitivity + 0.20:
                                self._validate_ml_methodology('suboptimal_threshold', {
                                    'default_sensitivity': default_sensitivity,
                                    'optimal_sensitivity': optimal_thresholds['youden']['sensitivity'],
                                    'improvement_possible': optimal_thresholds['youden']['sensitivity'] - default_sensitivity
                                })
                        else:
                            self.logger.info(f"   â„¹ï¸ Threshold optimization skipped (multiclass with {n_classes} classes)")
                    except Exception as e:
                        self.logger.debug(f"   Threshold optimization skipped: {e}")
                
                # Report ML methodology validation summary
                if self.ml_validation_warnings:
                    self.logger.warning("âš ï¸ ML METHODOLOGY ISSUES DETECTED:")
                    for warning in self.ml_validation_warnings:
                        self.logger.warning(f"   - {warning}")
                else:
                    self.logger.info("âœ… ML methodology validation passed - all best practices followed!")
                
                prediction_results['best_model'] = {
                    'name': best_model_name,
                    'f1_weighted': best_f1_score,  # Cross-validation F1 score
                    'ml_methodology_valid': len(self.ml_validation_warnings) == 0,
                    'ml_warnings': self.ml_validation_warnings if self.ml_validation_warnings else None,
                    'threshold_optimization': optimal_thresholds if optimal_thresholds else None,
                    'test_f1_weighted': test_f1_weighted,  # Test set F1 score
                    'test_f1_macro': test_f1_macro,
                    'test_precision': test_precision,
                    'test_recall': test_recall,
                    'test_accuracy': test_accuracy,
                    'classification_report': classification_report_dict
                }
                
                # Feature importance (if available)
                importance_scores = None
                feature_names = None
                
                if hasattr(best_model, 'feature_importances_'):
                    importance_scores = best_model.feature_importances_
                    feature_names = list(X_selected.columns if best_model_name == 'Ensemble' else X.columns)
                elif hasattr(best_model, 'estimators_') and best_model_name == 'Ensemble':
                    # For ensemble models, get feature importance from first estimator
                    try:
                        importance_scores = best_model.estimators_[0][1].feature_importances_
                        feature_names = list(X_selected.columns)
                    except:
                        pass
                
                if importance_scores is not None and feature_names is not None:
                    # Get top 10 most important features
                    importance_df = pd.DataFrame({
                        'feature': feature_names,
                        'importance': importance_scores
                    }).sort_values('importance', ascending=False)
                    
                    # Intelligently determine how many features to show
                    total_features = len(importance_df)
                    max_features_to_show = self._calculate_optimal_display_count(total_features, 'feature_importance')
                    top_features = importance_df.head(max_features_to_show)
                    
                    prediction_results['feature_importance'] = top_features.to_dict('records')
                    
                    # Show fewer features in log for readability
                    log_features_count = min(5, len(top_features))
                    self.logger.info(f"   Top {log_features_count} predictive features:")
                    for _, row in top_features.head(log_features_count).iterrows():
                        self.logger.info(f"      {row['feature']}: {row['importance']:.3f}")
                
                # F1-FOCUSED Clinical insights
                insights = []
                test_f1_weighted = prediction_results['best_model']['test_f1_weighted']
                test_f1_macro = prediction_results['best_model']['test_f1_macro']
                test_precision = prediction_results['best_model']['test_precision']
                test_recall = prediction_results['best_model']['test_recall']
                test_accuracy = prediction_results['best_model']['test_accuracy']
                
                # F1-based clinical assessment (more important than accuracy for medical applications)
                if test_f1_weighted > 0.85:
                    insights.append("ðŸ† EXCELLENT F1 PERFORMANCE: Weighted F1 > 0.85 - Clinical deployment ready")
                elif test_f1_weighted > 0.80:
                    insights.append("âœ… STRONG F1 PERFORMANCE: Weighted F1 > 0.80 - Clinically acceptable")
                elif test_f1_weighted > 0.75:
                    insights.append("ðŸŽ¯ GOOD F1 PERFORMANCE: Weighted F1 > 0.75 - Shows clinical promise")
                else:
                    insights.append("âš ï¸ LIMITED F1 PERFORMANCE: Consider feature engineering or data augmentation")
                
                # Precision-Recall balance analysis
                if test_precision > 0.80 and test_recall > 0.80:
                    insights.append("âš–ï¸ BALANCED PRECISION-RECALL: Low false positives AND low false negatives")
                elif test_precision > 0.80:
                    insights.append("ðŸŽ¯ HIGH PRECISION: Low false positive rate - conservative predictions")
                elif test_recall > 0.80:
                    insights.append("ðŸ” HIGH RECALL: Low false negative rate - good case detection")
                else:
                    insights.append("ðŸ“Š MODERATE PRECISION-RECALL: Room for improvement in clinical decision balance")
                
                # Class balance analysis
                if abs(test_f1_weighted - test_f1_macro) < 0.05:
                    insights.append("ðŸŽ¯ WELL-BALANCED CLASS PERFORMANCE: Similar performance across all CDR levels")
                else:
                    insights.append("âš ï¸ CLASS IMBALANCE DETECTED: Some CDR levels predicted better than others")
                
                if len(X.columns) > 20:
                    insights.append("ðŸ“Š HIGH-DIMENSIONAL ANALYSIS: Feature selection may improve interpretability")
                
                prediction_results['clinical_insights'] = insights
                
                # F1-FOCUSED Enhanced logging (F1 scores first!)
                self.logger.info(f"\nðŸ† ===== FINAL F1-FOCUSED CLINICAL EVALUATION =====")
                self.logger.info(f"ðŸ¥‡ Best Model: {best_model_name}")
                self.logger.info(f"ðŸŽ¯ PRIMARY METRICS (Clinical Focus):")
                self.logger.info(f"   ðŸ”¥ F1-Score (Weighted): {test_f1_weighted:.3f}")
                self.logger.info(f"   ðŸ”¥ F1-Score (Macro): {test_f1_macro:.3f}")
                self.logger.info(f"   ðŸ”¥ Precision (Weighted): {test_precision:.3f}")
                self.logger.info(f"   ðŸ”¥ Recall (Weighted): {test_recall:.3f}")
                self.logger.info(f"ðŸ“Š SECONDARY METRICS:")
                self.logger.info(f"   ðŸ“ˆ Accuracy: {test_accuracy:.3f}")
                self.logger.info(f"   ðŸ“ˆ CV F1-Score: {best_f1_score:.3f}")
                
                # Per-class F1 performance (clinical decision support)
                classification_report_data = prediction_results['best_model']['classification_report']
                self.logger.info(f"ðŸ¥ PER-CLASS CLINICAL PERFORMANCE:")
                for class_name, metrics in classification_report_data.items():
                    if class_name.isdigit():
                        cdr_value = {0: '0.0', 1: '0.5', 2: '1.0'}.get(int(class_name), class_name)
                        f1 = metrics.get('f1-score', 0)
                        precision = metrics.get('precision', 0)
                        recall = metrics.get('recall', 0)
                        support = metrics.get('support', 0)
                        self.logger.info(f"   CDR {cdr_value}: F1={f1:.3f}, Prec={precision:.3f}, Rec={recall:.3f} (n={int(support)})")
                
                # Clinical acceptability assessment
                clinically_acceptable = test_f1_weighted >= 0.75 and test_precision >= 0.75 and test_recall >= 0.75
                self.logger.info(f"ðŸ¥ CLINICAL ACCEPTABILITY: {'âœ… APPROVED' if clinically_acceptable else 'âŒ NEEDS IMPROVEMENT'}")
                self.logger.info(f"ðŸ’¡ F1-FOCUSED INSIGHTS:")
                for insight in insights:
                    self.logger.info(f"   {insight}")
                
            else:
                prediction_results['error'] = 'No models successfully trained'
                
        except Exception as e:
            prediction_results['error'] = f"Advanced CDR prediction failed: {str(e)}"
            self.logger.error(f"Advanced CDR prediction error: {e}")
        
        return prediction_results
    
    def _detect_data_leakage(self, df: pd.DataFrame, target_col: str, correlation_threshold: float = 0.95) -> List[str]:
        """
        Comprehensive data leakage detection system
        
        Detects various forms of data leakage:
        1. Same column names (exact matches)
        2. Similar column names (fuzzy matching) 
        3. Perfect or near-perfect correlations
        4. Future information (temporal leakage)
        """
        leakage_columns = []
        
        # Legitimate predictors that should NOT be considered leakage even with high correlation
        legitimate_predictors = ['MMSE', 'Age', 'EDUC', 'SES', 'eTIV', 'nWBV', 'ASF', 'Gender', 'Gender_M', 'Gender_F', 'Hand']
        
        try:
            # 1. Exact name matches (case-insensitive)
            exact_matches = [col for col in df.columns if col != target_col and target_col.upper() in col.upper()]
            leakage_columns.extend(exact_matches)
            
            # 2. Fuzzy name matching for variations
            import difflib
            target_variations = [target_col.lower(), target_col.upper(), f"{target_col}_", f"_{target_col}"]
            for col in df.columns:
                if col != target_col:
                    for variation in target_variations:
                        if variation in col.lower() or difflib.SequenceMatcher(None, col.lower(), target_col.lower()).ratio() > 0.8:
                            if col not in leakage_columns:
                                leakage_columns.append(col)
            
            # 3. High correlation detection (only for numeric columns, excluding legitimate predictors)
            if target_col in df.columns and df[target_col].dtype in ['int64', 'float64']:
                
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                for col in numeric_cols:
                    if col != target_col and col not in leakage_columns:
                        # Skip legitimate predictors
                        if any(legit in col for legit in legitimate_predictors):
                            continue
                            
                        # Calculate correlation, handling NaN values
                        try:
                            corr_data = df[[col, target_col]].dropna()
                            if len(corr_data) > 10:  # Need sufficient data points
                                correlation = corr_data[col].corr(corr_data[target_col])
                                if abs(correlation) > correlation_threshold:
                                    leakage_columns.append(col)
                                    self.logger.warning(f"   High correlation detected: {col} vs {target_col} = {correlation:.3f}")
                        except:
                            pass
            
            # 4. Temporal leakage detection (columns that might contain future information)
            # Skip legitimate predictors from temporal analysis
            temporal_keywords = ['future', 'outcome', 'result', 'final', 'end', 'discharge', 'follow', 'post']
            for col in df.columns:
                if col != target_col and any(keyword in col.lower() for keyword in temporal_keywords):
                    # Skip legitimate predictors
                    if not any(legit in col for legit in legitimate_predictors):
                        if col not in leakage_columns:
                            leakage_columns.append(col)
            
            # 5. Log leakage detection summary
            if leakage_columns:
                self.logger.warning(f"   ðŸ“Š LEAKAGE ANALYSIS: Found {len(leakage_columns)} potentially leaky features")
                leakage_types = {
                    'name_based': [col for col in exact_matches],
                    'correlation_based': [col for col in leakage_columns if col not in exact_matches],
                    'temporal_based': [col for col in leakage_columns if any(kw in col.lower() for kw in temporal_keywords)]
                }
                for leak_type, cols in leakage_types.items():
                    if cols:
                        self.logger.warning(f"       {leak_type}: {cols}")
            else:
                self.logger.info(f"   âœ… LEAKAGE CHECK: No obvious data leakage detected")
                        
        except Exception as e:
            self.logger.error(f"Error in leakage detection: {e}")
            # Fallback to basic name-based detection
            leakage_columns = [col for col in df.columns if col != target_col and target_col.upper() in col.upper()]
        
        return leakage_columns
    
    def _run_surveillance_analysis(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Run specialized analysis for surveillance data (like BRFSS)"""
        self.logger.info("   ðŸ”¬ Analyzing surveillance data patterns...")
        
        df = self.combined_data
        
        # 1. Temporal Analysis
        if 'YearStart' in df.columns:
            self.logger.info("   ðŸ“ˆ Performing temporal trend analysis...")
            temporal_analysis = {}
            
            # Group by year and calculate trends
            yearly_stats = df.groupby('YearStart')['Data_Value'].agg(['mean', 'median', 'std', 'count'])
            temporal_analysis['yearly_trends'] = yearly_stats.to_dict()
            
            # Detect trend direction
            years = yearly_stats.index.values
            values = yearly_stats['mean'].values
            if len(years) > 2:
                from scipy import stats
                slope, intercept, r_value, p_value, std_err = stats.linregress(years, values)
                temporal_analysis['trend'] = {
                    'direction': 'increasing' if slope > 0 else 'decreasing',
                    'slope': float(slope),
                    'r_squared': float(r_value**2),
                    'p_value': float(p_value)
                }
            
            analysis_results['temporal_analysis'] = temporal_analysis
        
        # 2. Geographic Analysis
        if 'LocationAbbr' in df.columns:
            self.logger.info("   ðŸ—ºï¸ Performing geographic analysis...")
            geographic_analysis = {}
            
            # State-level statistics
            state_stats = df.groupby('LocationAbbr')['Data_Value'].agg(['mean', 'median', 'std', 'count'])
            geographic_analysis['state_statistics'] = state_stats.nlargest(10, 'mean').to_dict()
            geographic_analysis['high_risk_states'] = state_stats.nlargest(5, 'mean').index.tolist()
            geographic_analysis['low_risk_states'] = state_stats.nsmallest(5, 'mean').index.tolist()
            
            analysis_results['geographic_analysis'] = geographic_analysis
        
        # 3. Topic/Health Category Analysis
        if 'Topic' in df.columns:
            self.logger.info("   ðŸ¥ Analyzing health topics...")
            topic_analysis = {}
            
            # Topic statistics
            topic_stats = df.groupby('Topic')['Data_Value'].agg(['mean', 'median', 'count'])
            topic_analysis['topic_statistics'] = topic_stats.nlargest(10, 'count').to_dict()
            
            # Focus on cognitive-related topics
            cognitive_topics = df[df['Topic'].str.contains('Cognitive|Dementia|Alzheimer', case=False, na=False)]
            if not cognitive_topics.empty:
                topic_analysis['cognitive_metrics'] = {
                    'records': len(cognitive_topics),
                    'mean_value': float(cognitive_topics['Data_Value'].mean()),
                    'median_value': float(cognitive_topics['Data_Value'].median())
                }
            
            analysis_results['topic_analysis'] = topic_analysis
        
        # 4. Predictive Modeling for Surveillance Data
        self.logger.info("   ðŸŽ¯ Building predictive models for population health...")
        predictive_results = self._build_surveillance_predictive_models(df)
        analysis_results['predictive_models'] = predictive_results
        
        # 5. Risk Score Calculation
        if 'LocationAbbr' in df.columns and 'Data_Value' in df.columns:
            self.logger.info("   ðŸ“Š Calculating population risk scores...")
            risk_scores = self._calculate_population_risk_scores(df)
            analysis_results['risk_scores'] = risk_scores
        
        # 6. Stratification Analysis
        if 'Stratification1' in df.columns:
            self.logger.info("   ðŸ‘¥ Analyzing demographic stratifications...")
            strat_analysis = {}
            strat_stats = df.groupby('Stratification1')['Data_Value'].agg(['mean', 'median', 'std', 'count'])
            strat_analysis['stratification_statistics'] = strat_stats.to_dict()
            analysis_results['stratification_analysis'] = strat_analysis
        
        return analysis_results
    
    def _build_surveillance_predictive_models(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Build predictive models for surveillance data"""
        results = {}
        
        try:
            # Create feature matrix from pivoted data
            if 'LocationAbbr' in df.columns and 'YearStart' in df.columns and 'Topic' in df.columns:
                # Pivot to create features
                feature_matrix = df.pivot_table(
                    index=['LocationAbbr', 'YearStart'],
                    columns='Topic',
                    values='Data_Value',
                    aggfunc='mean'
                ).reset_index()
                
                # Identify target (cognitive decline if available)
                cognitive_cols = [col for col in feature_matrix.columns 
                                if 'Cognitive' in str(col) or 'Dementia' in str(col)]
                
                if cognitive_cols:
                    target_col = cognitive_cols[0]
                    feature_cols = [col for col in feature_matrix.columns 
                                  if col not in ['LocationAbbr', 'YearStart', target_col]]
                    
                    # Prepare data
                    valid_data = feature_matrix.dropna(subset=[target_col])
                    X = valid_data[feature_cols].fillna(valid_data[feature_cols].median())
                    y = valid_data[target_col]
                    
                    if len(X) > 30:  # Need enough data for modeling
                        from sklearn.model_selection import train_test_split
                        from sklearn.ensemble import RandomForestRegressor
                        from sklearn.metrics import r2_score, mean_absolute_error
                        
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, y, test_size=0.2, random_state=42
                        )
                        
                        # Train model
                        model = RandomForestRegressor(n_estimators=100, random_state=42)
                        model.fit(X_train, y_train)
                        
                        # Evaluate
                        y_pred = model.predict(X_test)
                        r2 = r2_score(y_test, y_pred)
                        mae = mean_absolute_error(y_test, y_pred)
                        
                        # Feature importance
                        feature_importance = pd.DataFrame({
                            'feature': feature_cols,
                            'importance': model.feature_importances_
                        }).sort_values('importance', ascending=False)
                        
                        results = {
                            'model_type': 'RandomForest',
                            'target': target_col,
                            'r2_score': float(r2),
                            'mae': float(mae),
                            'top_features': feature_importance.head(10).to_dict(),
                            'sample_size': len(X)
                        }
                        
                        self.logger.info(f"      âœ… Predictive model RÂ²={r2:.3f}, MAE={mae:.2f}")
        
        except Exception as e:
            self.logger.warning(f"      âš ï¸ Could not build predictive models: {e}")
        
        return results
    
    def _calculate_population_risk_scores(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate risk scores for populations"""
        risk_scores = {}
        
        try:
            # Calculate composite risk score by location
            location_scores = df.groupby('LocationAbbr')['Data_Value'].agg(['mean', 'std', 'count'])
            
            # Normalize scores
            location_scores['risk_score'] = (
                (location_scores['mean'] - location_scores['mean'].min()) / 
                (location_scores['mean'].max() - location_scores['mean'].min())
            )
            
            risk_scores = {
                'high_risk': location_scores.nlargest(5, 'risk_score').to_dict(),
                'low_risk': location_scores.nsmallest(5, 'risk_score').to_dict(),
                'median_risk': float(location_scores['risk_score'].median())
            }
            
        except Exception as e:
            self.logger.warning(f"      âš ï¸ Could not calculate risk scores: {e}")
        
        return risk_scores
    
    def _get_risk_color_palette(self):
        """Get intelligent color palette for risk visualization"""
        return {
            'high': '#d32f2f',      # Red for high risk
            'low': '#388e3c',       # Green for low risk  
            'neutral': '#42a5f5',   # Blue for neutral
            'secondary': '#ff9800'   # Orange for secondary categories
        }
    
    def _get_topic_color_palette(self):
        """Get intelligent color palette for topic analysis"""
        return {
            'prevalence': '#ff7043',    # Orange-red for prevalence
            'sample_size': '#29b6f6',   # Light blue for sample sizes
            'cognitive': '#ab47bc',     # Purple for cognitive metrics
            'health': '#66bb6a'         # Green for health metrics
        }
    
    def _calculate_optimal_display_count(self, total_items, item_type='items'):
        """Intelligently determine optimal number of items to display"""
        if item_type == 'states':
            # For states, show enough to capture patterns but not overwhelm
            return min(total_items, max(8, int(total_items * 0.6)))
        elif item_type == 'topics':
            # For topics, focus on most significant ones
            return min(total_items, max(6, int(total_items * 0.4)))
        elif item_type == 'performance_metrics':
            # For performance metrics, balance detail with readability
            return min(total_items, max(4, min(8, int(total_items * 0.75))))
        elif item_type == 'feature_importance':
            # For feature importance, show meaningful subset
            return min(total_items, max(5, min(15, int(total_items * 0.8))))
        else:
            # General case
            return min(total_items, max(5, int(total_items * 0.5)))
    
    def _calculate_text_truncation_length(self, text_list, max_width_chars=40):
        """Intelligently determine optimal text truncation length"""
        if not text_list:
            return max_width_chars
        
        avg_length = sum(len(text) for text in text_list) / len(text_list)
        max_length = max(len(text) for text in text_list)
        
        # If average is reasonable, use a bit more than average
        if avg_length <= max_width_chars * 0.7:
            return int(avg_length * 1.3)
        else:
            # If text is long, use adaptive truncation
            return max(20, max_width_chars)
    
    def _calculate_figure_size(self, num_items, plot_type='bar'):
        """Intelligently calculate figure size based on content"""
        if plot_type == 'geographic':
            width = max(12, min(20, num_items * 1.2))
            height = 8
        elif plot_type == 'topic_horizontal':
            width = max(14, min(18, num_items * 0.8))
            height = max(8, min(12, num_items * 0.6))
        elif plot_type == 'dashboard':
            width = 20
            height = 12
        else:
            width = max(10, min(16, num_items * 0.8))
            height = max(6, min(10, num_items * 0.5))
        
        return (width, height)
    
    def _calculate_correlation_limit(self, cols_a_count, cols_b_count):
        """Intelligently calculate correlation analysis limits to prevent computational explosion"""
        total_combinations = cols_a_count * cols_b_count
        
        # Scale limits based on total combinations to prevent exponential growth
        if total_combinations <= 25:  # Small dataset
            return min(cols_a_count, cols_b_count)
        elif total_combinations <= 100:  # Medium dataset  
            return min(10, max(cols_a_count, cols_b_count))
        elif total_combinations <= 400:  # Large dataset
            return min(8, max(cols_a_count, cols_b_count))
        else:  # Very large dataset
            return min(5, max(cols_a_count, cols_b_count))
    
    def _create_temporal_trends_plot(self, temporal_analysis):
        """Create temporal trend visualization"""
        trends = temporal_analysis.get('yearly_trends', {})
        if not trends or not trends.get('mean'):
            self.logger.warning("      âš ï¸ No temporal trends data available for plotting")
            return
        
        try:
            years = sorted([int(year) for year in trends['mean'].keys()])
            means = [trends['mean'][str(year)] for year in years]
            stds = [trends['std'][str(year)] for year in years] if 'std' in trends else [0] * len(years)
            counts = [trends['count'][str(year)] for year in years] if 'count' in trends else [1] * len(years)
        except (KeyError, ValueError) as e:
            self.logger.warning(f"      âš ï¸ Error parsing temporal trends data: {e}")
            return
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # Main trend plot
        ax1.errorbar(years, means, yerr=stds, marker='o', linewidth=2, 
                    markersize=8, capsize=5, capthick=2, color='steelblue')
        ax1.set_xlabel('Year')
        ax1.set_ylabel('Mean Data Value (%)')
        ax1.set_title('Population Health Surveillance: Temporal Trends', fontsize=16, pad=20)
        ax1.grid(True, alpha=0.3)
        
        # Add trend line
        z = np.polyfit(years, means, 1)
        p = np.poly1d(z)
        ax1.plot(years, p(years), "r--", alpha=0.8, linewidth=2, 
                label=f'Trend: {z[0]:+.2f}%/year')
        
        # Add RÂ² and p-value
        trend_info = temporal_analysis.get('trend', {})
        r_squared = trend_info.get('r_squared', 0)
        p_value = trend_info.get('p_value', 1)
        significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
        
        ax1.text(0.05, 0.95, f'RÂ² = {r_squared:.3f}\np = {p_value:.4f} {significance}', 
                 transform=ax1.transAxes, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        ax1.legend()
        
        # Sample size plot
        ax2.bar(years, counts, alpha=0.7, color='lightcoral', edgecolor='black')
        ax2.set_xlabel('Year')
        ax2.set_ylabel('Number of Records')
        ax2.set_title('Sample Sizes by Year')
        ax2.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/temporal_trends.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("      âœ… Temporal trends plot saved")
    
    def _create_geographic_patterns_plot(self, geographic_analysis):
        """Create geographic risk pattern visualization"""
        state_stats = geographic_analysis.get('state_statistics', {})
        high_risk = geographic_analysis.get('high_risk_states', [])
        low_risk = geographic_analysis.get('low_risk_states', [])
        
        if not state_stats or not state_stats.get('mean'):
            self.logger.warning("      âš ï¸ No geographic patterns data available for plotting")
            return
        
        # Intelligently determine number of states to show based on data
        all_states = list(state_stats['mean'].keys())
        num_states = min(len(all_states), max(8, len(high_risk) + len(low_risk)))  # At least 8, or enough to show risk states
        states = all_states[:num_states]
        means = [state_stats['mean'][state] for state in states]
        stds = [state_stats['std'][state] for state in states] if 'std' in state_stats else [0] * len(states)
        
        # Intelligent color coding based on risk categories
        risk_colors = self._get_risk_color_palette()
        colors = []
        for state in states:
            if state in high_risk:
                colors.append(risk_colors['high'])
            elif state in low_risk:
                colors.append(risk_colors['low'])
            else:
                colors.append(risk_colors['neutral'])
        
        # Dynamic figure sizing based on number of states
        fig_width = max(12, min(20, num_states * 1.2))
        fig, ax = plt.subplots(1, 1, figsize=(fig_width, 8))
        
        bars = ax.bar(range(len(states)), means, yerr=stds, capsize=4,
                     color=colors, alpha=0.7, edgecolor='black')
        ax.set_xlabel('State')
        ax.set_ylabel('Mean Data Value (%)')
        ax.set_title('Geographic Risk Patterns Across States', fontsize=16, pad=20)
        ax.set_xticks(range(len(states)))
        ax.set_xticklabels(states, rotation=45)
        ax.grid(True, alpha=0.3, axis='y')
        
        # Intelligent legend creation based on actual data
        from matplotlib.patches import Patch
        legend_elements = []
        if any(state in high_risk for state in states):
            legend_elements.append(Patch(facecolor=risk_colors['high'], alpha=0.7, label='High Risk States'))
        if any(state in low_risk for state in states):
            legend_elements.append(Patch(facecolor=risk_colors['low'], alpha=0.7, label='Low Risk States'))
        if any(state not in high_risk and state not in low_risk for state in states):
            legend_elements.append(Patch(facecolor=risk_colors['neutral'], alpha=0.7, label='Other States'))
        
        if legend_elements:
            ax.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/geographic_patterns.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("      âœ… Geographic patterns plot saved")
    
    def _create_topic_analysis_plot(self, topic_analysis):
        """Create health topic analysis visualization"""
        topic_stats = topic_analysis.get('topic_statistics', {})
        cognitive_metrics = topic_analysis.get('cognitive_metrics', {})
        
        if not topic_stats:
            return
        
        # Intelligently determine number of topics to show
        all_topics = list(topic_stats['mean'].keys())
        num_topics = self._calculate_optimal_display_count(len(all_topics), 'topics')
        topics = all_topics[:num_topics]
        means = [topic_stats['mean'][topic] for topic in topics]
        counts = [topic_stats['count'][topic] for topic in topics] if 'count' in topic_stats else [1] * len(topics)
        
        # Intelligent text truncation based on actual topic lengths
        truncation_length = self._calculate_text_truncation_length(topics)
        short_topics = [topic[:truncation_length] + '...' if len(topic) > truncation_length else topic for topic in topics]
        
        # Dynamic figure sizing based on number of topics
        fig_size = self._calculate_figure_size(num_topics, 'topic_horizontal')
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=fig_size)
        
        # Get intelligent colors
        colors = self._get_topic_color_palette()
        
        # Topic prevalence
        bars = ax1.barh(range(len(short_topics)), means, alpha=0.7, 
                        color=colors['prevalence'], edgecolor='black')
        ax1.set_xlabel('Mean Prevalence (%)')
        ax1.set_ylabel('Health Topics')
        ax1.set_title('Health Topic Prevalence Rates', fontsize=14)
        ax1.set_yticks(range(len(short_topics)))
        ax1.set_yticklabels(short_topics)
        ax1.grid(True, alpha=0.3, axis='x')
        
        # Sample sizes
        ax2.barh(range(len(short_topics)), counts, alpha=0.7,
                color=colors['sample_size'], edgecolor='black')
        ax2.set_xlabel('Number of Records')
        ax2.set_ylabel('Health Topics')
        ax2.set_title('Sample Sizes by Topic', fontsize=14)
        ax2.set_yticks(range(len(short_topics)))
        ax2.set_yticklabels(short_topics)
        ax2.grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/topic_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Cognitive health specific plot
        if cognitive_metrics and cognitive_metrics.get('records', 0) > 0:
            # Dynamic figure sizing for cognitive metrics
            fig_size = self._calculate_figure_size(3, 'cognitive_summary')
            fig, ax = plt.subplots(1, 1, figsize=fig_size)
            
            records = cognitive_metrics.get('records', 0)
            mean_value = cognitive_metrics.get('mean_value', 0)
            median_value = cognitive_metrics.get('median_value', 0)
            
            categories = ['Records\n(thousands)', 'Mean Value', 'Median Value']
            values = [records / 1000, mean_value, median_value]
            
            # Intelligent color selection
            topic_colors = self._get_topic_color_palette()
            colors = [topic_colors['cognitive'], topic_colors['health'], topic_colors['sample_size']]
            
            bars = ax.bar(categories, values, color=colors, alpha=0.7, edgecolor='black')
            ax.set_title('Cognitive Health Analysis Summary', fontsize=16, pad=20)
            ax.set_ylabel('Value')
            ax.grid(True, alpha=0.3, axis='y')
            
            plt.tight_layout()
            plt.savefig('outputs/visualizations/cognitive_health_summary.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        self.logger.info("      âœ… Topic analysis plots saved")
    
    def _create_risk_stratification_plot(self, risk_scores):
        """Create risk stratification visualization"""
        high_risk = risk_scores.get('high_risk', {})
        low_risk = risk_scores.get('low_risk', {})
        
        if not high_risk or not low_risk:
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # High-risk states
        hr_states = list(high_risk['mean'].keys())
        hr_means = list(high_risk['mean'].values())
        hr_risks = list(high_risk['risk_score'].values())
        
        bars1 = ax1.bar(hr_states, hr_means, alpha=0.7, color='red', edgecolor='black')
        ax1.set_xlabel('High-Risk States')
        ax1.set_ylabel('Mean Data Value (%)')
        ax1.set_title('High-Risk States', fontsize=14, pad=15)
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3, axis='y')
        
        # Low-risk states
        lr_states = list(low_risk['mean'].keys())
        lr_means = list(low_risk['mean'].values())
        
        bars2 = ax2.bar(lr_states, lr_means, alpha=0.7, color='green', edgecolor='black')
        ax2.set_xlabel('Low-Risk States')
        ax2.set_ylabel('Mean Data Value (%)')
        ax2.set_title('Low-Risk States', fontsize=14, pad=15)
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig('outputs/visualizations/risk_stratification.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("      âœ… Risk stratification plot saved")
    
    def _create_surveillance_dashboard(self, analysis_results):
        """Create comprehensive surveillance dashboard"""
        # Dynamic dashboard sizing
        fig_size = self._calculate_figure_size(8, 'dashboard')
        fig = plt.figure(figsize=fig_size)
        
        # Get key metrics
        total_subjects = analysis_results.get('data_summary', {}).get('total_subjects', 0)
        temporal = analysis_results.get('temporal_analysis', {})
        geographic = analysis_results.get('geographic_analysis', {})
        topic = analysis_results.get('topic_analysis', {})
        
        # Title
        fig.suptitle('Population Health Surveillance Dashboard', fontsize=24, fontweight='bold', y=0.95)
        
        # Key metrics panels
        ax1 = plt.subplot(2, 4, 1)
        ax1.text(0.5, 0.7, f'{total_subjects:,}', ha='center', va='center',
                fontsize=24, fontweight='bold', color='darkblue')
        ax1.text(0.5, 0.3, 'Total Records\nAnalyzed', ha='center', va='center',
                fontsize=12, fontweight='bold')
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)
        ax1.axis('off')
        
        # Temporal trend summary
        ax2 = plt.subplot(2, 4, 2)
        trend = temporal.get('trend', {})
        r_squared = trend.get('r_squared', 0)
        direction = trend.get('direction', 'unknown')
        ax2.text(0.5, 0.7, f'RÂ² = {r_squared:.3f}', ha='center', va='center',
                fontsize=18, fontweight='bold', color='green' if r_squared > 0.5 else 'orange')
        ax2.text(0.5, 0.3, f'Temporal Trend\n({direction})', ha='center', va='center',
                fontsize=12, fontweight='bold')
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)
        ax2.axis('off')
        
        # Geographic summary
        ax3 = plt.subplot(2, 4, 3)
        high_risk_count = len(geographic.get('high_risk_states', []))
        ax3.text(0.5, 0.7, f'{high_risk_count}', ha='center', va='center',
                fontsize=24, fontweight='bold', color='red')
        ax3.text(0.5, 0.3, 'High-Risk\nStates', ha='center', va='center',
                fontsize=12, fontweight='bold')
        ax3.set_xlim(0, 1)
        ax3.set_ylim(0, 1)
        ax3.axis('off')
        
        # Cognitive health records
        ax4 = plt.subplot(2, 4, 4)
        cog_records = topic.get('cognitive_metrics', {}).get('records', 0)
        ax4.text(0.5, 0.7, f'{cog_records:,}', ha='center', va='center',
                fontsize=20, fontweight='bold', color='purple')
        ax4.text(0.5, 0.3, 'Cognitive Health\nRecords', ha='center', va='center',
                fontsize=12, fontweight='bold')
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.axis('off')
        
        # Mini plots in bottom row
        if temporal.get('yearly_trends') and temporal['yearly_trends'].get('mean'):
            ax5 = plt.subplot(2, 4, 5)
            try:
                trends = temporal['yearly_trends']
                years = sorted([int(y) for y in trends['mean'].keys()])
                means = [trends['mean'][str(y)] for y in years]
                ax5.plot(years, means, 'o-', linewidth=2, markersize=6, color='steelblue')
                ax5.set_title('Temporal Trend', fontweight='bold')
                ax5.grid(True, alpha=0.3)
            except (KeyError, ValueError) as e:
                self.logger.warning(f"      âš ï¸ Error creating mini temporal plot: {e}")
                ax5 = plt.subplot(2, 4, 5)
                ax5.text(0.5, 0.5, 'Temporal data\nprocessing error', ha='center', va='center')
                ax5.set_title('Temporal Trend', fontweight='bold')
                ax5.axis('off')
        
        # Mini geographic plot
        if geographic.get('state_statistics'):
            ax6 = plt.subplot(2, 4, 6)
            all_states = list(geographic['state_statistics']['mean'].keys())
            display_count = self._calculate_optimal_display_count(len(all_states), 'states')
            states = all_states[:min(display_count, 8)]  # Limit for dashboard space
            means = [geographic['state_statistics']['mean'][s] for s in states]
            
            # Use intelligent colors
            high_risk = geographic.get('high_risk_states', [])
            low_risk = geographic.get('low_risk_states', [])
            risk_colors = self._get_risk_color_palette()
            colors = []
            for s in states:
                if s in high_risk:
                    colors.append(risk_colors['high'])
                elif s in low_risk:
                    colors.append(risk_colors['low'])
                else:
                    colors.append(risk_colors['neutral'])
                    
            ax6.bar(range(len(states)), means, color=colors, alpha=0.7)
            ax6.set_title('Geographic Patterns', fontweight='bold')
            ax6.set_xticks(range(len(states)))
            ax6.set_xticklabels(states, rotation=45, fontsize=8)
        
        # Analysis summary text
        ax7 = plt.subplot(2, 4, (7, 8))
        summary_text = f"""
SURVEILLANCE ANALYSIS SUMMARY:

â€¢ Population health surveillance dataset analyzed
â€¢ {total_subjects:,} total health records processed
â€¢ {high_risk_count} high-risk geographic regions identified  
â€¢ {cog_records:,} cognitive health indicators analyzed
â€¢ Temporal trend: {direction} pattern (RÂ²={r_squared:.3f})
â€¢ Geographic health disparities detected and quantified
â€¢ Multi-modal health surveillance indicators processed
â€¢ Population-level risk stratification completed
â€¢ Predictive modeling framework applied

This analysis provides comprehensive surveillance insights
for population health monitoring and intervention planning.
        """
        ax7.text(0.05, 0.95, summary_text, ha='left', va='top', fontsize=11,
                transform=ax7.transAxes)
        ax7.set_xlim(0, 1)
        ax7.set_ylim(0, 1)
        ax7.axis('off')
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.92)
        plt.savefig('outputs/visualizations/surveillance_dashboard.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("      âœ… Surveillance dashboard saved")


def main():
    """Test the cognitive analysis agent"""
    agent = CognitiveAnalysisAgent()
    results = agent.run_complete_analysis()
    agent.print_analysis_summary(results)
    return results


if __name__ == "__main__":
    main()