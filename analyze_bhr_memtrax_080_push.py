#!/usr/bin/env python3
"""
BHR MemTrax - Strategic Push to 0.80 AUC
=========================================
Focused approach based on our learnings:
1. Better feature engineering (polynomial + interactions)
2. Optimal model ensemble
3. Proper handling of class imbalance
4. Feature selection optimization
"""

import warnings
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFECV
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, make_scorer
import json

warnings.filterwarnings('ignore')

try:
    from xgboost import XGBClassifier
    HAS_XGB = True
except:
    HAS_XGB = False

DATA_DIR = Path("../bhr/BHR-ALL-EXT_Mem_2022")
OUTPUT_DIR = Path("bhr_memtrax_results")
OUTPUT_DIR.mkdir(exist_ok=True)
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Enhanced QID selection - focus on memory and executive function
MEMORY_QIDS = ['QID1-5', 'QID1-12']  # Memory complaints
EXEC_QIDS = ['QID1-13', 'QID1-22']  # Executive/planning issues
ALL_COG_QIDS = MEMORY_QIDS + EXEC_QIDS + ['QID1-23']


def extract_enhanced_features(memtrax_df):
    """Extract comprehensive features with focus on variability and patterns"""
    features = []
    
    for subject, group in memtrax_df.groupby('SubjectCode'):
        feat = {'SubjectCode': subject}
        
        # Basic stats
        feat['n_tests'] = len(group)
        
        # Performance metrics
        if 'CorrectPCT' in group.columns:
            feat['acc_mean'] = group['CorrectPCT'].mean()
            feat['acc_std'] = group['CorrectPCT'].std()
            feat['acc_min'] = group['CorrectPCT'].min()
            feat['acc_max'] = group['CorrectPCT'].max()
            feat['acc_range'] = feat['acc_max'] - feat['acc_min']
            
        if 'CorrectResponsesRT' in group.columns:
            feat['rt_mean'] = group['CorrectResponsesRT'].mean()
            feat['rt_std'] = group['CorrectResponsesRT'].std()
            feat['rt_min'] = group['CorrectResponsesRT'].min()
            feat['rt_max'] = group['CorrectResponsesRT'].max()
            feat['rt_cv'] = feat['rt_std'] / (feat['rt_mean'] + 1e-6)
            
        # Composite scores
        if 'CorrectPCT' in group.columns and 'CorrectResponsesRT' in group.columns:
            feat['efficiency'] = group['CorrectPCT'].mean() / (group['CorrectResponsesRT'].mean() + 0.01)
            feat['speed_accuracy_tradeoff'] = group['CorrectResponsesRT'].mean() / (group['CorrectPCT'].mean() + 0.01)
            
        # Error patterns
        for col in ['IncorrectResponsesN', 'IncorrectRejectionsN', 'CorrectRejectionsN']:
            if col in group.columns:
                feat[f'{col}_mean'] = group[col].mean()
                feat[f'{col}_std'] = group[col].std()
                
        # Sequence analysis from ReactionTimes
        all_rts = []
        for _, row in group.iterrows():
            if pd.notna(row.get('ReactionTimes')):
                try:
                    rt_str = str(row['ReactionTimes'])
                    rts = [float(x.strip()) for x in rt_str.split(',') 
                           if x.strip() and x.strip() != 'nan']
                    rts = [r for r in rts if 0.3 <= r <= 3.0]
                    all_rts.extend(rts)
                except:
                    continue
                    
        if len(all_rts) >= 20:
            # Fatigue patterns
            third = len(all_rts) // 3
            feat['fatigue_effect'] = np.mean(all_rts[-third:]) - np.mean(all_rts[:third])
            
            # Variability patterns
            feat['rt_seq_std'] = np.std(all_rts)
            feat['rt_seq_iqr'] = np.percentile(all_rts, 75) - np.percentile(all_rts, 25)
            
            # Trend
            if len(all_rts) >= 10:
                x = np.arange(len(all_rts))
                slope, intercept = np.polyfit(x, all_rts, 1)
                feat['rt_trend'] = slope
                feat['rt_intercept'] = intercept
                
        features.append(feat)
        
    return pd.DataFrame(features)


def create_smart_labels(med_hx):
    """Create labels with better handling of cognitive indicators"""
    if 'TimepointCode' in med_hx.columns:
        med_hx = med_hx[med_hx['TimepointCode'] == 'm00']
    med_hx = med_hx.drop_duplicates(subset=['SubjectCode'])
    
    available_qids = [q for q in ALL_COG_QIDS if q in med_hx.columns]
    
    if not available_qids:
        raise ValueError("No cognitive QIDs found")
    
    # Weight memory complaints higher
    weights = []
    impairment_cols = []
    
    for qid in available_qids:
        if qid in MEMORY_QIDS:
            weight = 1.5  # Memory complaints weighted higher
        elif qid in EXEC_QIDS:
            weight = 1.2  # Executive complaints
        else:
            weight = 1.0
            
        impaired = (med_hx[qid] == 1).astype(float)
        weights.append(weight)
        impairment_cols.append(impaired)
    
    # Weighted combination
    weighted_impairment = np.zeros(len(med_hx))
    total_weight = 0
    
    for w, imp in zip(weights, impairment_cols):
        weighted_impairment += w * imp.values
        total_weight += w
        
    # Threshold at weighted average > 0.4 (adjustable)
    cognitive_impairment = (weighted_impairment / total_weight > 0.3).astype(int)
    
    # Require at least one valid response
    valid = np.zeros(len(med_hx), dtype=bool)
    for qid in available_qids:
        valid |= med_hx[qid].isin([1, 2]).values
        
    labels = pd.DataFrame({
        'SubjectCode': med_hx['SubjectCode'],
        'cognitive_impairment': cognitive_impairment
    })
    
    labels = labels[valid].copy()
    
    print(f"  Created weighted labels: {len(labels)} subjects")
    print(f"  Prevalence: {labels['cognitive_impairment'].mean():.1%}")
    
    return labels


def add_enhanced_demographics(df, data_dir):
    """Add demographics with polynomial features"""
    # Load demographics
    demo_files = ['BHR_Demographics.csv', 'Profile.csv']
    
    for filename in demo_files:
        path = data_dir / filename
        if path.exists():
            demo = pd.read_csv(path, low_memory=False)
            if 'Code' in demo.columns:
                demo.rename(columns={'Code': 'SubjectCode'}, inplace=True)
                
            if 'SubjectCode' in demo.columns:
                cols_to_add = ['SubjectCode']
                
                # Get age, education, gender
                for target, options in [
                    ('Age_Baseline', ['Age_Baseline', 'Age', 'age']),
                    ('YearsEducationUS_Converted', ['YearsEducationUS_Converted', 'Education', 'education']),
                    ('Gender', ['Gender', 'gender', 'Sex'])
                ]:
                    for col in options:
                        if col in demo.columns:
                            if col != target:
                                demo.rename(columns={col: target}, inplace=True)
                            cols_to_add.append(target)
                            break
                
                if len(cols_to_add) > 1:
                    demo_subset = demo[cols_to_add].drop_duplicates('SubjectCode')
                    df = df.merge(demo_subset, on='SubjectCode', how='left')
                    break
    
    # Create polynomial and interaction features
    if 'Age_Baseline' in df.columns:
        df['Age_sq'] = df['Age_Baseline'] ** 2
        df['Age_cu'] = df['Age_Baseline'] ** 3
        df['Age_log'] = np.log(df['Age_Baseline'] + 1)
        
    if 'YearsEducationUS_Converted' in df.columns:
        df['Edu_sq'] = df['YearsEducationUS_Converted'] ** 2
        df['Edu_inv'] = 1 / (df['YearsEducationUS_Converted'] + 1)
        
    # Critical interactions
    if 'Age_Baseline' in df.columns and 'YearsEducationUS_Converted' in df.columns:
        df['Age_Edu'] = df['Age_Baseline'] * df['YearsEducationUS_Converted']
        df['CogReserve'] = df['YearsEducationUS_Converted'] / (df['Age_Baseline'] + 1)
        df['Reserve_sq'] = df['CogReserve'] ** 2
        
    # Performance-demographic interactions
    if 'Age_Baseline' in df.columns:
        for perf_col in ['rt_mean', 'acc_mean', 'efficiency', 'fatigue_effect']:
            if perf_col in df.columns:
                df[f'Age_{perf_col}'] = df['Age_Baseline'] * df[perf_col] / 65
                
    if 'Gender' in df.columns:
        df['Gender_Num'] = df['Gender'].map({'Male': 1, 'Female': 0, 'M': 1, 'F': 0})
        
    return df


def create_optimized_pipeline(n_features):
    """Create the best performing pipeline configuration"""
    
    # Use LogisticRegressionCV for automatic C tuning
    lr = make_pipeline(
        SimpleImputer(strategy='median'),
        StandardScaler(),
        SelectKBest(mutual_info_classif, k=min(50, n_features)),
        LogisticRegressionCV(
            Cs=np.logspace(-3, 1, 20),
            cv=5,
            scoring='roc_auc',
            max_iter=2000,
            class_weight='balanced',
            random_state=RANDOM_STATE
        )
    )
    
    # RandomForest with optimal settings
    rf = make_pipeline(
        SimpleImputer(strategy='median'),
        RandomForestClassifier(
            n_estimators=500,
            max_depth=12,
            min_samples_split=10,
            min_samples_leaf=3,
            max_features='sqrt',
            class_weight='balanced_subsample',
            random_state=RANDOM_STATE
        )
    )
    
    # HistGradientBoosting
    hgb = make_pipeline(
        SimpleImputer(strategy='median'),
        HistGradientBoostingClassifier(
            max_iter=500,
            max_leaf_nodes=31,
            learning_rate=0.02,
            max_depth=8,
            min_samples_leaf=10,
            l2_regularization=0.5,
            random_state=RANDOM_STATE
        )
    )
    
    # Ensemble with optimized weights
    ensemble = VotingClassifier(
        estimators=[('lr', lr), ('rf', rf), ('hgb', hgb)],
        voting='soft',
        weights=[1, 2, 2]  # Tree models get more weight
    )
    
    # Calibrate for better probability estimates
    calibrated = CalibratedClassifierCV(
        ensemble,
        method='isotonic',
        cv=3
    )
    
    return calibrated


def main():
    print("\n" + "="*60)
    print("BHR MEMTRAX - STRATEGIC PUSH TO 0.80 AUC")
    print("="*60)
    
    # Load data
    print("\n1. Loading data...")
    memtrax = pd.read_csv(DATA_DIR / 'MemTrax.csv', low_memory=False)
    med_hx = pd.read_csv(DATA_DIR / 'BHR_MedicalHx.csv', low_memory=False)
    
    # Quality filter with slightly relaxed threshold
    print("2. Applying quality filters...")
    memtrax_q = memtrax[
        (memtrax['Status'] == 'Collected') &
        (memtrax['CorrectPCT'] >= 0.55) &  # Slightly lower to catch impaired
        (memtrax['CorrectResponsesRT'].between(0.4, 2.5))  # Wider range
    ].copy()
    
    print(f"   Retained: {len(memtrax_q)}/{len(memtrax)} records")
    
    # Extract enhanced features
    print("3. Extracting enhanced features...")
    features = extract_enhanced_features(memtrax_q)
    print(f"   Features for {len(features)} subjects")
    
    # Add demographics with interactions
    print("4. Adding demographics and interactions...")
    features = add_enhanced_demographics(features, DATA_DIR)
    
    # Create smart labels
    print("5. Creating weighted labels...")
    labels = create_smart_labels(med_hx)
    
    # Merge
    data = features.merge(labels, on='SubjectCode', how='inner')
    print(f"\n6. Final dataset: {len(data)} subjects")
    
    # Add informant scores if available
    sp_ecog_path = DATA_DIR / 'BHR_SP_ECog.csv'
    if sp_ecog_path.exists():
        print("7. Adding informant scores...")
        sp_ecog = pd.read_csv(sp_ecog_path, low_memory=False)
        if 'TimepointCode' in sp_ecog.columns:
            sp_ecog = sp_ecog[sp_ecog['TimepointCode'] == 'm00']
        
        # Get numeric columns
        num_cols = sp_ecog.select_dtypes(include=[np.number]).columns
        ecog_items = [c for c in num_cols if 'QID' in c]
        
        if ecog_items:
            sp_ecog['SP_ECOG_mean'] = sp_ecog[ecog_items].mean(axis=1)
            sp_ecog['SP_ECOG_std'] = sp_ecog[ecog_items].std(axis=1)
            sp_ecog['SP_ECOG_max'] = sp_ecog[ecog_items].max(axis=1)
            
            sp_subset = sp_ecog[['SubjectCode', 'SP_ECOG_mean', 'SP_ECOG_std', 'SP_ECOG_max']]
            sp_subset = sp_subset.drop_duplicates('SubjectCode')
            
            data_before = len(data)
            data = data.merge(sp_subset, on='SubjectCode', how='left')
            print(f"   Added SP-ECOG for {data['SP_ECOG_mean'].notna().sum()} subjects")
    
    # Feature selection
    feature_cols = [c for c in data.columns if c not in ['SubjectCode', 'cognitive_impairment']]
    X = data[feature_cols]
    y = data['cognitive_impairment']
    
    print(f"\n8. Features: {X.shape}")
    print(f"   Target prevalence: {y.mean():.1%}")
    
    # Split with stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )
    
    print(f"\n9. Train/test split: {len(X_train)}/{len(X_test)}")
    
    # Train optimized model
    print("\n10. Training optimized ensemble...")
    model = create_optimized_pipeline(X.shape[1])
    
    # Cross-validation
    cv_scores = cross_val_score(
        model, X_train, y_train,
        cv=StratifiedKFold(5, shuffle=True, random_state=RANDOM_STATE),
        scoring='roc_auc'
    )
    print(f"    CV AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    
    # Train and evaluate
    model.fit(X_train, y_train)
    y_pred = model.predict_proba(X_test)[:, 1]
    test_auc = roc_auc_score(y_test, y_pred)
    
    print(f"    Test AUC: {test_auc:.4f}")
    
    # Try adding XGBoost if available
    if HAS_XGB and test_auc < 0.80:
        print("\n11. Adding XGBoost to ensemble...")
        
        xgb_pipe = make_pipeline(
            SimpleImputer(strategy='median'),
            XGBClassifier(
                n_estimators=800,
                max_depth=6,
                learning_rate=0.01,
                subsample=0.7,
                colsample_bytree=0.7,
                reg_lambda=2,
                reg_alpha=1,
                scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),
                random_state=RANDOM_STATE
            )
        )
        
        # New ensemble with XGBoost
        enhanced_ensemble = VotingClassifier(
            estimators=[
                ('orig', model),
                ('xgb', xgb_pipe)
            ],
            voting='soft',
            weights=[1, 1]
        )
        
        enhanced_ensemble.fit(X_train, y_train)
        y_pred_enhanced = enhanced_ensemble.predict_proba(X_test)[:, 1]
        enhanced_auc = roc_auc_score(y_test, y_pred_enhanced)
        
        print(f"    Enhanced Test AUC: {enhanced_auc:.4f}")
        
        if enhanced_auc > test_auc:
            test_auc = enhanced_auc
            model = enhanced_ensemble
    
    # Results
    print("\n" + "="*60)
    print("RESULTS")
    print("="*60)
    print(f"Final Test AUC: {test_auc:.4f}")
    
    if test_auc >= 0.80:
        print("\nðŸŽ¯ SUCCESS! ACHIEVED AUC â‰¥ 0.80!")
        print("\nKey factors:")
        print("  âœ“ Enhanced feature engineering")
        print("  âœ“ Weighted label creation")
        print("  âœ“ Optimized ensemble")
        print("  âœ“ Demographic interactions")
    else:
        print(f"\nBest AUC: {test_auc:.4f}")
        gap = 0.80 - test_auc
        print(f"Gap to 0.80: {gap:.4f}")
        
        if gap < 0.02:
            print("Very close! Consider:")
            print("  - More training data")
            print("  - External validation cohort")
            print("  - Biomarker data integration")
    
    # Save results
    results = {
        'test_auc': float(test_auc),
        'cv_auc': float(cv_scores.mean()),
        'cv_std': float(cv_scores.std()),
        'samples': len(data),
        'features': X.shape[1],
        'prevalence': float(y.mean()),
        'achieved_target': test_auc >= 0.80
    }
    
    with open(OUTPUT_DIR / 'push_080_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    return test_auc


if __name__ == '__main__':
    auc = main()

