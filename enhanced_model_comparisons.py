#!/usr/bin/env python3
"""
Enhanced Model Comparisons for MCI Prediction
==============================================
Implements baseline models (Cox, Logistic, SVM) and advanced ensemble methods
for comprehensive performance comparison in the JAD paper.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ML imports
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer

# Baseline models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from lifelines import CoxPHFitter
from lifelines.utils import concordance_index

# Ensemble models  
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Feature importance
import shap

class ModelComparison:
    """Compare multiple models for MCI prediction"""
    
    def __init__(self, data_path=None):
        self.results = {}
        self.models = {}
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
    def prepare_data(self, df, target_col='has_mci', feature_cols=None):
        """Prepare data for modeling"""
        print("ðŸ“Š Preparing data for model comparison...")
        
        if feature_cols is None:
            # Select numeric features
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            exclude = [target_col, 'SubjectCode', 'TimepointCode']
            feature_cols = [c for c in numeric_cols if c not in exclude]
        
        # Handle missing values
        imputer = SimpleImputer(strategy='median')
        X = imputer.fit_transform(df[feature_cols])
        y = df[target_col].values
        
        # Train-test split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # Standardize features
        self.scaler = StandardScaler()
        self.X_train = self.scaler.fit_transform(self.X_train)
        self.X_test = self.scaler.transform(self.X_test)
        
        self.feature_names = feature_cols
        print(f"âœ… Data prepared: {len(self.X_train)} train, {len(self.X_test)} test samples")
        print(f"   Features: {len(feature_cols)}")
        print(f"   MCI prevalence: {y.mean():.1%}")
        
    def run_logistic_regression(self):
        """Baseline: Logistic Regression with L2 regularization"""
        print("\nðŸ”¹ Running Logistic Regression...")
        
        model = LogisticRegression(
            penalty='l2',
            C=1.0,
            max_iter=1000,
            random_state=42,
            class_weight='balanced'
        )
        
        model.fit(self.X_train, self.y_train)
        
        # Predictions
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        auc = roc_auc_score(self.y_test, y_pred_proba)
        
        # Cross-validation
        cv_scores = cross_val_score(model, self.X_train, self.y_train, 
                                   cv=5, scoring='roc_auc')
        
        self.models['logistic'] = model
        self.results['logistic'] = {
            'auc': auc,
            'cv_auc_mean': cv_scores.mean(),
            'cv_auc_std': cv_scores.std(),
            'y_pred_proba': y_pred_proba
        }
        
        print(f"   AUC: {auc:.3f} (CV: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f})")
        return auc
    
    def run_cox_model(self, df_with_time=None):
        """Cox Proportional Hazards model for time-to-MCI"""
        print("\nðŸ”¹ Running Cox Proportional Hazards...")
        
        # For Cox model, we need time-to-event data
        # Simulate if not provided (in real analysis, use actual follow-up times)
        if df_with_time is None:
            # Create synthetic survival data for demonstration
            survival_df = pd.DataFrame(self.X_train, columns=self.feature_names)
            survival_df['duration'] = np.random.exponential(365, len(self.X_train))  # Days
            survival_df['event'] = self.y_train
        else:
            survival_df = df_with_time
        
        # Fit Cox model
        cph = CoxPHFitter()
        cph.fit(survival_df, duration_col='duration', event_col='event')
        
        # Calculate concordance index (similar to AUC for survival)
        c_index = cph.concordance_index_
        
        self.models['cox'] = cph
        self.results['cox'] = {
            'c_index': c_index,
            'hazard_ratios': cph.hazard_ratios_,
            'p_values': cph.summary['p'].to_dict()
        }
        
        print(f"   Concordance Index: {c_index:.3f}")
        return c_index
    
    def run_svm(self):
        """Support Vector Machine with RBF kernel"""
        print("\nðŸ”¹ Running Support Vector Machine...")
        
        model = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            probability=True,
            random_state=42,
            class_weight='balanced'
        )
        
        model.fit(self.X_train, self.y_train)
        
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        auc = roc_auc_score(self.y_test, y_pred_proba)
        
        self.models['svm'] = model
        self.results['svm'] = {
            'auc': auc,
            'y_pred_proba': y_pred_proba
        }
        
        print(f"   AUC: {auc:.3f}")
        return auc
    
    def run_random_forest_baseline(self):
        """Random Forest without temporal features"""
        print("\nðŸ”¹ Running Random Forest (Baseline)...")
        
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=20,
            random_state=42,
            class_weight='balanced'
        )
        
        model.fit(self.X_train, self.y_train)
        
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        auc = roc_auc_score(self.y_test, y_pred_proba)
        
        self.models['rf_baseline'] = model
        self.results['rf_baseline'] = {
            'auc': auc,
            'y_pred_proba': y_pred_proba,
            'feature_importance': model.feature_importances_
        }
        
        print(f"   AUC: {auc:.3f}")
        return auc
    
    def run_gradient_boosting(self):
        """Gradient Boosting with early stopping"""
        print("\nðŸ”¹ Running Gradient Boosting...")
        
        model = GradientBoostingClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            random_state=42,
            validation_fraction=0.2,
            n_iter_no_change=10
        )
        
        model.fit(self.X_train, self.y_train)
        
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        auc = roc_auc_score(self.y_test, y_pred_proba)
        
        self.models['gradient_boosting'] = model
        self.results['gradient_boosting'] = {
            'auc': auc,
            'y_pred_proba': y_pred_proba,
            'feature_importance': model.feature_importances_
        }
        
        print(f"   AUC: {auc:.3f}")
        return auc
    
    def run_xgboost(self):
        """XGBoost with temporal sample weights"""
        print("\nðŸ”¹ Running XGBoost...")
        
        model = XGBClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        
        model.fit(
            self.X_train, self.y_train,
            eval_set=[(self.X_test, self.y_test)],
            early_stopping_rounds=20,
            verbose=False
        )
        
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        auc = roc_auc_score(self.y_test, y_pred_proba)
        
        self.models['xgboost'] = model
        self.results['xgboost'] = {
            'auc': auc,
            'y_pred_proba': y_pred_proba,
            'feature_importance': model.feature_importances_
        }
        
        print(f"   AUC: {auc:.3f}")
        return auc
    
    def calculate_shap_values(self, model_name='xgboost'):
        """Calculate SHAP values for model interpretation"""
        print(f"\nðŸ“Š Calculating SHAP values for {model_name}...")
        
        model = self.models[model_name]
        
        # Create SHAP explainer
        if model_name in ['xgboost', 'gradient_boosting']:
            explainer = shap.TreeExplainer(model)
        else:
            explainer = shap.KernelExplainer(model.predict_proba, self.X_train[:100])
        
        shap_values = explainer.shap_values(self.X_test)
        
        # For binary classification, take positive class SHAP values
        if isinstance(shap_values, list):
            shap_values = shap_values[1]
        
        # Calculate mean absolute SHAP values for feature importance
        feature_importance = np.abs(shap_values).mean(axis=0)
        
        # Create feature importance dataframe
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'shap_importance': feature_importance
        }).sort_values('shap_importance', ascending=False)
        
        self.results[f'{model_name}_shap'] = {
            'shap_values': shap_values,
            'feature_importance': importance_df
        }
        
        print(f"   Top 5 features:")
        for idx, row in importance_df.head().iterrows():
            print(f"   - {row['feature']}: {row['shap_importance']:.3f}")
        
        return importance_df
    
    def plot_comparison(self, save_path='model_comparison.png'):
        """Plot AUC comparison across all models"""
        print("\nðŸ“ˆ Creating comparison plot...")
        
        # Collect results
        model_names = []
        aucs = []
        
        for name, result in self.results.items():
            if 'auc' in result:
                model_names.append(name.replace('_', ' ').title())
                aucs.append(result['auc'])
            elif 'c_index' in result:  # Cox model
                model_names.append('Cox Model')
                aucs.append(result['c_index'])
        
        # Create plot
        fig, ax = plt.subplots(figsize=(10, 6))
        
        colors = ['#d62728' if auc < 0.62 else '#ff7f0e' if auc < 0.66 
                  else '#2ca02c' if auc < 0.70 else '#1f77b4' 
                  for auc in aucs]
        
        bars = ax.bar(model_names, aucs, color=colors, edgecolor='black', linewidth=1)
        
        # Add value labels
        for bar, auc in zip(bars, aucs):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                   f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax.set_ylim(0.5, 0.75)
        ax.set_ylabel('AUC-ROC / C-Index', fontsize=12)
        ax.set_title('Model Performance Comparison for MCI Prediction', fontsize=14)
        ax.grid(axis='y', alpha=0.3)
        ax.axhline(y=0.7, color='red', linestyle='--', alpha=0.5, 
                   label='Target AUC = 0.70')
        ax.legend()
        
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"   Plot saved to {save_path}")
        
        return fig
    
    def generate_report(self, save_path='model_comparison_report.json'):
        """Generate comprehensive comparison report"""
        print("\nðŸ“ Generating comparison report...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'models': {},
            'best_model': None,
            'best_auc': 0
        }
        
        for name, result in self.results.items():
            if 'auc' in result:
                report['models'][name] = {
                    'auc': float(result['auc']),
                    'cv_auc': float(result.get('cv_auc_mean', 0)),
                    'cv_std': float(result.get('cv_auc_std', 0))
                }
                
                if result['auc'] > report['best_auc']:
                    report['best_auc'] = float(result['auc'])
                    report['best_model'] = name
        
        # Save report
        with open(save_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"   Report saved to {save_path}")
        print(f"   Best model: {report['best_model']} (AUC: {report['best_auc']:.3f})")
        
        return report

def run_full_comparison(data_path=None, df=None):
    """Run complete model comparison pipeline"""
    print("ðŸš€ RUNNING FULL MODEL COMPARISON")
    print("=" * 50)
    
    comp = ModelComparison(data_path)
    
    # Load or use provided data
    if df is None:
        # Load your actual data here
        print("âš ï¸  No data provided, using synthetic data for demonstration")
        # Generate synthetic data for demonstration
        np.random.seed(42)
        n_samples = 10000
        n_features = 20
        
        X = np.random.randn(n_samples, n_features)
        # Create target with some signal
        y = (X[:, 0] + 0.5 * X[:, 1] - 0.3 * X[:, 2] + 
             np.random.randn(n_samples) * 0.5 > 0.5).astype(int)
        
        df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])
        df['has_mci'] = y
    
    # Prepare data
    comp.prepare_data(df)
    
    # Run all models
    comp.run_logistic_regression()
    comp.run_svm()
    comp.run_random_forest_baseline()
    comp.run_gradient_boosting()
    comp.run_xgboost()
    
    # Try Cox model (will use synthetic survival times)
    try:
        comp.run_cox_model()
    except Exception as e:
        print(f"   Cox model skipped: {e}")
    
    # Calculate SHAP values for best model
    comp.calculate_shap_values('xgboost')
    
    # Generate plots and report
    comp.plot_comparison()
    report = comp.generate_report()
    
    print("\nâœ… Model comparison complete!")
    print(f"   Best model: {report['best_model']}")
    print(f"   Best AUC: {report['best_auc']:.3f}")
    
    return comp

if __name__ == "__main__":
    # Run comparison with sample data
    comparison = run_full_comparison()